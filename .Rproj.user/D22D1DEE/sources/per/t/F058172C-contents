#' @title Fréchet Variance Trajectory for densities
#' @description Modeling time varying density objects with respect to $L^2$-Wasserstein distance by Fréchet variance trajectory
#' @param tgrid Time grid vector for the time varying object data.
#' @param yin An array or list of lists holding the samples of observations. If \code{yin} is an array, it has size \code{n} x \code{length(tgrid)} x numbers of samples holding the observation, such that \code{yin[i,j,]} holds the observations to the ith sample at the jth time grid. If \code{yin} is a list of lists, \code{yin[[i]][[j]]} holds the observations to the ith sample at the jth time grid.     
#' @param hin A list of lists holding the histogram for each subject. \code{hin[[i]][[j]]} holds the histogram to the ith sample at the jth time grid.     
#' @param din A three dimension array of size \code{n} x \code{length(tgrid)} x \code{length(optns$dSup)} holding the observed densities, such that \code{din[i,j,]} holds the observed density function taking values on \code{optns$dSup} corresponding to the ith sample at the jth time grid.
#' @param qin A three dimension array of size \code{n} x \code{length(tgrid)} x \code{length(optns$qSup)} holding the observed quantiles, such that \code{din[i,j,]} holds the observed density function taking values on \code{optns$qSup} corresponding to the ith sample at the jth time grid.
#' Note that only one of \code{yin}, \code{hin}, \code{din} and \code{qin} needs to be input. If more than one of them are specified, \code{yin} overwrites \code{hin}, \code{hin} overwrites \code{din} and \code{din} overwrites \code{qin}.
#' where each row holds the observations for one subject on the common grid \code{tGrid}.
#' @param optns a list of options control parameters specified by \code{list(name=value)}.
#' @details Available control options are \code{qSup}, \code{nqSup}, \code{dSup} and other options in FPCA of fdapace.
#' @return A list of the following:
#' \item{tgridout}{Time grid vector for the output time varying object data.}
#' \item{K}{Numbers of principal components.}
#' \item{nu}{A vector of dimension \code{length(tgridout)} giving the mean function support on \code{tgridout} of the Fréchet variance function.}
#' \item{lambda}{A vector of dimension \code{K} containing eigenvalues.}
#' \item{phi}{A \code{length(tgridout)} X \code{K} matrix containing eigenfunctions support on \code{tgridout} of the Fréchet variance function.}
#' \item{xiEst}{A \code{n} X \code{K} matrix containing the FPC estimates.}
#' \item{cumFVE}{A vector of dimension \code{K} with the fraction of the cumulative total variance explained with each additional FPC.}
#' \item{FPCAObj}{FPCA Object of Fréchet variance function.}
#' \item{tgridin}{Input \code{tgrid}.}
#' \item{qSup}{A vector of dimension \code{length(tgridin)} giving the domain grid of quantile functions \code{qout}.}
#' \item{qout}{A three dimension array of dimension \code{n} x \code{length(tgridin)} x \code{length(qSup)} holding the observed quantiles, such that \code{qout[i,j,]} holds the observed density function taking values on \code{qSup} corresponding to the ith sample at the jth time grid.}
#' \item{qmean}{A \code{length(tgridin)} X \code{length(qSup)} matrix containing the time varying Fréchet mean function.}
#' \item{VarTraj}{A \code{n} X \code{length(tgridin)} matrix containing the variance trajectory.}
#' @examples
#' \donttest{
#' set.seed(1)
#' #use yin 
#' tgrid = seq(1, 50, length.out = 50)
#' dSup = seq(-10, 60, length.out = 100)
#' yin = array(dim=c(30, 50, 100))
#' for(i in 1:30){
#'   yin[i,,] = t(sapply(tgrid, function(t){
#'     rnorm(100, mean = rnorm(1, mean = 1, sd = 1/t))
#'   }))
#' }
#' result1 = VarObj(tgrid, yin = yin)
#' plot(result1$phi[,1])
#' plot(result1$phi[,2])
#' yin2 = replicate(30, vector("list", 50), simplify = FALSE)
#' for(i in 1:30){
#'   for(j in 1:50){
#'     yin2[[i]][[j]] = yin[i,j,]
#'   }}
#' result1 = VarObj(tgrid, yin = yin2)
#' 
#' # use hin
#' tgrid = seq(1, 50, length.out = 50)
#' dSup = seq(-10, 60, length.out = 100)
#' hin =  replicate(30, vector("list", 50), simplify = FALSE)
#' for(i in 1:30){
#'   for (j in 1:50){
#'     hin[[i]][[j]] = hist(yin[i,j,])
#'   }
#' }
#' result2 = VarObj(tgrid, hin = hin)
#' 
#' # use din
#' tgrid = seq(1, 50, length.out = 50)
#' dSup = seq(-10, 60, length.out = 100)
#' din = array(dim=c(30, 50, 100))
#' for(i in 1:30){
#'   din[i,,] = t(sapply(tgrid, function(t){
#'     dnorm(dSup, mean = rnorm(1, mean = t, sd = 1/t))
#'   }))
#' }
#' result3 = VarObj(tgrid, din = din, optns=list(dSup = dSup))
#' 
#' # use qin
#' tgrid = seq(1, 50, length.out = 50)
#' qSup = seq(0.00001,1-0.00001,length.out = 100)
#' qin = array(dim=c(30, 50, 100))
#' for(i in 1:30){
#'   qin[i,,] = t(sapply(tgrid, function(t){
#'     qnorm(qSup, mean = rnorm(1, mean = t, sd = 1/t))
#'   }))
#' }
#' result4 = VarObj(tgrid, qin = qin, optns=list(qSup = round(qSup, 4)))
#' }
#' @references
#' \cite{Dubey, P., & Müller, H. G. (2021). Modeling Time-Varying Random Objects and Dynamic Networks. Journal of the American Statistical Association, 1-33.}
#' @export
#' @import fdadensity
#' @import fdapace

VarObj <- function(tgrid, yin = NULL, hin = NULL, din = NULL, qin = NULL, optns=list()){
  
  ### Check input ###
  if(is.null(tgrid)){
    stop ("tgrid has no default and must be input by users.")
  }
  if (is.null(din) & is.null(qin) & is.null(yin) & is.null(hin))
    stop ("One of the four arguments, yin, hin, din and qin, should be input by users.")
  
  #check din
  if (!is.null(din)){ 
    if (!is.array(din)){
      stop ("din must be a three dimensional array ")
    }
    if (length(dim(din))!=3 || dim(din)[2] != length(tgrid)){
      stop ("din must be a three dimensional array with the dimension of second argument consisting with the length of tgrid")
    }
  }
  
  #check qin
  if (!is.null(qin)){ 
    if (!is.array(qin)){
      stop ("qin must be a three dimensional array")
    }
    if (length(dim(qin))!=3 || dim(qin)[2] != length(tgrid)){
      stop ("qin must be a three dimensional array with the dimension of second argument consisting with the length of tgrid")
    }
  }
  
  #check yin
  if (!is.null(yin)){
    if (is.list(yin)){
      length_y <- sapply(yin, function(yi) length(yi))
      if (sum(length_y != length(tgrid))){
        stop ("length of each subject of yin shoule be consistent with the length of tgrid")
      }
    }else if(is.array(yin)){
      if (length(dim(yin)) != 3 || dim(yin)[2] != length(tgrid) ){
        stop ("yin must be a three dimensional array with the dimension of second argument consisting with the length of tgrid")
      }
    }else{
      stop("input for yin should be either a three dimensional array of a list of a list ")
    }
    
  }
  
  #check hin
  if (!is.null(hin)){
    length_h <- sapply(hin, function(hi) length(hi))
    if (sum(length_h != length(tgrid))){
      stop ("length of each subject of hin shoule be consistent with the length of tgrid")
    }
  }
  
  #Specify qSup #modified from GloDenReg.R
  if (!is.null(optns$qSup)) {  #optns$qSup available
    if (min(optns$qSup) != 0 | max(optns$qSup) - 1 != 0)
      stop ("optns$qSup must have minimum 0 and maximum 1.")
    if (sum(duplicated(optns$qSup)) > 0) {
      optns$qSup <- unique(optns$qSup)
      warning ("optns$qSup has duplicated elements which has been removed.")
    }
    if (is.unsorted(optns$qSup)) {
      optns$qSup <- sort(optns$qSup)
      warning ("optns$qSup has been reordered to be increasing.")
    }
  }else {  #optns$qSup not available, create one!
    if (!is.null(qin)){ #qin available
      optns$qSup <- seq(0,1,length.out = dim(qin)[3])
      warning ("optns$qSup is missing and is set by default as an equidistant grid on [0,1] with length equal to the number of columns in matrix qin.")
    } else{ #din available, check optns$nqSup
      if(is.null(optns$nqSup)) {
        optns$nqSup <- 201
      }
      optns$qSup <- seq(0,1,length.out = optns$nqSup)
    }
  }
  qSup <- optns$qSup
  
  ### Create qin for different types of input ###
  nt <- length(tgrid)
  nq <- length(qSup)
  #create qin based on yin or hin 
  optnsRegIdx <- match(c("bwReg","kernelReg","lower","upper","qSup","nqSup","bwRange"), names(optns))
  optnsRegIdx <- optnsRegIdx[!is.na(optnsRegIdx)]
  optnsReg <- optns[optnsRegIdx]
  if (is.null(optnsReg$kernelReg))
    optnsReg$kernelReg <- "gauss"
  names(optnsReg)[which(names(optnsReg) == "kernelReg")] <- "ker"
  if (!is.null(optnsReg$bwReg))
    names(optnsReg)[which(names(optnsReg) == "bwReg")] <- "bw"
  
  optnsDen <- optns[-optnsRegIdx]
  if (!is.null(optnsDen$kernelDen))
    names(optnsDen)[which(names(optnsDen) == "kernelDen")] <- "kernel"
  if (!is.null(optnsDen$bwDen))
    names(optnsDen)[which(names(optnsDen) == "bwDen")] <- "userBwMu"
  
  if (!is.null(yin)){ 
    # use yin 
    if (!is.null(hin) || !is.null(din) || !is.null(qin)){
      warning("yin overwrites the other inputs")
    }
    # create qin based on yin
    if (is.list(yin)){
      n <- length(yin)
    }else{
      n <- dim(yin)[1]
    }
    qin <- array(dim=c(n, nt, nq))
    for(i in 1:n){
      for(j in 1:nt){
        if (is.list(yin)){
          dij <- CreateDensity(yin[[i]][[j]], optns = optnsDen)
        }else{
          dij <- CreateDensity(yin[i,j,], optns = optnsDen)
        }
        qin[i,j,] <- fdadensity::dens2quantile(dens = dij$y, dSup = dij$x, qSup = qSup)
      }
    }
  } else if (!is.null(hin)){
    if (!is.null(din) || !is.null(qin)){
      warning("hin overwrites the other inputs")
    }
    n <- length(hin)
    # create qin based on hin 
    qin <- array(dim=c(length(hin), nt, nq))
    for(i in 1:n){
      for(j in 1:nt){
        dij <- CreateDensity(histogram = hin[[i]][[j]], optns = optnsDen)
        qin[i,j,] <- fdadensity::dens2quantile(dens = dij$y, dSup = dij$x, qSup = qSup)
      }
    }
  } else if (!is.null(din)){
    if (!is.null(qin)){
      warning("din overwrites the other inputs")
    }
    # create qin based on din
    if(is.null(optns$dSup)){
      stop ("dSup need to be specified when using din")
    }
    dSup <- optns$dSup
    if (length(dSup)!=dim(din)[3]){
      stop ('dimension of the third arguement of din is not consistent with length of dSup')
    }
    n = dim(din)[1]
    qin <- array(dim = c(n, nt, nq))
    for (i in 1:n){
      for (j in 1: nt){
        qin[i,j,] <- fdadensity::dens2quantile(din[i,j,], dSup = dSup, qSup = qSup)
      }
    }
  } 
  n <- dim(qin)[1]
  

  
  ### FPCA ###
  #compute time varying fréchet mean based on qin 
  qmean <- sapply(1:nt, function(t){
    apply(qin[,t,], 2, mean)
  }) 
  qmean <- t(qmean) # dim nt x nq

  #compute variance trajectory based on qin, qmean
  VarTraj <- t(sapply(1:n, function(i){
    apply(qin[i,,] - qmean, 1, function(v){
      fdapace::trapzRcpp(qSup, v^2)
    })
  })) # dim of n x nt
  
  #FPCA based on VarTraj
  Lt <- lapply(1:n, function(i) tgrid)
  Ly <- lapply(1:n, function(i) VarTraj[i,])
  optnsFPCA <- optns[!(names(optns) %in% c("qSup", "nqSup", "dSup"))]
  VarTrajFPCA <- fdapace::FPCA(Ly = Ly, Lt = Lt, optns = optnsFPCA)
  
  return(list(tgridout = VarTrajFPCA$workGrid,
              nu = VarTrajFPCA$mu,
              lambda = VarTrajFPCA$lambda,
              phi = VarTrajFPCA$phi,
              xiEst = VarTrajFPCA$xiEst,
              K = VarTrajFPCA$selectK,
              cumFVE = VarTrajFPCA$cumFVE,
              FPCAObj = VarTrajFPCA,
              tgridin = tgrid,
              qSup = qSup,
              qout = qin,
              qmean = qmean,
              VarTraj = VarTraj
              ))
}

#' @description Helper function computing bootstrap replications of
#'   the Fréchet CPD test statistics for densities/quantiles.
#' @importFrom pracma trapz
#' @noRd

DenCPDStatistic <- function(data, indices, cutOff, qSup, bootSize = length(indices)) {
  LyBoot <- data[indices[1:bootSize]] # booted sample. m see bootstrap scheme in the paper
  n <- length(LyBoot) # number of observations
  scope <- ceiling(cutOff * n):(n - ceiling(cutOff * n))
  nTn <- sapply(scope, function(i) {
    n1 <- i
    n2 <- n - i
    lambda <- n1 / n
    LyBoot1 <- LyBoot[1:n1]
    LyBoot2 <- LyBoot[(n1 + 1):n]
    mup <- rowMeans(matrix(unlist(LyBoot), ncol = n)) # overall Frechet mean
    mu1 <- rowMeans(matrix(unlist(LyBoot1), ncol = n1)) # first part Frechet mean
    mu2 <- rowMeans(matrix(unlist(LyBoot2), ncol = n2)) # second part Frechet mean
    V1 <- mean(sapply(LyBoot1, function(LyBoot1i) {
      pracma::trapz(qSup, (LyBoot1i - mu1)^2)
    }))
    V2 <- mean(sapply(LyBoot2, function(LyBoot2i) {
      pracma::trapz(qSup, (LyBoot2i - mu2)^2)
    }))
    V1C <- mean(sapply(LyBoot1, function(LyBoot1i) {
      pracma::trapz(qSup, (LyBoot1i - mu2)^2)
    }))
    V2C <- mean(sapply(LyBoot2, function(LyBoot2i) {
      pracma::trapz(qSup, (LyBoot2i - mu1)^2)
    }))
    Di <- sapply(LyBoot, function(LyBooti) {
      pracma::trapz(qSup, (LyBooti - mup)^2)
    })
    sigma2 <- mean(Di^2) - mean(Di)^2
    nTni <- n * lambda * (1 - lambda) * ((V1 - V2)^2 + (V1C - V1 + V2C - V2)^2) / sigma2 # formula 2.5 nTn
    nTni
  })
  c(maxnTn = max(nTn), tau = which.max(nTn)) # test statistic and location of change point
}
#' Generate color bar/scale.
#' @param colVal A numeric vector giving the variable values to which each color is corresponding. It overrides \code{min} (and \code{max}) if \code{min > min(colVal)} (\code{max < max(colVal)}).
#' @param colBreaks A numeric vector giving the breaks dividing the range of variable into different colors. It overrides \code{min} and \code{max}.
#' @param min A scalar giving the minimum value of the variable represented by colors.
#' @param max A scalar giving the maximum value of the variable represented by colors.
#' @param lut Color vector. Default is \cr
#' \code{colorRampPalette(colors = c("pink","royalblue"))(length(colBreaks)-1)}.
#' @param nticks An integer giving the number of ticks used in the axis of color bar.
#' @param ticks A numeric vector giving the locations of ticks used in the axis of color bar; it overrides \code{nticks}.
#' @param title A character giving the label of the variable according to which the color bar is generated.
#' @return No return value.
#' @export

color.bar <- function(colVal = NULL, colBreaks = NULL, min = NULL, max = NULL,
                      lut = NULL, nticks = 5, ticks = NULL, title = NULL) {
  if (is.null(colVal) & is.null(min) & is.null(colBreaks))
    stop("At least one of colVal, colBreaks and min must be specified.")

  if (!is.null(colBreaks)) {
    if (!is.null(lut)) {
      if (length(colBreaks) - length(lut) != 1)
        stop("length(colBreaks) - length(lut) must be equal to 1.")
    }
    min <- min(colBreaks)
    max <- max(colBreaks)
  } else {
    if (!is.null(min)) {
      if (is.null(colVal)) {
        if (is.null(max)) stop("max must be given when min is specified but neither are colVal and colBreaks.")
        colVal <- seq(min, max, length.out = length(lut))
      } else {
        if (min > min(colVal))
          min <- min(colVal)
        if (is.null(max)) {
          max <- max(colVal)
        } else if(max < max(colVal)) {
          max <- max(colVal)
        }
      }
    } else if (!is.null(colVal)) {
      if (!is.null(lut)) {
        if (length(colVal) != length(lut))
          stop("colVal and lut must have the same length.")
      }
      min <- min(colVal)
      max <- max(colVal)
    }
    colBreaks <- c(colVal[1], colVal[-1] - diff(colVal)/2, colVal[length(colVal)])
    colBreaks[1] <- min
    colBreaks[length(colBreaks)] <- max
  }
  if (is.null(lut))
    lut <- colorRampPalette(colors = c("pink","royalblue"))(length(colBreaks)-1)
  if (is.null(ticks)) ticks <- seq(min, max, len=nticks)
  plot(c(0,10), c(min,max), type='n', bty='n', xaxt='n', xlab='', yaxt='n', ylab='')
  title(xlab = title, line = 1)
  #plot(c(0,10), c(min,max), type='n', bty='n', xaxt='n', xlab='', yaxt='n', ylab='', main=title)
  axis(2, ticks, las=1)
  for (i in 1:length(lut)) {
    rect(0,colBreaks[i],10,colBreaks[i+1], col=lut[i], border=NA)
  }
}
#'@title Plots for Fréchet regression for covariance matrices.
#' @param x A \code{covReg} object obtained from \code{\link{CovFMean}}, \code{\link{GloCovReg}} or \code{\link{LocCovReg}}.
#' @param optns A list of control options specified by \code{list(name=value)}. See 'Details'.
#' @details Available control options are
#' \describe{
#' \item{ind.xout}{A vector holding the indices of elements in \code{x$Mout} at which the plots will be made. Default is \itemize{
#' \item \code{1:length(x$Mout)} when \code{x$Mout} is of length no more than 3;
#' \item \code{c(1,round(length(x$Mout)/2),length(x$Mout))} when \code{x$Mout} is of length greater than 3.
#' }}
#' \item{nrow}{An integer --- default: 1; subsequent figures will be drawn in an \code{optns$nrow}-by-\cr
#' \code{ceiling(length(ind.xout)/optns$nrow)} array.}
#' \item{plot.type}{Character with two choices, "continuous" and "categorical".
#' The former plots the correlations in a continuous scale of colors by magnitude
#' while the latter categorizes the positive and negative entries into two different colors.
#' Default is "continuous"}
#' \item{plot.clust}{Character, the ordering method of the correlation matrix. 
#' \code{"original"} for original order (default);
#' \code{"AOE"} for the angular order of the eigenvectors;
#' \code{"FPC"} for the first principal component order;
#' \code{"hclust"} for the hierarchical clustering order, drawing 4 rectangles on the graph according to the hierarchical cluster;
#' \code{"alphabet"} for alphabetical order.}
#' \item{plot.method}{Character, the visualization method of correlation matrix to be used.
#' Currently, it supports seven methods, named "circle" (default), "square", "ellipse", "number", "pie", "shade" and "color". }
#' \item{CorrOut}{Logical, indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{plot.display}{Character, "full" (default), "upper" or "lower", display full matrix, lower triangular or upper triangular matrix.}
#'}
#'@return No return value.
#'@examples
#'#Example y input
#'n=20             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Cov_est=GloCovReg(x=x,y=y,xout=xout,optns=list(corrOut = FALSE, metric="power",alpha=3))
#'CreateCovRegPlot(Cov_est, optns = list(ind.xout = 2, plot.method = "shade"))
#'\donttest{
#'CreateCovRegPlot(Cov_est, optns = list(plot.method = "color"))
#'}
#'@export

CreateCovRegPlot <- function(x, optns = list()) {
  if (is.null(x$xout)) {
    ind.xout <- 1
  } else {
    if (is.null(optns$ind.xout)) {
      lMout <- length(x$Mout)
      if (lMout > 3) {
        ind.xout <- c(1,round(lMout/2),lMout)
      } else ind.xout <- seq_len(lMout)
    } else {
      if (!all(optns$ind.xout %in% (1:length(x$Mout))))
        stop("Each element in optns$ind.xout must be integers between 1 and length of x$Mout.")
      ind.xout <- optns$ind.xout
    }
  }

  if (is.null(optns$nrow)) {
    nrow <- 1
  } else {
    if (optns$nrow < 1)
      optns$nrow <- 1
    optns$nrow <- round(optns$nrow)
    nrow <- optns$nrow
  }
  
  oldpar <- par(no.readonly = TRUE)
  on.exit(par(oldpar))
  
  par(mfrow = c(nrow, ceiling(length(ind.xout)/nrow)))
  for (ind in 1:length(ind.xout)) {
    mout = x$Mout[[ind]]
    covplot(mout, optns = optns)
  }
}


#' @title Global Fréchet Regression for Spherical Data
#' 
#' @description  Global Fréchet regression for spherical data with respect to the geodesic distance.
#' 
#' @param xin A vector of length \eqn{n} or an \eqn{n}-by-\eqn{p} matrix with input measurement points.
#' @param yin An \eqn{n}-by-\eqn{m} matrix holding the spherical data, of which the sum of squares of elements within each row is 1.
#' @param xout A vector of length \eqn{k} or an \eqn{k}-by-\eqn{p}  with output measurement points; Default: the same grid as given in \code{xin}.
#' @return A list containing the following components:
#' \item{xout}{Input \code{xout}.}
#' \item{yout}{A \eqn{k}-by-\eqn{m} matrix holding the fitted responses, of which each row is a spherical vector, corresponding to each element in \code{xout}.}
#' \item{xin}{Input \code{xin}.}
#' \item{yin}{Input \code{yin}.}
#' 
#' @examples
#' \donttest{
#' n <- 101
#' xin <- seq(-1,1,length.out = n)
#' theta_true <- rep(pi/2,n)
#' phi_true <- (xin + 1) * pi / 4
#' ytrue <- apply( cbind( 1, phi_true, theta_true ), 1, pol2car )
#' yin <- t( ytrue )
#' xout <- xin
#' res <- GloSpheReg(xin=xin, yin=yin, xout=xout)
#' }
#' @references
#' \cite{Petersen, A., & Müller, H.-G. (2019). "Fréchet regression for random objects with Euclidean predictors." The Annals of Statistics, 47(2), 691--719.}
#' @export 

GloSpheReg <- function(xin=NULL, yin=NULL, xout=NULL){
  
  if (is.null(xin))
    stop ("xin has no default and must be input by users.")
  if (is.null(yin))
    stop ("yin has no default and must be input by users.")
  if (is.null(xout))
    xout <- xin
  if (!is.numeric(xin))
    stop("xin should be a numerical vector or matrix.")
  if (!is.matrix(yin) | !is.numeric(yin))
    stop("yin should be a numerical matrix.")
  if (!is.numeric(xout))
    stop("xout should be a numerical vector or matrix.")
  if(is.vector(xin)){
    xin <- as.matrix(xin)
  }
  if(is.vector(xout)){
    xout <- as.matrix(xout)
  }
  if (length(xin)!=nrow(yin))
    stop("The length of xin should be the same as the number of rows in yin.")
  if (sum(abs(rowSums(yin^2) - rep(1,nrow(yin))) > 1e-6)){
    yin = yin / sqrt(rowSums(yin^2))
    warning("Each row of yin has been standardized to enforce sum of squares equal to 1.")
  }
  
  yout <- GloSpheGeoReg(xin = xin, yin = yin, xout = xout)
  res <- list(xout = xout, yout = yout, xin = xin, yin = yin)
  class(res) <- "spheReg"
  return(res)
}
#' @title Generalized Fréchet integrals of covariance matrix
#' @description Calculating generalized Fréchet integrals of covariance (equipped with Frobenius norm) 
#' @param phi An eigenfunction along which we want to project the network
#' @param t_out Support of \code{phi}
#' @param X A three dimensional array of dimension \code{length(t_out) x m x m}, where \code{X[i,,]} is an m x m covariance matrix. 
#' @return A list of the following:
#' \item{f}{An adjacency matrix which corresponds to the Fréchet integral of \code{X} along \code{phi}}
#' @examples
#' \donttest{
#' set.seed(5)
#' library(mpoly)
#' n <- 100
#' N <- 50
#' t_out <- seq(0,1,length.out = N)
#' 
#' p2 <- as.function(mpoly::jacobi(2,4,3),silent=TRUE)
#' p4 <- as.function(mpoly::jacobi(4,4,3),silent=TRUE)
#' p6 <- as.function(mpoly::jacobi(6,4,3),silent=TRUE)
#' 
#' # first three eigenfunctions
#' phi1 <- function(t){
#'   p2(2*t-1)*t^(1.5)*(1-t)^2/(integrate(function(x) p2(2*x-1)^2*x^(3)*(1-x)^4,0,1))$value^(1/2)
#' }
#' phi2 <- function(t){
#'   p4(2*t-1)*t^(1.5)*(1-t)^2/(integrate(function(x) p4(2*x-1)^2*x^(3)*(1-x)^4,0,1))$value^(1/2)
#' }
#' phi3 <- function(t){
#'   p6(2*t-1)*t^(1.5)*(1-t)^2/(integrate(function(x) p6(2*x-1)^2*x^(3)*(1-x)^4,0,1))$value^(1/2)
#' }
#' 
#' # random component of covariance matrices
#' P12 <- 0 ## elements between communities
#' Score <- matrix(runif(n*4), nrow = n)
#' # first community
#' P1_cov <- 0.5 + 0.4*Score[,1] %*% t(phi1(t_out)) + 0.1*Score[,2] %*% t(phi3(t_out))  
#' # second community
#' P2_cov <- 0.5 + 0.3*Score[,3] %*% t(phi2(t_out)) + 0.1*Score[,4] %*% t(phi3(t_out))  
#' P1_diag <- 2 #diagonal elements of the first community
#' P2_diag <- 3 #diagonal elements of the second community
#' 
#' # create Network edge matrix 
#' N_net1 <- 5 # first community number
#' N_net2 <- 5 # second community number
#' 
#' # I: four dimension array of n x n matrix of squared distances between the time point u  
#' # of the ith process and process and the time point v of the jth object process, 
#' # e.g.: I[i,j,u,v] <- d_F^2(X_i(u) X_j(v)).
#' I <- array(0, dim = c(n,n,N,N))
#' for(u in 1:N){
#'   for(v in 1:N){
#'     #frobenius norm between two adjcent matrix  
#'     I[,,u,v] <- outer(P1_cov[,u], P1_cov[,v], function(a1, a2) (a1-a2)^2*(N_net1^2-N_net1)) + 
#'       outer(P2_cov[,u], P2_cov[,v], function(a1, a2) (a1-a2)^2*(N_net2^2-N_net2)) 
#'   }
#' }
#' 
#' # check ObjCov work 
#' Cov_result <- ObjCov(t_out, I, 3, smooth=FALSE)
#' Cov_result$lambda  # 0.266 0.15 0.04
#' 
#' # e.g. subj 2 
#' subj <- 2
#' # X_mat is the network for varying times with X[i,,] 
#' # is the adjacency matrices for the ith time point
#' X_mat <- array(0, c(N,(N_net1+N_net2), (N_net1+N_net2)))
#' for(i in 1:N){
#'   # edge between communities is P12
#'   Mat <- matrix(P12, nrow = (N_net1+N_net2), ncol = (N_net1+N_net2)) 
#'   # edge within the first communitiy is P1
#'   Mat[1:N_net1, 1:N_net1] <- P1_cov[subj, i] 
#'   # edge within the second community is P2
#'   Mat[(N_net1+1):(N_net1+N_net2), (N_net1+1):(N_net1+N_net2)] <- P2_cov[subj, i] 
#'   diag(Mat) <- c(rep(P1_diag,N_net1),rep(P2_diag, N_net2)) #diagonal element is 0 
#'   X_mat[i,,] <- Mat
#' }
#' # output the functional principal network(adjacency matrice) of the second eigenfunction
#' CovFIntegral(Cov_result$phi[,2], t_out, X_mat)
#' }
#' @references 
#' \cite{Dubey, P., & Müller, H. G. (2020). Functional models for time‐varying random objects. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2), 275-327.}
#' @import fdapace
#' @importFrom Matrix nearPD
#' @export 

CovFIntegral <- function(phi, t_out, X){
  
  N <- length(phi)
  m <- dim(X)[2]
  if(dim(X)[1] != N){
    stop("length of first argument of X and length of phi are not consistenet ")
  }
  if(dim(X)[3] != dim(X)[2]){
    stop("X[i,,] should be a square matrix")
  }
  
  phi_out <- phi / fdapace::trapzRcpp(t_out, phi)
  g_mini <- matrix(0, nrow = m, ncol = m)
  for(i in 1:(m-1)){
    for(j in (i+1):m){
      g_mini[i,j] <- fdapace::trapzRcpp(t_out, phi_out * X[,i,j])
      g_mini[j,i] <- g_mini[i,j]
    }
    g_mini[i,i] <- max(fdapace::trapzRcpp(t_out, phi * X[,i,i]),10^(-5))
  }
  g_mini[m,m] <- max(fdapace::trapzRcpp(t_out, phi * X[,m,m]),10^(-5))
  # find the nearest positive definite matrix to g_mini
  f = Matrix::nearPD(g_mini,doSym = TRUE)$mat
  return(list(f=f))
}


#' frechet: Statistical Analysis for Random Objects and Non-Euclidean Data
#' @description Provides implementation of statistical methods for random objects 
#' lying in various metric spaces, which are not necessarily linear spaces. 
#' The core of this package is Fréchet regression for random objects with 
#' Euclidean predictors, which allows one to perform regression analysis 
#' for non-Euclidean responses under some mild conditions. 
#' Examples include distributions in 2-Wasserstein space, 
#' covariance matrices endowed with power metric (with Frobenius metric as a special case), Cholesky and log-Cholesky metrics.  
#' References: Petersen, A., & Müller, H.-G. (2019) <doi:10.1214/17-AOS1624>.
#' @docType package
#' @aliases frechet-package
#' @name frechet
#' @importFrom grDevices colorRampPalette dev.new palette
#' @importFrom graphics abline axis barplot boxplot grid hist layout legend lines matlines matplot par plot points polygon rect text
#' @importFrom methods as
#' @importFrom Matrix Matrix
#' @importFrom pracma trapz
#' @importFrom stats aggregate approx approxfun binomial cov cor density dist dnorm dunif fitted glm kmeans lm median na.omit optim optimize poly predict quantile rnorm runif spline var sd weighted.mean
#' @importFrom utils head tail
NULL

utils::globalVariables(c("y"))
# using trust package and perturbation for initial value
#' @noRd
#' @import trust trust

GloSpheGeoReg <- function(xin, yin, xout) {
  k = length(xout)
  n = length(xin)
  m = ncol(yin)
  
  xbar <- colMeans(xin)
  Sigma <- cov(xin) * (n-1) / n
  invSigma <- solve(Sigma)
  
  yout = sapply(1:k, function(j){
    s <- 1 + t(t(xin) - xbar) %*% invSigma %*% (xout[j,] - xbar)
    s <- as.vector(s)
    
    # initial guess
    y0 = colMeans(yin*s)
    y0 = y0 / l2norm(y0)
    if (sum(sapply(1:n, function(i) sum(yin[i,]*y0)) > 1-1e-8)){
      #if (sum( is.infinite (sapply(1:n, function(i) (1 - sum(yin[i,]*y0)^2)^(-0.5) )[ker((xout[j] - xin) / bw)>0] ) ) + 
      #   sum(sapply(1:n, function(i) 1 - sum(yin[i,] * y0)^2 < 0)) > 0){
      y0[1] = y0[1] + 1e-3
      y0 = y0 / l2norm(y0)
    }
    
    objFctn = function(y){
      if ( ! isTRUE( all.equal(l2norm(y),1) ) ) {
        return(list(value = Inf))
      }
      f = mean(s * sapply(1:n, function(i) SpheGeoDist(yin[i,], y)^2))
      g = 2 * colMeans(t(sapply(1:n, function(i) SpheGeoDist(yin[i,], y) * SpheGeoGrad(yin[i,], y))) * s)
      res = sapply(1:n, function(i){
        grad_i = SpheGeoGrad(yin[i,], y)
        return((grad_i %*% t(grad_i) + SpheGeoDist(yin[i,], y) * SpheGeoHess(yin[i,], y)) * s[i])
      }, simplify = "array")
      h = 2 * apply(res, 1:2, mean)
      return(list(value=f, gradient=g, hessian=h))
    }
    res = trust::trust(objFctn, y0, 0.1, 1)
    return(res$argument)
  })
  return(t(yout))
}
#'@title Local Fréchet regression of conditional covariance matrices with Log-Cholesky and Cholesky metric
#'@noRd
#'@description Local Fréchet regression of covariance matrices with Euclidean predictors.
#'
#'@param x an n by p matrix of predictors.
#'@param M an q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#'@param xout an m by p matrix of output predictor levels.
#'@param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#'@details Available control options are 
#'\describe{
#'\item{bwCov}{bandwidth for conditional covariance estimation. If \code{bwCov} is not provided, it is chosen by cross validation.}
#'\item{kernel}{Name of the kernel function to be chosen from 'gauss', 'rect', 'epan', 'gausvar' and 'quar'. Default is 'gauss'.}
#'\item{corrOut}{Boolean indicating if Mout is shown as correlation or covariance matrix. Default: \code{FALSE} for only a covariance matrix.}
#'\item{metric}{Metric type choice, "log_cholesky", "cholesky" - default: \code{log_cholesky} for log Cholesky metric}
#' }
#' 
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance matrices at \code{xout}.}
#' \item{opts}{A list containing the \code{opts} parameters utilized.}
#' 
#' @examples
#' n=30 #sample size
#' m=5 # dimension of covariance matrices
#' x=cbind(matrix(rnorm(n),n),matrix(rnorm(n),n)) #vector of predictor values
#' M <- array(0,c(m,m,n))
#' a = rnorm(m); b = rnorm(m)
#' A = diag(m)+a%*%t(a);
#' B = diag(m)+3*b%*%t(b);
#' for (i in 1:n){
#'   aux <- x[i,1]*A + x[i,2]**2*B
#'   M[,,i] <- aux %*% t(aux)
#' }
#' xout=cbind(runif(5),runif(5)) #output predictor levels
#' Covlist = LFRCovCholesky(x,M,xout)
#'
#' @references
#' \cite{A Petersen and HG Müller (2019). "Fréchet regression for random objects with Euclidean predictors." An. Stat. 47, 691-719.}
#' \cite{Z Lin (2019). " Riemannian Geometry of Symmetric Positive Definite Matrices via Cholesky Decomposition." Siam. J. Matrix. Anal, A. 40, 1353–1370.}
#' @importFrom Matrix forceSymmetric

LFRCovCholesky <- function(x, M, xout, optns=list()){
  if(!is.matrix(x)&!is.vector(x)){
    stop('x must be a matrix or vector')
  }
  if(!is.matrix(xout)&!is.vector(xout)){
    stop('xout must be a matrix or vector')
  }
  
  if(is.vector(x)){x<- matrix(x,length(x)) }
  if(is.vector(xout)){xout<- matrix(xout,length(xout))}
  
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have same number of columns')
  }

  
  if(is.null(optns$bwCov)){
    bwCov = NA
  } else {
    bwCov = optns$bwCov
    if(min(bwCov)<=0){
      stop("bandwidth must be positive")
    }
  }

  if(is.null(optns$kernel)){
    kernel= 'gauss'
  } else {
    kernel = optns$kernel
  } 

  if(is.null(optns$corrOut)){
    corrOut = FALSE
  } else {
    corrOut = optns$corrOut
  }
  
  if(is.null(optns$metric)){
    metric = 'log_cholesky'
  } else {
    metric =  optns$metric
  }
  
  p = ncol(x)
  if(p>2){
    stop("The number of dimensions of the predictor x must be at most 2")
  }
  m = nrow(xout)
  n = nrow(x)

   
  Kern=kerFctn(kernel)
  K = function(x,h){
    k = 1
    for(i in 1:p){
      k=k*Kern(x[,i]/h[i])
    }
    return(as.numeric(k))
  }
  if(is.null(M)){
    stop("M must be provided")
  }
  
  if(is.array(M)){
    if (length(dim(M))!=3) {
      stop('M must be an array or a list')
    }
    MM = list()
    if(is.array(M)){
      for (i in 1:dim(M)[3]) {
        MM[[i]] = M[,,i]
      }
    }
    M = lapply(MM, function(X) (X+t(X))/2)
  }else{
    if(!is.list(M)){
      stop('M must be an array or a list')
    }
    M = lapply(M, function(X) (X+t(X))/2)
  }
  
  if(nrow(x)!= length(M)){
    stop("the number of rows of x must be the same as the number of covariance matrices in M")
  }
  
  computeLFRSPD=function(idx,x0,bw2){
    #idx: index for x
    #x0 m-by-p matrix,
    #bw2 are in b-by-p
    x=as.matrix(x[idx,])
    aux=K(x-matrix(t(x0),nrow=length(idx),ncol=length(x0),byrow=TRUE),bw2)
    mu0 = mean(aux)
    mu1 = colMeans(aux*(x - matrix(t(x0),nrow=length(idx),ncol=length(x0),byrow=TRUE)))
    mu2=0
    for(i in 1:length(idx)){
      mu2 = mu2 + aux[i]*(x[i,]-x0) %*% t(x[i,]-x0)/length(idx)
    }
    sL = array(0,length(idx))
    for(i in 1:length(idx)){
      sL[i] =aux[i]*(1-t(mu1)%*%solve(mu2)%*%(x[i,]-x0))
    }
    s = sum(sL)
    
    Mout = list()
    MM = M[idx]
    n = length(idx)
    if(metric == 'log_cholesky'){
      LL = lapply(MM, chol)
      L = lapply(LL, function(X) X - diag(diag(X)))
      D = lapply(LL, function(X) diag(X))

      U = 0
      E = 0
      for (i in 1:n) {
        U = U + sL[i]*L[[i]]
        E = E + sL[i]*log(D[[i]])
      }
      SS = U/s + diag(exp(E/s))
      Mout = t(SS)%*%SS
      
    } else {
      L = lapply(MM, chol)
      U = 0
      for (i in 1:n) {
        U = U + sL[i]*L[[i]]
      }
      Mout = t(U/s) %*% (U/s)
    }

    return(Mout)
  }
  
  distance <- function(M1, M2){
    if(metric == 'log_cholesky'){
      LL1 = chol(M1); LL2 = chol(M2)
      L1 = LL1 - diag(diag(LL1)); L2 = LL2 - diag(diag(LL2))
      D1 = diag(LL1); D2 = diag(LL2)
      L = L1 - L2; D = log(D1) - log(D2)
      res = sqrt(sum(sum(L^2))+sum(D^2))
    }else{ 
      L1 = chol(M1); L2 = chol(M2)
      L = L1 - L2;
      res = sqrt(sum(sum(L^2)))
    }
    return(res)
  }
  
  #CV for bwCov selection
  if(is.na(sum(bwCov))){
    if(p==1){
      bw_choice=SetBwRange(as.vector(x), as.vector(xout), kernel)
      objF=matrix(0,nrow=20,ncol=1)
      aux1=as.matrix(seq(bw_choice$min,bw_choice$max,length.out=nrow(objF)))
      for(i in 1:nrow(objF)){
        #Try-catch statement in case bandwidth is too small and produces numerical issues
        objF[i] = tryCatch({
          sum(sapply(1:dim(x)[1],function(j){
            distance(computeLFRSPD(setdiff(1:dim(x)[1],j),x[j],aux1[i]), M[[j]])
          }))
        }, error = function(e) {
          return(NA)
        })
      }
      if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
        stop("Bandwidth too small in cross-validation search")
      }else{
        ind=which(objF==min(objF,na.rm=TRUE))[1]
        bwCV=aux1[ind]
      }
    }
    if(p==2){
      bw_choice1=SetBwRange(as.vector(x[,1]), as.vector(xout[,1]), kernel)
      bw_choice2=SetBwRange(as.vector(x[,2]), as.vector(xout[,2]), kernel)
      if(n<=30){
        objF=matrix(0,nrow=6,ncol=6)
        aux1=seq(bw_choice1$min,bw_choice1$max,length.out=nrow(objF))
        aux2=seq(bw_choice2$min,bw_choice2$max,length.out=ncol(objF))
        for(i1 in 1:nrow(objF)){
          for(i2 in 1:ncol(objF)){
            #Try-catch statement in case bandwidth is too small and produces numerical issues
            objF[i1,i2] = tryCatch({
              sum(sapply(1:dim(x)[1],function(j){
                distance(computeLFRSPD(setdiff(1:dim(x)[1],j),x[j,],c(aux1[i1],aux2[i2])), M[[j]])
              }))
            }, error = function(e) {
              return(NA)
            })
          }
        }
        if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
          stop("Bandwidth too small in cross-validation search")
        }else{
          ind=which(objF==min(objF,na.rm=TRUE),arr.ind = TRUE)
          bwCV=c(aux1[ind[1]],aux2[ind[2]])
        }
      }else{
        randIndices=sample(dim(x)[1])
        groupIndices=cut(seq(1,dim(x)[1]),breaks=10,labels=FALSE)
        cv10fold_compute=function(v,leaveIn){
          distance(computeLFRSPD(leaveIn,x[v,],c(aux1[i1],aux2[i2])),M[[v]])
        }
        objF=matrix(0,nrow=6,ncol=6)
        aux1=seq(bw_choice1$min,bw_choice1$max,length.out=nrow(objF))
        aux2=seq(bw_choice2$min,bw_choice2$max,length.out=ncol(objF))
        for(i1 in 1:nrow(objF)){
          for(i2 in 1:ncol(objF)){
            #Try-catch statement in case bandwidth is too small and produces numerical issues
            objF[i1,i2] = tryCatch({
              sum(sapply(1:10,function(j){
                leaveIn=setdiff(1:(dim(x)[1]),randIndices[groupIndices==j])
                sum(sapply(randIndices[groupIndices==j],function(v){cv10fold_compute(v,leaveIn)}))
              }))
            }, error = function(e) {
              return(NA)
            })
          }
        }
        if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
          stop("Bandwidth too small in cross-validation search")
        }else{
          ind=which(objF==min(objF,na.rm=TRUE),arr.ind = TRUE)
          bwCV=c(aux1[ind[1]],aux2[ind[2]])
        }
      }
    }
    bwCov=bwCV
  }
  
  
  Mout = list()
  for (j in 1:nrow(xout)) {
    Mout[[j]] = computeLFRSPD(1:dim(x)[1], xout[j,], bwCov)
  }
  
  if(corrOut){
    for(j in 1:nrow(xout)){
      D=diag(1/sqrt(diag(Mout[[j]])))
      Mout[[j]]=D%*%Mout[[j]]%*%D
      Mout[[j]]=as.matrix(Matrix::forceSymmetric(Mout[[j]]))
    }
  }
  out = list(xout=xout,Mout=Mout,optns=list(bwCov =bwCov,kernel=kernel,corrOut=corrOut,metric=metric))
  return(out)
}




#' @title Local Fréchet regression for correlation matrices
#' @description Local Fréchet regression for correlation matrices
#'   with Euclidean predictors.
#' @param x an n by p matrix or data frame of predictors.
#' @param M a q by q by n array (resp. a list of q by q matrices) where
#'   \code{M[, , i]} (resp. \code{M[[i]]}) contains the i-th correlation matrix
#'   of dimension q by q.
#' @param xOut an m by p matrix or data frame of output predictor levels.
#'   It can be a vector of length p if m = 1.
#' @param optns A list of options control parameters specified by
#'   \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{metric}{choice of metric. \code{'frobenius'} and \code{'power'} are supported,
#'   which corresponds to Frobenius metric and Euclidean power metric,
#'   respectively. Default is Frobenius metric.}
#' \item{alpha}{the power for Euclidean power metric. Default is 1 which
#'   corresponds to Frobenius metric.}
#' \item{kernel}{Name of the kernel function to be chosen from \code{'gauss'},
#'   \code{'rect'}, \code{'epan'}, \code{'gausvar'} and \code{'quar'}. Default is \code{'gauss'}.}
#' \item{bw}{bandwidth for local Fréchet regression, if not entered
#'   it would be chosen from cross validation.}
#' \item{digits}{the integer indicating the number of decimal places (round)
#'   to be kept in the output. Default is NULL, which means no round operation.}
#' }
#' @return A \code{corReg} object --- a list containing the following fields:
#' \item{fit}{a list of estimated correlation matrices at \code{x}.}
#' \item{predict}{a list of estimated correlation matrices at \code{xOut}.
#'   Included if \code{xOut} is not \code{NULL}.}
#' \item{residuals}{Frobenius distance between the true and
#'   fitted correlation matrices.}
#' \item{xOut}{the output predictor level used.}
#' \item{optns}{the control options used.}
#' @examples
#' # Generate simulation data
#' \donttest{
#' n <- 100
#' q <- 10
#' d <- q * (q - 1) / 2
#' xOut <- seq(0.1, 0.9, length.out = 9)
#' x <- runif(n, min = 0, max = 1)
#' y <- list()
#' for (i in 1:n) {
#'   yVec <- rbeta(d, shape1 = sin(pi * x[i]), shape2 = 1 - sin(pi * x[i]))
#'   y[[i]] <- matrix(0, nrow = q, ncol = q)
#'   y[[i]][lower.tri(y[[i]])] <- yVec
#'   y[[i]] <- y[[i]] + t(y[[i]])
#'   diag(y[[i]]) <- 1
#' }
#' # Frobenius metric
#' fit1 <- LocCorReg(x, y, xOut,
#'   optns = list(metric = "frobenius", digits = 2)
#' )
#' # Euclidean power metric
#' fit2 <- LocCorReg(x, y, xOut,
#'   optns = list(
#'     metric = "power", alpha = .5,
#'     kernel = "epan", bw = 0.08
#'   )
#' )
#' }
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' }
#' @importFrom Matrix nearPD
#' @export

LocCorReg <- function(x, M, xOut = NULL, optns = list()) {
  if (is.null(optns$metric)) {
    metric <- "frobenius"
  } else {
    metric <- optns$metric
  }
  if (!(metric %in% c("frobenius", "power"))) {
    stop("metric choice not supported")
  }
  if (is.null(optns$kernel)) {
    kernel <- "gauss"
  } else {
    kernel <- optns$kernel
  }
  if (is.null(optns$bw)) {
    bw <- NA
  } else {
    bw <- optns$bw
  }
  if (is.null(optns$alpha)) {
    alpha <- 1
  } else {
    alpha <- optns$alpha
  }
  if (alpha < 0) {
    stop("alpha must be non-negative")
  }
  if (is.null(optns$digits)) {
    digits <- NA
  } else {
    digits <- optns$digits
  }
  if (!is.matrix(x)) {
    if (is.data.frame(x) | is.vector(x)) {
      x <- as.matrix(x)
    } else {
      stop("x must be a matrix or a data frame")
    }
  }
  if (!is.null(xOut)) {
    if (!is.matrix(xOut)) {
      if (is.data.frame(xOut)) {
        xOut <- as.matrix(xOut)
      } else if (is.vector(xOut)) {
        if (ncol(x) == 1) {
          xOut <- as.matrix(xOut)
        } else {
          xOut <- t(xOut)
        }
      } else {
        stop("xOut must be a matrix or a data frame")
      }
    }
    if (ncol(x) != ncol(xOut)) {
      stop("x and xOut must have the same number of columns")
    }
    m <- nrow(xOut) # number of predictions
  } else {
    m <- 0
  }
  y <- M
  if (!is.list(y)) {
    if (is.array(y)) {
      y <- lapply(seq(dim(y)[3]), function(i) y[, , i])
    } else {
      stop("y must be a list or an array")
    }
  }
  if (nrow(x) != length(y)) {
    stop("the number of rows in x must be the same as the number of correlation matrices in y")
  }
  n <- nrow(x) # number of observations
  p <- ncol(x) # number of covariates
  if (p > 2) {
    stop("local method is designed to work in low dimensional case (p is either 1 or 2)")
  }
  if (!is.na(sum(bw))) {
    if (sum(bw <= 0) > 0) {
      stop("bandwidth must be positive")
    }
    if (length(bw) != p) {
      stop("dimension of bandwidth does not agree with x")
    }
  }
  q <- ncol(y[[1]]) # dimension of the correlation matrix
  yVec <- matrix(unlist(y), ncol = q^2, byrow = TRUE) # n by q^2
  if (substr(metric, 1, 1) == "p") {
    yAlpha <- lapply(y, function(yi) {
      eigenDecom <- eigen(yi)
      Lambda <- pmax(Re(eigenDecom$values), 0) # exclude 0i
      U <- eigenDecom$vectors
      U %*% diag(Lambda^alpha) %*% t(U)
    })
    yAlphaVec <- matrix(unlist(yAlpha), ncol = q^2, byrow = TRUE) # n by q^2
  }
  # define different kernels
  Kern <- kerFctn(kernel)
  K <- function(x, h) {
    k <- 1
    for (i in 1:p) {
      k <- k * Kern(x[i] / h[i])
    }
    return(as.numeric(k))
  }

  # choose bandwidth by cross-validation
  if (is.na(sum(bw))) {
    hs <- matrix(0, p, 20)
    for (l in 1:p) {
      hs[l, ] <- exp(seq(
        from = log(n^(-1 / (1 + p)) * (max(x[, l]) - min(x[, l])) / 10),
        to = log(5 * n^(-1 / (1 + p)) * (max(x[, l]) - min(x[, l]))),
        length.out = 20
      ))
    }
    cv <- array(0, 20^p)
    for (k in 0:(20^p - 1)) {
      h <- array(0, p)
      for (l in 1:p) {
        kl <- floor((k %% (20^l)) / (20^(l - 1))) + 1
        h[l] <- hs[l, kl]
      }
      for (j in 1:n) {
        a <- x[j, ]
        if (p > 1) {
          mu1 <- rowMeans(apply(as.matrix(x[-j, ]), 1, function(xi) K(xi - a, h) * (xi - a)))
          mu2 <- matrix(rowMeans(apply(as.matrix(x[-j, ]), 1, function(xi) K(xi - a, h) * ((xi - a) %*% t(xi - a)))), ncol = p)
        } else {
          mu1 <- mean(apply(as.matrix(x[-j, ]), 1, function(xi) K(xi - a, h) * (xi - a)))
          mu2 <- mean(apply(as.matrix(x[-j, ]), 1, function(xi) K(xi - a, h) * ((xi - a) %*% t(xi - a))))
        }
        skip <- FALSE
        tryCatch(solve(mu2), error = function(e) skip <<- TRUE)
        if (skip) {
          cv[k + 1] <- Inf
          break
        }
        wc <- t(mu1) %*% solve(mu2) # 1 by p
        w <- apply(as.matrix(x[-j, ]), 1, function(xi) {
          K(xi - a, h) * (1 - wc %*% (xi - a))
        }) # weight
        if (substr(metric, 1, 1) == "f") {
          qNew <- apply(yVec[-j, ], 2, weighted.mean, w) # q^2
          fitj <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
          fitj <- (fitj + t(fitj)) / 2 # symmetrize
          if (!is.na(digits)) fitj <- round(fitj, digits = digits) # round
          cv[k + 1] <- cv[k + 1] + sum((y[[j]] - fitj)^2) / n
        } else if (substr(metric, 1, 1) == "p") {
          bAlpha <- matrix(apply(yAlphaVec[-j, ], 2, weighted.mean, w), ncol = q) # q by q
          eigenDecom <- eigen(bAlpha)
          Lambda <- pmax(Re(eigenDecom$values), 0) # projection to M_m
          U <- eigenDecom$vectors
          qNew <- as.vector(U %*% diag(Lambda^(1 / alpha)) %*% t(U)) # inverse power
          fitj <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
          fitj <- (fitj + t(fitj)) / 2 # symmetrize
          if (!is.na(digits)) fitj <- round(fitj, digits = digits) # round
          cv[k + 1] <- cv[k + 1] + sum((y[[j]] - fitj)^2) / n
        }
      }
    }
    bwi <- which.min(cv)
    bw <- array(0, p)
    for (l in 1:p) {
      kl <- floor((bwi %% (20^l)) / (20^(l - 1))) + 1
      bw[l] <- hs[l, kl]
    }
  }

  fit <- vector(mode = "list", length = n)
  residuals <- rep.int(0, n)
  if (substr(metric, 1, 1) == "f") {
    for (i in 1:n) {
      a <- x[i, ]
      if (p > 1) {
        mu1 <- rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
        mu2 <- matrix(rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a)))), ncol = p)
      } else {
        mu1 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
        mu2 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a))))
      }
      wc <- t(mu1) %*% solve(mu2) # 1 by p
      w <- apply(x, 1, function(xi) {
        K(xi - a, bw) * (1 - wc %*% (xi - a))
      }) # weight
      qNew <- apply(yVec, 2, weighted.mean, w) # q^2
      temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
      temp <- (temp + t(temp)) / 2 # symmetrize
      if (!is.na(digits)) temp <- round(temp, digits = digits) # round
      fit[[i]] <- temp
      residuals[i] <- sqrt(sum((y[[i]] - temp)^2))
    }
    if (m > 0) {
      predict <- vector(mode = "list", length = m)
      for (i in 1:m) {
        a <- xOut[i, ]
        if (p > 1) {
          mu1 <- rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
          mu2 <- matrix(rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a)))), ncol = p)
        } else {
          mu1 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
          mu2 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a))))
        }
        wc <- t(mu1) %*% solve(mu2) # 1 by p
        w <- apply(x, 1, function(xi) {
          K(xi - a, bw) * (1 - wc %*% (xi - a))
        }) # weight
        qNew <- apply(yVec, 2, weighted.mean, w) # q^2
        temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
        temp <- (temp + t(temp)) / 2 # symmetrize
        if (!is.na(digits)) temp <- round(temp, digits = digits) # round
        predict[[i]] <- temp
      }
      res <- list(fit = fit, predict = predict, residuals = residuals, xOut = xOut, optns = optns)
    } else {
      res <- list(fit = fit, residuals = residuals, optns = optns)
    }
  } else if (substr(metric, 1, 1) == "p") {
    for (i in 1:n) {
      a <- x[i, ]
      if (p > 1) {
        mu1 <- rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
        mu2 <- matrix(rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a)))), ncol = p)
      } else {
        mu1 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
        mu2 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a))))
      }
      wc <- t(mu1) %*% solve(mu2) # 1 by p
      w <- apply(x, 1, function(xi) {
        K(xi - a, bw) * (1 - wc %*% (xi - a))
      }) # weight
      bAlpha <- matrix(apply(yAlphaVec, 2, weighted.mean, w), ncol = q) # q by q
      eigenDecom <- eigen(bAlpha)
      Lambda <- pmax(Re(eigenDecom$values), 0) # projection to M_m
      U <- eigenDecom$vectors
      qNew <- as.vector(U %*% diag(Lambda^(1 / alpha)) %*% t(U)) # inverse power
      temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
      temp <- (temp + t(temp)) / 2 # symmetrize
      if (!is.na(digits)) temp <- round(temp, digits = digits) # round
      fit[[i]] <- temp
      residuals[i] <- sqrt(sum((y[[i]] - temp)^2))
    }
    if (m > 0) {
      predict <- vector(mode = "list", length = m)
      for (i in 1:m) {
        a <- xOut[i, ]
        if (p > 1) {
          mu1 <- rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
          mu2 <- matrix(rowMeans(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a)))), ncol = p)
        } else {
          mu1 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * (xi - a)))
          mu2 <- mean(apply(x, 1, function(xi) K(xi - a, bw) * ((xi - a) %*% t(xi - a))))
        }
        wc <- t(mu1) %*% solve(mu2) # 1 by p
        w <- apply(x, 1, function(xi) {
          K(xi - a, bw) * (1 - wc %*% (xi - a))
        }) # weight
        bAlpha <- matrix(apply(yAlphaVec, 2, weighted.mean, w), ncol = q) # q^2
        eigenDecom <- eigen(bAlpha)
        Lambda <- pmax(Re(eigenDecom$values), 0) # projection to M_m
        U <- eigenDecom$vectors
        qNew <- as.vector(U %*% diag(Lambda^(1 / alpha)) %*% t(U)) # inverse power
        temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
        temp <- (temp + t(temp)) / 2 # symmetrize
        if (!is.na(digits)) temp <- round(temp, digits = digits) # round
        predict[[i]] <- temp
      }
      res <- list(fit = fit, predict = predict, residuals = residuals, xOut = xOut, optns = optns)
    } else {
      res <- list(fit = fit, residuals = residuals, optns = optns)
    }
  }
  class(res) <- "corReg"
  res
}
#'@title Local Fréchet regression of conditional covariance matrices with power metric
#'@noRd
#'@description Local Fréchet regression of covariance matrices with Euclidean predictors and power metric.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.
#'@param M A q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#'@param xout An m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{alpha}{Non-negative parameter from the power metric. Default is 1 which corresponds to Frobenius metric.}
#' \item{bwMean}{A vector of length p holding the bandwidths for conditional mean estimation if \code{y} is provided. If \code{bwMean} is not provided, it is chosen by cross validation.}
#' \item{bwCov}{A vector of length p holding the bandwidths for conditional covariance estimation. If \code{bwCov} is not provided, it is chosen by cross validation.}
#' \item{kernel}{Name of the kernel function to be chosen from 'gauss', 'rect', 'epan', 'gausvar' and 'quar'. Default is 'gauss'.}
#' }
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance or correlation matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' #Example y input
#' n=200             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Cov_est=LFRCovPower(x=x,y=y,xout=xout,optns=list(alpha=3,corrOut=FALSE))
#'#Example M input
#'n=30 #sample size
#'m=30 #dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-15*diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'x=matrix(rnorm(n),n)
#'xout = matrix(c(0.25,0.5,0.75),3) #output predictor levels
#'Cov_rst=LFRCovPower(x=x,M=M,xout=xout,optns=list(alpha=0,corrOut=FALSE))
#' @references
#' \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' @importFrom Matrix nearPD forceSymmetric

LFRCovPower= function(x,y=NULL,M=NULL, xout,optns = list()){
  if(is.null(optns$corrOut)){
    corrOut=FALSE
  } else{
    corrOut=optns$corrOut
  }
  if(is.null(optns$kernel)){
    kernel = 'gauss'
  } else{
    kernel=optns$kernel
  }
  if(is.null(optns$bwMean)){
    bwMean = NA
  } else{
    bwMean=optns$bwMean
  }
  bw1=bwMean

  if(is.null(optns$bwCov)){
    bwCov=NA
  } else{
    bwCov=optns$bwCov
  }
  bw=bwCov

  if(is.null(optns$alpha)){
    alpha=1
  } else{
    alpha=optns$alpha
  }
  if(alpha<0){
    stop('alpha must be non-negative')
  }
  
  if(!is.matrix(x)&!is.vector(x)){
    stop('x must be a matrix or vector')
  }
  if(is.vector(x)){
    x<- matrix(x,length(x))
  }
  if(is.vector(xout)){
    xout<- matrix(xout,length(xout))
  }
  
  if(!is.matrix(x)){
    stop('x must be a matrix')
  }
  if(!is.matrix(xout)){
    stop('xout must be a matrix')
  }
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have the same number of columns')
  }
  if(!is.na(sum(bw))){
    if(sum(bw<=0)>0){
      stop("bandwidth must be positive")
    }
  }
  p = ncol(x)
  if(p>2){
    stop("The number of dimensions of the Euclidean predictor x must be at most 2")
  }
  m = nrow(xout)

  Kern=kerFctn(kernel)
  K = function(x,h){
    k = 1
    for(i in 1:p){
      k=k*Kern(x[,i]/h[i])
    }
    return(as.numeric(k))
  }

  computeLFR_originalSpace=function(idx,x0,bw2){
    #x0 and bw2 are in R^p
    x=as.matrix(x[idx,])
    aux=K(x-matrix(t(x0),nrow=length(idx),ncol=length(x0),byrow=TRUE),bw2)
    mu0 = mean(aux)
    mu1 = colMeans(aux*(x - matrix(t(x0),nrow=length(idx),ncol=length(x0),byrow=TRUE)))
    mu2=0
    for(i in 1:length(idx)){
      mu2 = mu2 + aux[i]*(x[i,]-x0) %*% t(x[i,]-x0)/length(idx)
    }
    sL = array(0,length(idx))
    for(i in 1:length(idx)){
      sL[i] =aux[i]*(1-t(mu1)%*%solve(mu2)%*%(x[i,]-x0))
    }
    s = sum(sL)

    M_hat=array(0,c(dim(M)[1],dim(M)[1],1))
    if(alpha>0){
      for(i in 1:length(idx)){
        P=eigen(M[,,idx[i]])$vectors
        Lambd_alpha=diag(pmax(0,eigen(M[,,idx[i]])$values)**alpha)
        M_alpha=P%*%Lambd_alpha%*%t(P)
        M_hat[,,1]=M_hat[,,1]+sL[i]*M_alpha/s
      }
      M_hat[,,1]=as.matrix(Matrix::nearPD(M_hat[,,1],corr = FALSE)$mat)
      P=eigen(M_hat[,,1])$vectors
      Lambd_alpha=diag(pmax(0,eigen(M_hat[,,1])$values)**(1/alpha))
      M_hat[,,1]=P%*%Lambd_alpha%*%t(P)
      M_hat[,,1]=as.matrix(Matrix::forceSymmetric(M_hat[,,1]))
    } else{
      for(i in 1:length(idx)){
        P=eigen(M[,,idx[i]])$vectors
        Lambd_alpha=diag(log(pmax(1e-30,eigen(M[,,idx[i]])$values)))
        M_alpha=P%*%Lambd_alpha%*%t(P)
        M_hat[,,1]=M_hat[,,1]+sL[i]*M_alpha/s
      }
      M_hat[,,1]=as.matrix(Matrix::nearPD(M_hat[,,1],corr = FALSE)$mat)
      P=eigen(M_hat[,,1])$vectors
      Lambd_alpha=diag(exp(pmax(0,eigen(M_hat[,,1])$values)))
      M_hat[,,1]=P%*%Lambd_alpha%*%t(P)
      M_hat[,,1]=as.matrix(Matrix::forceSymmetric(M_hat[,,1]))
    }
    M_hat[,,1]
  }

  if(!is.null(y)){
    if(!is.matrix(y)){
      stop('y must be a matrix')
    }
    if(nrow(x) != nrow(y)){
      stop('x and y must have the same number of rows')
    }
    n = nrow(y)
    nGrid = ncol(y)
    cm = mean4LocCovReg(x=x,y=y,xout=x,optns=list(bwMean = bw1))
    bw1 = cm$optns$bwMean
    cmh = cm$mean_out

    M=array(0,c(dim(y)[2], dim(y)[2], dim(y)[1]))
    for(i in 1:n){
      M[,,i] = (y[i,] - cmh[i,]) %*% t(y[i,] - cmh[i,])
    }
  } else{
    if(is.null(M)){
      stop("y or M must be provided")
    }
    if(is.list(M)){
      M=array(as.numeric(unlist(M)), dim=c(dim(M[[1]])[1],dim(M[[1]])[1],length(M)))
    } else{
      if(!is.array(M)){
        stop('M must be an array or a list')
      } else if (length(dim(M))!=3) {
        stop('M must be an array or a list')
      }
    }
    if(nrow(x)!=dim(M)[3]){
      stop("The number of rows of x must be the same as the number of covariance matrices in M")
    }

    n=dim(M)[3]
  }
  #CV for bandwidth bw selection
  if(is.na(sum(bw))){
    if(p==1){
      bw_choice=SetBwRange(as.vector(x), as.vector(xout), kernel)
      objF=matrix(0,nrow=20,ncol=1)
      aux1=as.matrix(seq(bw_choice$min,bw_choice$max,length.out=nrow(objF)))
      for(i in 1:length(aux1)){
        #Try-catch statement in case bandwidth is too small and produces numerical issues
        objF[i] = tryCatch({
          sum(sapply(1:dim(x)[1],function(j){
            aux=computeLFR_originalSpace(setdiff(1:dim(x)[1],j),x[j],aux1[i])-M[,,j]
            sum(diag(aux%*%t(aux)))
          }))
        }, error = function(e) {
          return(NA)
        })
      }
      if(sum(is.na(objF))==dim(objF)[1]){
        stop("Bandwidth too small in cross-validation search")
      }
      ind=which(objF==min(objF,na.rm=TRUE))[1]
      bwCV=aux1[ind]
    }
    if(p==2){
      bw_choice1=SetBwRange(as.vector(x[,1]), as.vector(xout[,1]), kernel)
      bw_choice2=SetBwRange(as.vector(x[,2]), as.vector(xout[,2]), kernel)
      if(n<=30){
        objF=matrix(0,nrow=6,ncol=6)
        aux1=seq(bw_choice1$min,bw_choice1$max,length.out=6)
        aux2=seq(bw_choice2$min,bw_choice2$max,length.out=6)
        for(i1 in 1:nrow(objF)){
          for(i2 in 1:ncol(objF)){
            #Try-catch statement in case bandwidth is too small and produces numerical issues
            objF[i1,i2] = tryCatch({
              sum(sapply(1:dim(x)[1],function(j){
                aux=computeLFR_originalSpace(setdiff(1:dim(x)[1],j),x[j,],c(aux1[i1],aux2[i2]))-M[,,j]
                sum(diag(aux%*%t(aux)))
              }))
            }, error = function(e) {
              return(NA)
            })
          }
        }
        if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
          stop("Bandwidth too small in cross-validation search")
        }else{
          ind=which(objF==min(objF,na.rm=TRUE),arr.ind = TRUE)
          bwCV=c(aux1[ind[1]],aux2[ind[2]])
        }
      } else{
        randIndices=sample(dim(x)[1])
        groupIndices=cut(seq(1,dim(x)[1]),breaks=10,labels=FALSE)
        cv10fold_compute=function(v,leaveIn){
          aux=computeLFR_originalSpace(leaveIn,x[v,],c(aux1[i1],aux2[i2]))-M[,,v]
          sum(diag(aux%*%t(aux)))
        }
        objF=matrix(0,nrow=6,ncol=6)
        aux1=seq(bw_choice1$min,bw_choice1$max,length.out=6)
        aux2=seq(bw_choice2$min,bw_choice2$max,length.out=6)
        for(i1 in 1:nrow(objF)){
          for(i2 in 1:ncol(objF)){
            #Try-catch statement in case bandwidth is too small and produces numerical issues
            objF[i1,i2] = tryCatch({
              sum(sapply(1:10,function(j){
                leaveIn=setdiff(1:(dim(x)[1]),randIndices[groupIndices==j])
                sum(sapply(randIndices[groupIndices==j],function(v){cv10fold_compute(v,leaveIn)}))
              }))
            }, error = function(e) {
              return(NA)
            })
          }
        }
        if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
          stop("Bandwidth too small in cross-validation search")
        }else{
          ind=which(objF==min(objF,na.rm=TRUE),arr.ind = TRUE)
          bwCV=c(aux1[ind[1]],aux2[ind[2]])
        }
      }
    }
    bw=bwCV
  }
  Mout = list()
  if(corrOut){
    for(j in 1:m){
      x0 = xout[j,]
      aux=computeLFR_originalSpace(1:dim(x)[1],x0,bw)
      D=diag(1/sqrt(diag(aux)))
      aux=D%*%aux%*%D
      aux=as.matrix(Matrix::forceSymmetric(aux))
      Mout = c(Mout,list(aux))
    }
  } else{
    for(j in 1:m){
      x0 = xout[j,]
      Mout = c(Mout,list(computeLFR_originalSpace(1:dim(x)[1],x0,bw)))
    }
  }
  optns$corrOut=corrOut
  optns$kernel=kernel
  optns$bwMean=bw1
  optns$bwCov=bw
  return(list(xout=xout, Mout=Mout, optns=optns))
}

kerFctn <- function(kernel_type){
  if (kernel_type=='gauss'){
    ker <- function(x){
      dnorm(x) #exp(-x^2 / 2) / sqrt(2*pi)
    }
  } else if(kernel_type=='rect'){
    ker <- function(x){
      as.numeric((x<=1) & (x>=-1))
    }
  } else if(kernel_type=='epan'){
    ker <- function(x){
      n <- 1
      (2*n+1) / (4*n) * (1-x^(2*n)) * (abs(x)<=1)
    }
  } else if(kernel_type=='gausvar'){
    ker <- function(x) {
      dnorm(x)*(1.25-0.25*x^2)
    }
  } else if(kernel_type=='quar'){
    ker <- function(x) {
      (15/16)*(1-x^2)^2 * (abs(x)<=1)
    }
  } else {
    stop('Unavailable kernel')
  }
  return(ker)
}
#' @title Local Cox point process regression.
#' @description Local Fréchet regression for replicated Cox point processes with respect to \eqn{L^2}-Wasserstein distance on shape space and Euclidean 2-norm on intensity factor space.
#' @param xin An n by p matrix with input measurements of the predictors, where p is at most 2.
#' @param tin A list holding the sample of event times of each replicated point process, where the ith element of the list \code{tin} holds the event times of the point process corresponding to the ith row of \code{xin}.
#' @param T0 A positive scalar that defines the time window [0,\code{T0}] where the replicated Cox point processes are observed.
#' @param xout A k by p matrix with output measurements of the predictors. Default is \code{xin}.
#' @param optns A list of control parameters specified by \code{list(name=value)}.
#' @details Available control options are \code{bwDen}, \code{kernelReg} (see \code{\link{LocDenReg}} for these option descriptions) and
#' \describe{
#' \item{L}{Upper Lipschitz constant for quantile space; numeric -default: 1e10.}
#' \item{M}{Lower Lipschitz constant for quantile space; numeric -default: 1e-10.}
#' \item{dSup}{User defined output grid for the support of kernel density estimation used in \code{CreateDensity()} for mapping from quantile space to shape space. This grid must be in [0,\code{T0}]. Default is an equidistant with \code{nqSup}+2 points.}
#' \item{nqSup}{A scalar with the number of equidistant points in (0,1) used to obtain the empirical quantile function from each point process. Default: 500.}
#' \item{bwReg}{A vector of length p used as the bandwidth for the Fréchet regression or \code{"CV"} (default), i.e., a data-adaptive selection done by leave-one-out cross-validation.}
#' }
#' @return A list containing the following components:
#' \item{xout}{Input \code{xout}.}
#' \item{dSup}{Support of each estimated (up to a constant) conditional intensity regression function in the columns of \code{intensityReg}.}
#' \item{intensityReg}{A matrix of dimension \code{length(dSup)} by \code{nrow(xout)} holding the estimated intensity regression functions up to a constant over the support grid \code{dSup}, where each column corresponds to a predictor level in the corresponding row of \code{xout}.}
#' \item{xin}{Input \code{xin}.}
#' \item{optns}{A list of control options used.}
#' @examples
#' \donttest{
#' n=100
#' alpha_n=sqrt(n)
#' alpha1=2.0
#' beta1=1.0
#' gridQ=seq(0,1,length.out=500+2)[2:(500+1)]
#' X=runif(n,0,1)#p=1
#' tau=matrix(0,nrow=n,ncol=1)
#' for(i in 1:n){
#'   tau[i]=alpha1+beta1*X[i]+truncnorm::rtruncnorm(1, a=-0.3, b=0.3, mean = 0, sd = 1.0)
#' }
#' Ni_n=matrix(0,nrow=n,ncol=1)
#' u0=0.4
#' u1=0.5
#' u2=0.05
#' u3=-0.01
#' tin=list()
#' for(i in 1:n){
#'   Ni_n[i]=rpois(1,alpha_n*tau[i])
#'   mu_x=u0+u1*X[i]+truncnorm::rtruncnorm(1,a=-0.1,b=0.1,mean=0,sd=1)
#'   sd_x=u2+u3*X[i]+truncnorm::rtruncnorm(1,a=-0.02,b=0.02,mean=0,sd=0.5)
#'   if(Ni_n[i]==0){
#'     tin[[i]]=c()
#'   }else{
#'     tin[[i]]=truncnorm::rtruncnorm(Ni_n[i],a=0,b=1,mean=mu_x,sd=sd_x) #Sample from truncated normal
#'   }
#' }
#' res=LocPointPrReg(
#'   xin=matrix(X,ncol=1),
#'   tin=tin,T0=1,xout=matrix(seq(0,1,length.out=10),ncol=1),
#'   optns=list(bwDen=0.1,bwReg=0.1)
#' )
#' }
#' @references
#' \cite{Petersen, A., & Müller, H.-G. (2019). "Fréchet regression for random objects with Euclidean predictors." The Annals of Statistics, 47(2), 691--719.}
#' 
#' \cite{Gajardo, Á. and Müller, H.-G. (2022). "Cox Point Process Regression." IEEE Transactions on Information Theory, 68(2), 1133-1156.}
#' @importFrom quadprog solve.QP
#' @export

LocPointPrReg <- function(xin=NULL, tin=NULL,T0=NULL, xout=NULL, optns=list()) {
  
  if(is.null(xin)){
    stop("xin has no default and must be input")
  }
  if(!is.matrix(xin)){
    stop("xin must be a matrix")
  }
  if(is.null(xout)){
    xout=xin
  }
  if(!is.matrix(xout)){
    stop("xout must be a matrix")
  }
  if(ncol(xin)!=ncol(xout)){
    stop("xin and xout must have same number of columns")
  }
  if(is.null(tin)){
    stop("tin has no default and must be input")
  }
  if(is.null(T0)){
    stop("T0 has no default and must be input")
  }
  if(T0<max(unlist(tin))){
    stop("T0 cannot be smaller than any event time")
  }
  if(is.null(optns$L)){
    optns$L=1e10
  }
  if(is.null(optns$M)){
    optns$M=1e-10
  }
  if(optns$L<0 | optns$M<0 | optns$L<optns$M){
    stop("L and M must be positive with L>M")
  }
  if(is.null(xout)){
    xout <- xin
  }
  if(is.vector(xin)){
    xin = as.matrix(xin)
  }
  if(!is.null(optns$bwDen)){
    if(sum(!is.numeric(optns$bwDen) | !length(optns$bwDen)==1 | !(optns$bwDen>0))>0){
      stop("bwDen option must be a positive scalar")
    }
    optns$userBwMu=optns$bwDen
  }
  if(is.null(optns$nqSup)){
    optns$nqSup=500
  }else{
    if(sum(!is.numeric(optns$nqSup) | !length(optns$nqSup)==1 | !(optns$nqSup>0))>0){
      stop("nqSup must be a positive integer")
    }
    if(optns$nqSup<100){
      warning("nqSup option may be too small")
    }
  }
  
  
  if(!is.null(optns$bwReg)){
    if(optns$bwReg!="CV"){
      if(!is.numeric(optns$bwReg) | length(optns$bwReg)!=ncol(xin)){
        stop("optns$bwReg must be a vector of length p or 'CV'")
      }
    }
  }else{
    optns$bwReg="CV"
  }
  
  if(!is.null(optns$dSup)){
    if(optns$dSup[1]!=0 | optns$dSup[length(optns$dSup)]!=T0){
      stop("dSup must be a vector with endpoints 0 and T0")
    }
  }
  
  n=nrow(xin)
  p=ncol(xin)
  
  if(p>2){
    stop("p must be at most 2")
  }
  
  gridQ=seq(0,1,length.out=optns$nqSup+2)[2:(optns$nqSup+1)]
  Qi_hat=matrix(0,nrow=n,ncol=length(gridQ))
  m = ncol(Qi_hat)
  Ni_n=matrix(0,nrow=n,ncol=1)
  for(i in 1:n){
    Ni_n[i]=length(tin[[i]])
    if(Ni_n[i]==0){
      Qi_hat[i,]=gridQ #uniform quantile distribution function if there are no points
    }else{
      Qi_hat[i,]=as.vector(quantile(tin[[i]],gridQ))
    }
  }
  
  if(is.null(optns$kernelReg)){
    optns$kernelReg="gauss"
  }
  
  ker <- kerFctn(optns$kernelReg)
  
  K = function(x,h){
    k = 1
    for(i in 1:p){
      k=k*ker(x[,i]/h[i])
    }
    return(as.numeric(k))
  }
  
  getNplus=function(x_predict,N,X,bw){
    #bw is a vector of bandwidths in R^p
    nn=nrow(X)
    
    K = function(x,h){
      k = 1
      for(i in 1:p){
        k=k*ker(x[,i]/h[i])
      }
      return(as.numeric(k))
    }
    
    getLFRweights=function(x0){
      #x0 is a vector in R^p that corresponds to the covariate value at which we want to predict
      aux=K(X-matrix(t(x0),nrow=nn,ncol=length(x0),byrow=TRUE),bw)
      mu0 = mean(aux)
      mu1 = colMeans(aux*(X - matrix(t(x0),nrow=nn,ncol=length(x0),byrow=TRUE)))
      mu2=0
      for(i in 1:nn){
        mu2 = mu2 + aux[i]*(X[i,]-x0) %*% t(X[i,]-x0)/nn
      }
      sL = array(0,nn)
      for(i in 1:nn){
        sL[i] =aux[i]*(1-t(mu1)%*%solve(mu2)%*%(X[i,]-x0))
      }
      s = sum(sL)
      return(sL/s)
    }
    N_plus=max((t(N)%*%getLFRweights(x_predict))/mean(N),0)
    N_plus
  }
  
  getLFRweights=function(x0,X,bw){
    #x0 is a vector in R^p that corresponds to the covariate value at which we want to predict
    nn=nrow(X)
    aux=K(X-matrix(t(x0),nrow=nn,ncol=length(x0),byrow=TRUE),bw)
    mu0 = mean(aux)
    mu1 = colMeans(aux*(X - matrix(t(x0),nrow=nn,ncol=length(x0),byrow=TRUE)))
    mu2=0
    for(i in 1:nn){
      mu2 = mu2 + aux[i]*(X[i,]-x0) %*% t(X[i,]-x0)/nn
    }
    sL = array(0,nn)
    for(i in 1:nn){
      sL[i] =aux[i]*(1-t(mu1)%*%solve(mu2)%*%(X[i,]-x0))
    }
    s = sum(sL)
    return(sL/s)
  }
  
  lower=0
  upper=T0
  
  compute_LFR=function(xOut,X,bw,Qhat){
    #xOut is a matrix nn by p
    #Computes local Fréchet regression function on quantile space
    nn=nrow(X)
    k = nrow(xOut)
    A = cbind(diag(m), rep(0,m)) + cbind(rep(0,m), -diag(m))
    A=A[,c(1,ncol(A),2:(ncol(A)-1))]
    A=cbind(A,-A[,3:ncol(A)])
    A=cbind(c(-1,rep(0,m-1)),c(rep(0,m-1),1),c(rep(0,m-1),-1),A)
    b0 = c(-optns$L/(m+1),1-optns$L/(m+1),optns$M/(m+1)-1,optns$M/(m+1),-T0,rep(optns$M/(m+1),m-1))
    b0=c(b0,-rep(optns$L/(m+1),m-1))
    
    quantile_LFR = t(sapply(1:k, function(j){
      s=getLFRweights(xOut[j,],X,bw)
      s=as.vector(s)
      gx <- (s %*% Qhat)
      res = do.call(quadprog::solve.QP, list(diag(m), gx, A, b0))
      return(sort(res$solution))
    })) #each row contains the Local Frechet regression function in quantile space at xOut[j,]
    
    quantile_LFR
  }
  
  
  if(optns$bwReg=="CV"){
    if(p==1){
      aux=SetBwRange(xin = xin[,1], xout = xout[,1], kernel_type = optns$ker)
      bwRangeBounds <- matrix(c(aux$min,aux$max),nrow=2,ncol=1)
      bw_range=seq(bwRangeBounds[1,1],bwRangeBounds[2,1],length.out=20)
      
      error_cv=sapply(1:length(bw_range),function(j){
        Q_out=sapply(1:n,function(i){
          as.vector(compute_LFR(matrix(xin[i,],ncol=p),matrix(xin[setdiff(1:n,i),],ncol=p,byrow=TRUE),matrix(bw_range[j],nrow=p),Qi_hat[setdiff(1:n,i),]))
        })#predicted leave-one-out quantile functions at each predictor level are on the columns of Q_out
        
        L2_Q=colMeans((Q_out-t(Qi_hat))^2)*diff(gridQ)[1]#squared L2 error for predicted quantile functions
        
        tau_out=sapply(1:n,function(i){
          getNplus(matrix(xin[i,],nrow=p),Ni_n[setdiff(1:n,i)],matrix(xin[setdiff(1:n,i),],ncol=p,byrow=TRUE),matrix(bw_range[j],nrow=p))
        })
        
        E2_tau=(tau_out-Ni_n/mean(Ni_n))^2
        
        mean(L2_Q+E2_tau)
      })
      optns$bwReg=bw_range[which(error_cv==min(error_cv))[1]]
    }else{
      aux=SetBwRange(xin = xin[,1], xout = xout[,1], kernel_type = optns$ker)
      aux2=SetBwRange(xin = xin[,2], xout = xout[,2], kernel_type = optns$ker)
      bwRangeBounds <- cbind(c(aux$min,aux$max),c(aux2$min,aux2$max))
      bw_range1=seq(bwRangeBounds[1,1],bwRangeBounds[2,1],length.out=5)
      bw_range2=seq(bwRangeBounds[1,2],bwRangeBounds[2,2],length.out=5)
      
      error_cv=matrix(nrow=length(bw_range1),ncol=length(bw_range2))
      for(l in 1:length(bw_range1)){
        error_cv[l,]=sapply(1:length(bw_range2),function(j){
          tryCatch({
            Q_out=sapply(1:n,function(i){
              as.vector(compute_LFR(matrix(xin[i,],ncol=p),matrix(xin[setdiff(1:n,i),],ncol=p,byrow=TRUE),matrix(c(bw_range1[l],bw_range2[j]),nrow=p),Qi_hat[setdiff(1:n,i),]))
            })#predicted leave-one-out quantile functions at each predictor level are on the columns of Q_out
            L2_Q=colMeans((Q_out-t(Qi_hat))^2)*diff(gridQ)[1]#squared L2 error for predicted quantile functions
            
            tau_out=sapply(1:n,function(i){
              getNplus(matrix(xin[i,],nrow=p),Ni_n[setdiff(1:n,i)],matrix(xin[setdiff(1:n,i),],ncol=p,byrow=TRUE),matrix(c(bw_range1[l],bw_range2[j]),nrow=p))
            })
            
            E2_tau=(tau_out-Ni_n/mean(Ni_n))^2
            mean(L2_Q+E2_tau)
          }, error = function(e) {
            return(NA)
          })
        })
      }
      
      if(sum(is.na(error_cv))==dim(error_cv)[1]*dim(error_cv)[2]){
        stop("Bandwidth too small in cross-validation search")
      }else{
        ind=which(error_cv==min(error_cv,na.rm=TRUE),arr.ind = TRUE)
        optns$bwReg=c(bw_range1[ind[1]],bw_range2[ind[2]])
      }
    }
    matrix(optns$bwReg,nrow=p)
  }
  
  if(is.null(optns$dSup)){
    density_grid=as.vector(c(0,gridQ,T0))
  }else{
    density_grid=optns$dSup
    optns$dSup=NULL
  }
  
  quantile_GFR=compute_LFR(xout,xin,optns$bwReg,Qi_hat)
  
  k = nrow(xout)
  
  intensityReg=sapply(1:k,function(j){
    qf2pdf(qf=as.vector(c(0,quantile_GFR[j,],T0)),prob=as.vector(c(0,gridQ,1)),optns=list(outputGrid=density_grid,infSupport=FALSE,userBwMu=optns$bwDen))$y*getNplus(xout[j,],Ni_n,xin,optns$bwReg)
  })
  
  res <- list(xout = xout, dSup = density_grid, intensityReg = intensityReg, xin=xin, optns=optns)
  
  class(res) <- "PointPrReg"
  return(res)
}

#'@title \eqn{L^2} Wasserstein distance between two distributions.
#'@param d1,d2 Lists holding the density functions or quantile functions of the two distributions.
#' Each list consists of two numeric vectors \code{x} and \code{y} of the same length,
#' where \code{x} holds the support grid and \code{y} holds the values of the function.
#' Note that the type of functions representing the distributions in \code{d1} and \code{d2}
#' should be the same---either both are density functions, or both are quantile functions. 
#' If both are quantile functions, all elements in \code{d1$x} and \code{d2$x} must be between 0 and 1.
#' \code{d1$x} and \code{d2$x} may have different lengths. 
#'@param fctn_type Character vector of length 1 holding the function type in \code{d1} and \code{d2} 
#' representing the distributions: \code{"density"} (default), \code{"quantile"}.
#'@param optns A list of control parameters specified by \code{list(name=value)}.
#' @details Available control options are:
#' \describe{
#' \item{nqSup}{A scalar giving the length of the support grid of quantile functions based on which the \eqn{L^2} Wasserstein distance (i.e., the \eqn{L^2} distance between the quantile functions) is computed. Default is 201.}
#' }
#'@return A scalar holding the \eqn{L^2} Wasserstein distance between \code{d1} and \code{d2}.
#'@examples
#' d1 <- list(x = seq(-6,6,0.01))
#' d1$y <- dnorm(d1$x)
#' d2 <- list(x = d1$x + 1)
#' d2$y <- dnorm(d2$x, mean = 1)
#' dist <- dist4den(d1 = d1,d2 = d2)
#'@export
#'@importFrom fdadensity dens2quantile
#'@importFrom pracma trapz

dist4den <- function(d1 = NULL, d2 = NULL, fctn_type = NULL, optns = list()) {
  tol <- 1e-5
  if (is.null(d1) | is.null(d2)) {
    stop("Requires the input of both d1 and d2.")
  }
  if (is.null(fctn_type)) {
    fctn_type <- "density"
  }
  if (length(fctn_type) > 1) {
    fctn_type <- fctn_type[1]
    warning("fctn_type has length greater than 1---only the first element is used.")
  }
  if (!fctn_type %in% c("density","quantile")) {
    stop("Unrecognized value of fctn_type.")
  }
  if (!is.list(d1) | !is.list(d2)) {
    stop("d1 and d2 should be lists.")
  }
  if (!all(c("x", "y") %in% names(d1))) {
    stop("d1 should consist of two elements x and y.")
  }
  if (!all(c("x", "y") %in% names(d2))) {
    stop("d2 should consist of two elements x and y.")
  }
  if (abs(length(d1$x) - length(d1$y)) > 0) {
    stop("d1$x and d1$y should have the same length.")
  }
  if (abs(length(d2$x) - length(d2$y)) > 0) {
    stop("d2$x and d2$y should have the same length.")
  }
  
  if (is.null(optns$nqSup)) {
    optns$nqSup <- 201
  }
  nqSup <- optns$nqSup
  
  if (fctn_type == "density") {
    if (any(d1$y < 0) | abs(pracma::trapz(d1$x,d1$y) - 1) > tol) {
      stop("d1 should be a density function, i.e., it does not integrate to 1 with tolerance of ",tol,", or d1$y is not all non-negative.")
    }
    if (any(d2$y < 0) | abs(pracma::trapz(d2$x,d2$y) - 1) > tol) {
      stop("d2 should be a density function, i.e., it does not integrate to 1 with tolerance of ",tol,", or d2$y is not all non-negative.")
    }
    qSup <- seq(0,1,length.out = nqSup)
    q1 <- fdadensity::dens2quantile(d1$y, dSup = d1$x, qSup = qSup)
    q2 <- fdadensity::dens2quantile(d2$y, dSup = d2$x, qSup = qSup)
  } else if (fctn_type == "quantile") {
    if (any(d1$x < 0 | d1$x > 1)) {
      stop("Some elements in d1$x do not lie in [0,1].")
    }
    if (any(d2$x < 0 | d2$x > 1)) {
      stop("Some elements in d2$x do not lie in [0,1].")
    }
    if (is.unsorted(d1$x)) {
      d1$y <- d1$y[order(d1$x)]
      d1$x <- sort(d1$x)
    }
    if (is.unsorted(d2$x)) {
      d2$y <- d2$y[order(d2$x)]
      d2$x <- sort(d2$x)
    }
    if (is.unsorted(d1$y) | is.unsorted(d2$y)) {
      stop("Quantile functions given in d1 and d2 are not monotonic.")
    } else {
      if (any(diff(d1$y)<0)) {
        len <- length(d1$x)
        d1$x <- d1$x[len:1]
        d1$y <- d1$y[len:1]
      }
      if (any(diff(d2$y)<0)) {
        len <- length(d2$x)
        d2$x <- d2$x[len:1]
        d2$y <- d2$y[len:1]
      }
    }
    
    diffSupp <- FALSE
    if (abs(length(d1$x) - length(d2$x)) > 0) {
      diffSupp <- TRUE
    } else if (sum(abs(d1$x - d2$x)) > 0) {
      diffSupp <- TRUE
    }
    if (diffSupp) {
      qSup <- seq(0,1,length.out = nqSup)
      q1 <- approx(x = d1$x, y = d1$y, xout = qSup)$y
      q2 <- approx(x = d2$x, y = d2$y, xout = qSup)$y
    } else {
      qSup <- d1$x
      q1 <- d1$y
      q2 <- d2$y
    }
  }
  sqrt(pracma::trapz(qSup, (q1 - q2)^2))
}
#'@title Local Fréchet regression of conditional covariance matrices with Frobenius metric
#'@noRd
#'@description Local Fréchet regression of covariance matrices with Euclidean predictors and Frobenius metric.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.
#'@param M A q by q by n array (resp. list) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#'@param xout An m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{bwMean}{A vector of length p holding the bandwidths for conditional mean estimation if \code{y} is provided. If \code{bwMean} is not provided, it is chosen by cross validation.}
#' \item{bwCov}{A vector of length p holding the bandwidths for conditional covariance estimation. If \code{bwCov} is not provided, it is chosen by cross validation.}
#' \item{kernel}{Name of the kernel function to be chosen from 'gauss', 'rect', 'epan', 'gausvar' and 'quar'. Default is 'gauss'.}
#' }
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance or correlation matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' \donttest{
#'### Example y input
#'n=120             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout=matrix(c(0.25,0.5,0.75),3)
#'Cov_rst = LFRCov(x=x,y=y,xout=xout,optns=list(corrOut=FALSE,bwMean=1.5))
#'### Example M input
#'n=30 #sample size
#'m=30 # dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-15*diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'x=matrix(rnorm(n),n)
#'xout = matrix(c(0.25,0.5,0.75),3) #output predictor levels
#'Cov_rst=LFRCov(x=x,M=M,xout=xout,optns=list(corrOut=FALSE,bwCov=2))
#'}
#' @references
#' \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' @importFrom Matrix nearPD forceSymmetric
#' 
LFRCov  = function(x, y=NULL,M=NULL, xout,optns = list()){
  if(is.null(optns$corrOut)){
    corrOut=FALSE
  } else{
    corrOut=optns$corrOut
  }
  if(is.null(optns$kernel)){
    kernel = 'gauss'
  } else{
    kernel=optns$kernel
  }
  if(is.null(optns$bwMean)){
    bwMean = NA
  } else{
    bwMean=optns$bwMean
  }
  if(is.null(optns$bwCov)){
    bwCov=NA
  } else{
    bwCov=optns$bwCov
  }
  bw2=bwCov
  
  if(!is.matrix(x)&!is.vector(x)){
    stop('x must be a matrix or vector')
  }
  if(is.vector(x)){
    x<- matrix(x,length(x))
  }
  if(is.vector(xout)){
    xout<- matrix(xout,length(xout))
  }

  if(!is.matrix(x)){
    stop('x must be a matrix')
  }
  if(!is.matrix(xout)){
    stop('xout must be a matrix')
  }
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have the same number of columns')
  }
  p = ncol(x)
  if(p > 2){
    stop("The number of dimensions of the predictor x is greater than 2.")
  }
  m = nrow(xout)

  Kern=kerFctn(kernel)
  K = function(x,h){
    k = 1
    for(i in 1:p){
      k=k*Kern(x[,i]/h[i])
    }
    return(as.numeric(k))
  }

  computeLFR=function(idx,x0,bw2){
    #x0 and bw2 are in R^p
    x=as.matrix(x[idx,])
    aux=K(x-matrix(t(x0),nrow=length(idx),ncol=length(x0),byrow=TRUE),bw2)
    mu0 = mean(aux)
    mu1 = colMeans(aux*(x - matrix(t(x0),nrow=length(idx),ncol=length(x0),byrow=TRUE)))
    mu2=0
    for(i in 1:length(idx)){
      mu2 = mu2 + aux[i]*(x[i,]-x0) %*% t(x[i,]-x0)/length(idx)
    }
    sL = array(0,length(idx))
    for(i in 1:length(idx)){
      sL[i] =aux[i]*(1-t(mu1)%*%solve(mu2)%*%(x[i,]-x0))
    }
    s = sum(sL)

    M_aux=array(0,c(dim(M)[1],dim(M)[1],1))
    for(i in 1:length(idx)){
      M_aux[,,1]=M_aux[,,1]+sL[i]*M[,,idx[i]]/s
    }
    M_aux[,,1]
  }

  if(!is.null(y)){
    if(!is.matrix(y)){
      stop('y must be a matrix')
    }
    if(nrow(x) != nrow(y)){
      stop('x and y must have the same number of rows')
    }
    n = nrow(y)
    nGrid = ncol(y)
    cm = mean4LocCovReg(x=x,y=y,xout=x,list(bwMean = bwMean))
    bwMean = cm$optns$bwMean
    cmh = cm$mean_out

    M=array(0,c(dim(y)[2], dim(y)[2], dim(y)[1]))
    for(i in 1:n){
      M[,,i] = (y[i,] - cmh[i,]) %*% t(y[i,] - cmh[i,])
    }
    
  } else{
    if(!is.null(M)){
      if(is.list(M)){
        M=array(as.numeric(unlist(M)), dim=c(dim(M[[1]])[1],dim(M[[1]])[1],length(M)))
      }else{
        if(!is.array(M)){
          stop('M must be an array or a list')
        } else if (length(dim(M))!=3) {
          stop('M must be an array or a list')
        }
      }
      if(nrow(x)!=dim(M)[3]){
        stop("The number of rows of x must be the same as the number of covariance matrices in M")
      }
      
    } else{
      stop("y or M must be provided.")
    }
  }
  
  #CV for bandwidth bw2 selection
  if(is.na(sum(bw2))){
    if(p==1){
      bw_choice=SetBwRange(as.vector(x), as.vector(xout), kernel)
      objF=matrix(0,nrow=20,ncol=1)
      aux1=as.matrix(seq(bw_choice$min,bw_choice$max,length.out=nrow(objF)))
      for(i in 1:length(aux1)){
        #Try-catch statement in case bandwidth is too small and produces numerical issues
        objF[i] = tryCatch({
          sum(sapply(1:dim(x)[1],function(j){
            aux=as.matrix(Matrix::nearPD(computeLFR(setdiff(1:dim(x)[1],j),x[j],aux1[i]),corr = FALSE)$mat)-M[,,j]
            sum(diag(aux%*%t(aux)))
          }))
        }, error = function(e) {
          return(NA)
        })
      }
      if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
        stop("Bandwidth too small in cross-validation search")
      }else{
        ind=which(objF==min(objF,na.rm=TRUE))[1]
        bwCV=aux1[ind]
      }
    }
    if(p==2){
      bw_choice1=SetBwRange(as.vector(x[,1]), as.vector(xout[,1]), kernel)
      bw_choice2=SetBwRange(as.vector(x[,2]), as.vector(xout[,2]), kernel)
      objF=matrix(0,nrow=5,ncol=5)
      aux1=seq(bw_choice1$min,bw_choice1$max,length.out=nrow(objF))
      aux2=seq(bw_choice2$min,bw_choice2$max,length.out=ncol(objF))
      for(i1 in 1:length(aux1)){
        for(i2 in 1:length(aux2)){
          #Try-catch statement in case bandwidth is too small and produces numerical issues
          objF[i1,i2] = tryCatch({
            sum(sapply(1:dim(x)[1],function(j){
              aux=as.matrix(Matrix::nearPD(computeLFR(setdiff(1:dim(x)[1],j),x[j,],c(aux1[i1],aux2[i2])),corr = FALSE)$mat)-M[,,j]
              sum(diag(aux%*%t(aux)))
            }))
          }, error = function(e) {
            return(NA)
          })
        }
      }
      if(sum(is.na(objF))==dim(objF)[1]*dim(objF)[2]){
        stop("Bandwidth too small in cross-validation search")
      }else{
        ind=which(objF==min(objF,na.rm=TRUE),arr.ind = TRUE)
        bwCV=c(aux1[ind[1]],aux2[ind[2]])
      }
    }
    bw2=bwCV
  }

  Mout = list()
  if(corrOut){
    for(j in 1:m){
      x0 = xout[j,]
      aux=as.matrix(Matrix::nearPD(computeLFR(1:dim(x)[1],x0,bw2),corr=FALSE)$mat)
      D=diag(1/sqrt(diag(aux)))
      aux=D%*%aux%*%D
      aux=as.matrix(Matrix::forceSymmetric(aux))
      Mout = c(Mout,list(aux))
    }
  } else{
    for(j in 1:m){
      x0 = xout[j,]
      Mout = c(Mout,list(as.matrix(Matrix::nearPD(computeLFR(1:dim(x)[1],x0,bw2),corr = FALSE)$mat)))
    }
  }
  optns$corrOut=corrOut
  optns$kernel=kernel
  optns$bwMean=bwMean
  optns$bwCov=bw2
  return(list(xout=xout, Mout=Mout, optns=optns))
}






#'@title Plot of a single covariance matrix.
#'@noRd
#'@param mout A covariance or correlation matrix.
#' @param optns A list of optns control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control optns are
#' \describe{
#' \item{plot.type}{Character with two choices, "continuous" and "categorical".
#' The former plots the correlations in a continuous scale of colors by magnitude
#' while the latter categorizes the positive and negative entries into two different colors.
#' Default is "continuous"}
#' \item{plot.clust}{Character, the ordering method of the correlation matrix.
#' "original" for original order (default);
#' "AOE" for the angular order of the eigenvectors;
#' "FPC" for the first principal component order;
#' "hclust" for the hierarchical clustering order, drawing 4 rectangles on the graph according to the hierarchical cluster;
#' "alphabet" for the alphabetical order.}
#' \item{plot.method}{Character, the visualization method of correlation matrix to be used.
#' Currently, it supports seven methods, named "circle" (default), "square", "ellipse", "number", "pie", "shade" and "color". }
#' \item{CorrOut}{Logical, indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{plot.display}{Character, "full" (default), "upper" or "lower", display full matrix, lower triangular or upper triangular matrix.}
#'}
#'@examples
#'\donttest{
#'yy = matrix(c(rnorm(100)),nrow =10)
#'mm = cov(yy)
#'covplot(mm)
#'covplot(mm, optns= list(plot.type = "categorical", plot.clust = "hclust"))
#'covplot(mm, optns= list(plot.clust = "hclust"))
#'}
#'@importFrom corrplot corrplot


covplot <- function(mout,optns = list()){
  if(is.null(optns$plot.type)){
    plot.type = "continuous"
  }
  else{
    plot.type = optns$plot.type
  }
  if(is.null(optns$plot.clust)){
    plot.clust = NULL
    addrect = 4
  }
  else{
    plot.clust = optns$plot.clust
    if(plot.clust == "hclust"){
      addrect = 4
    }
  }
  if(is.null(optns$plot.method)){
    plot.method = "circle"
  }
  else{
    plot.method = optns$plot.method
  }
  if(is.null(optns$CorrOut)){
    CorrOut = FALSE
  }
  else{
    CorrOut = optns$CorrOut
  }
  if(is.null(optns$plot.display)){
    plot.display = "full"
  }
  else{
    plot.display = optns$plot.display
  }


  if(plot.type == "continuous"){
    col <- colorRampPalette(c("blue","white","red"))(200)
    if((dim(mout)[1] >10)){
      corrplot::corrplot(mout, method = plot.method, col = col, bg='lightblue',
                         type = plot.display, outline = TRUE,
                         order = plot.clust, addrect = addrect,
                         diag = FALSE, tl.pos = "n", is.corr = CorrOut)
    }
    else{
      corrplot::corrplot(mout, method = plot.method, col = col, bg='lightblue',
                         type = plot.display, outline = TRUE,
                         order = plot.clust, addrect = addrect,
                         diag = FALSE, is.corr = CorrOut)
    }
  }
  if(plot.type == "categorical"){
    col =c("blue","red")
    if((dim(mout)[1] >10)){
      corrplot::corrplot(mout, method = plot.method, col = col, bg='lightblue',
                         type = plot.display, outline = TRUE,
                         order = plot.clust, addrect = addrect,
                         diag = FALSE, tl.pos = "n",is.corr = CorrOut)
    }
    else{
      corrplot::corrplot(mout, method = plot.method, col = col, bg='lightblue',
                         type = plot.display, outline = TRUE,
                         order = plot.clust, addrect = addrect,
                         diag = FALSE, is.corr = CorrOut)
    }
  }
}
#' @title Global Fréchet regression for correlation matrices
#' @description Global Fréchet regression for correlation matrices
#'   with Euclidean predictors.
#' @param x an n by p matrix or data frame of predictors.
#' @param M a q by q by n array (resp. a list of q by q matrices) where
#'   \code{M[, , i]} (resp. \code{M[[i]]}) contains the i-th correlation matrix
#'   of dimension q by q.
#' @param xOut an m by p matrix or data frame of output predictor levels.
#'   It can be a vector of length p if m = 1.
#' @param optns A list of options control parameters specified by
#'   \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{metric}{choice of metric. \code{'frobenius'} and \code{'power'} are supported,
#'   which corresponds to Frobenius metric and Euclidean power metric,
#'   respectively. Default is Frobenius metric.}
#' \item{alpha}{the power for Euclidean power metric.
#'   Default is 1 which corresponds to Frobenius metric.}
#' \item{digits}{the integer indicating the number of decimal places (round)
#'   to be kept in the output. Default is NULL, which means no round operation.}
#' }
#' @return A \code{corReg} object --- a list containing the following fields:
#' \item{fit}{a list of estimated correlation matrices at \code{x}.}
#' \item{predict}{a list of estimated correlation matrices at \code{xOut}.
#'   Included if \code{xOut} is not \code{NULL}.}
#' \item{RSquare}{Fréchet coefficient of determination.}
#' \item{AdjRSquare}{adjusted Fréchet coefficient of determination.}
#' \item{residuals}{Frobenius distance between the true and
#'   fitted correlation matrices.}
#' \item{xOut}{the output predictor level used.}
#' \item{optns}{the control options used.}
#' @examples
#' # Generate simulation data
#' n <- 100
#' q <- 10
#' d <- q * (q - 1) / 2
#' xOut <- seq(0.1, 0.9, length.out = 9)
#' x <- runif(n, min = 0, max = 1)
#' y <- list()
#' for (i in 1:n) {
#'   yVec <- rbeta(d, shape1 = x[i], shape2 = 1 - x[i])
#'   y[[i]] <- matrix(0, nrow = q, ncol = q)
#'   y[[i]][lower.tri(y[[i]])] <- yVec
#'   y[[i]] <- y[[i]] + t(y[[i]])
#'   diag(y[[i]]) <- 1
#' }
#' # Frobenius metric
#' fit1 <- GloCorReg(x, y, xOut,
#'   optns = list(metric = "frobenius", digits = 5)
#' )
#' # Euclidean power metric
#' fit2 <- GloCorReg(x, y, xOut,
#'   optns = list(metric = "power", alpha = .5)
#' )
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' }
#' @importFrom Matrix nearPD
#' @export

GloCorReg <- function(x, M, xOut = NULL, optns = list()) {
  if (is.null(optns$metric)) {
    metric <- "frobenius"
  } else {
    metric <- optns$metric
  }
  if (!(metric %in% c("frobenius", "power"))) {
    stop("metric choice not supported")
  }
  if (is.null(optns$alpha)) {
    alpha <- 1
  } else {
    alpha <- optns$alpha
  }
  if (alpha < 0) {
    stop("alpha must be non-negative")
  }
  if (is.null(optns$digits)) {
    digits <- NA
  } else {
    digits <- optns$digits
  }
  if (!is.matrix(x)) {
    if (is.data.frame(x) | is.vector(x)) {
      x <- as.matrix(x)
    } else {
      stop("x must be a matrix or a data frame")
    }
  }
  if (!is.null(xOut)) {
    if (!is.matrix(xOut)) {
      if (is.data.frame(xOut)) {
        xOut <- as.matrix(xOut)
      } else if (is.vector(xOut)) {
        if (ncol(x) == 1) {
          xOut <- as.matrix(xOut)
        } else {
          xOut <- t(xOut)
        }
      } else {
        stop("xOut must be a matrix or a data frame")
      }
    }
    if (ncol(x) != ncol(xOut)) {
      stop("x and xOut must have the same number of columns")
    }
    m <- nrow(xOut) # number of predictions
  } else {
    m <- 0
  }
  y <- M
  if (!is.list(y)) {
    if (is.array(y)) {
      y <- lapply(seq(dim(y)[3]), function(i) y[, , i])
    } else {
      stop("y must be a list or an array")
    }
  }
  if (nrow(x) != length(y)) {
    stop("the number of rows in x must be the same as the number of correlation matrices in y")
  }
  n <- nrow(x) # number of observations
  p <- ncol(x) # number of covariates
  q <- ncol(y[[1]]) # dimension of the correlation matrix
  yVec <- matrix(unlist(y), ncol = q^2, byrow = TRUE) # n by q^2
  if (substr(metric, 1, 1) == "p") {
    yAlpha <- lapply(y, function(yi) {
      eigenDecom <- eigen(yi)
      Lambda <- pmax(Re(eigenDecom$values), 0) # exclude 0i
      U <- eigenDecom$vectors
      U %*% diag(Lambda^alpha) %*% t(U)
    })
    yAlphaVec <- matrix(unlist(yAlpha), ncol = q^2, byrow = TRUE) # n by q^2
  }
  xMean <- colMeans(x)
  invVa <- solve(var(x) * (n - 1) / n)
  fit <- vector(mode = "list", length = n)
  residuals <- rep.int(0, n)
  wc <- t(apply(x, 1, function(xi) t(xi - xMean) %*% invVa)) # n by p
  totVa <- sum((scale(yVec, scale = FALSE))^2)
  if (nrow(wc) != n) wc <- t(wc) # for p=1
  if (substr(metric, 1, 1) == "f") {
    for (i in 1:n) {
      w <- apply(wc, 1, function(wci) 1 + t(wci) %*% (x[i, ] - xMean))
      qNew <- apply(yVec, 2, weighted.mean, w) # q^2
      temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
      temp <- (temp + t(temp)) / 2 # symmetrize
      if (!is.na(digits)) temp <- round(temp, digits = digits) # round
      fit[[i]] <- temp
      residuals[i] <- sqrt(sum((y[[i]] - temp)^2))
    }
    resVa <- sum(residuals^2)
    RSquare <- 1 - resVa / totVa
    AdjRSquare <- RSquare - (1 - RSquare) * p / (n - p - 1)
    if (m > 0) {
      predict <- vector(mode = "list", length = m)
      for (i in 1:m) {
        w <- apply(wc, 1, function(wci) 1 + t(wci) %*% (xOut[i, ] - xMean))
        qNew <- apply(yVec, 2, weighted.mean, w) # q^2
        temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
        temp <- (temp + t(temp)) / 2 # symmetrize
        if (!is.na(digits)) temp <- round(temp, digits = digits) # round
        predict[[i]] <- temp
      }
      res <- list(fit = fit, predict = predict, RSquare = RSquare, AdjRSquare = AdjRSquare, residuals = residuals, xOut = xOut, optns = optns)
    } else {
      res <- list(fit = fit, RSquare = RSquare, AdjRSquare = AdjRSquare, residuals = residuals, optns = optns)
    }
  } else if (substr(metric, 1, 1) == "p") {
    for (i in 1:n) {
      w <- apply(wc, 1, function(wci) 1 + t(wci) %*% (x[i, ] - xMean))
      bAlpha <- matrix(apply(yAlphaVec, 2, weighted.mean, w), ncol = q) # q by q
      eigenDecom <- eigen(bAlpha)
      Lambda <- pmax(Re(eigenDecom$values), 0) # projection to M_m
      U <- eigenDecom$vectors
      qNew <- as.vector(U %*% diag(Lambda^(1 / alpha)) %*% t(U)) # inverse power
      temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
      temp <- (temp + t(temp)) / 2 # symmetrize
      if (!is.na(digits)) temp <- round(temp, digits = digits) # round
      fit[[i]] <- temp
      residuals[i] <- sqrt(sum((y[[i]] - temp)^2))
    }
    resVa <- sum(residuals^2)
    RSquare <- 1 - resVa / totVa
    AdjRSquare <- RSquare - (1 - RSquare) * p / (n - p - 1)
    if (m > 0) {
      predict <- vector(mode = "list", length = m)
      for (i in 1:m) {
        w <- apply(wc, 1, function(wci) 1 + t(wci) %*% (xOut[i, ] - xMean))
        bAlpha <- matrix(apply(yAlphaVec, 2, weighted.mean, w), ncol = q) # q^2
        eigenDecom <- eigen(bAlpha)
        Lambda <- pmax(Re(eigenDecom$values), 0) # projection to M_m
        U <- eigenDecom$vectors
        qNew <- as.vector(U %*% diag(Lambda^(1 / alpha)) %*% t(U)) # inverse power
        temp <- as.matrix(Matrix::nearPD(matrix(qNew, ncol = q), corr = TRUE, maxit = 1000)$mat)
        temp <- (temp + t(temp)) / 2 # symmetrize
        if (!is.na(digits)) temp <- round(temp, digits = digits) # round
        predict[[i]] <- temp
      }
      res <- list(fit = fit, predict = predict, RSquare = RSquare, AdjRSquare = AdjRSquare, residuals = residuals, xOut = xOut, optns = optns)
    } else {
      res <- list(fit = fit, RSquare = RSquare, AdjRSquare = AdjRSquare, residuals = residuals, optns = optns)
    }
  }
  class(res) <- "corReg"
  res
}
#' @title Global Cox point process regression.
#' @description Global Fréchet regression for replicated Cox point processes with respect to \eqn{L^2}-Wasserstein distance on shape space and Euclidean 2-norm on intensity factor space.
#' @param xin An n by p matrix with input measurements of the predictors.
#' @param tin A list holding the sample of event times of each replicated point process, where the ith element of the list \code{tin} holds the event times of the point process corresponding to the ith row of \code{xin}.
#' @param T0 A positive scalar that defines the time window [0,\code{T0}] where the replicated Cox point processes are observed.
#' @param xout A k by p matrix with output measurements of the predictors. Default is \code{xin}.
#' @param optns A list of control parameters specified by \code{list(name=value)}.
#' @details Available control options are \code{bwDen} (see \code{\link{LocDenReg}} for this option description) and
#' \describe{
#' \item{L}{Upper Lipschitz constant for quantile space; numeric -default: 1e10.}
#' \item{M}{Lower Lipschitz constant for quantile space; numeric -default: 1e-10.}
#' \item{dSup}{User defined output grid for the support of kernel density estimation used in \code{CreateDensity()} for mapping from quantile space to shape space. This grid must be in [0,\code{T0}]. Default is an equidistant grid with \code{nqSup}+2 points.}
#' \item{nqSup}{A scalar with the number of equidistant points in (0,1) used to obtain the empirical quantile function from each point process. Default: 500.}
#' }
#' @return A list containing the following components:
#' \item{xout}{Input \code{xout}.}
#' \item{dSup}{Support of each estimated (up to a constant) conditional intensity regression function in the columns of \code{intensityReg}.}
#' \item{intensityReg}{A matrix of dimension \code{length(dSup)} by \code{nrow(xout)} holding the estimated intensity regression functions up to a constant over the support grid \code{dSup}, where each column corresponds to a predictor level in the corresponding row of \code{xout}.}
#' \item{xin}{Input \code{xin}.}
#' \item{optns}{A list of control options used.}
#' @examples
#' \donttest{
#' n=100
#' alpha_n=sqrt(n)
#' alpha1=2.0
#' beta1=1.0
#' gridQ=seq(0,1,length.out=500+2)[2:(500+1)]
#' X=runif(n,0,1)#p=1
#' tau=matrix(0,nrow=n,ncol=1)
#' for(i in 1:n){
#'   tau[i]=alpha1+beta1*X[i]+truncnorm::rtruncnorm(1, a=-0.3, b=0.3, mean = 0, sd = 1.0)
#' }
#' Ni_n=matrix(0,nrow=n,ncol=1)
#' u0=0.4
#' u1=0.5
#' u2=0.05
#' u3=-0.01
#' tin=list()
#' for(i in 1:n){
#'   Ni_n[i]=rpois(1,alpha_n*tau[i])
#'   mu_x=u0+u1*X[i]+truncnorm::rtruncnorm(1,a=-0.1,b=0.1,mean=0,sd=1)
#'   sd_x=u2+u3*X[i]+truncnorm::rtruncnorm(1,a=-0.02,b=0.02,mean=0,sd=0.5)
#'   if(Ni_n[i]==0){
#'     tin[[i]]=c()
#'   }else{
#'     tin[[i]]=truncnorm::rtruncnorm(Ni_n[i],a=0,b=1,mean=mu_x,sd=sd_x) #Sample from truncated normal
#'   }
#' }
#' res=GloPointPrReg(
#'   xin=matrix(X,ncol=1),tin=tin,
#'   T0=1,xout=matrix(seq(0,1,length.out=10),ncol=1),
#'   optns=list(bwDen=0.1)
#' )
#' }
#' @references
#' \cite{Petersen, A., & Müller, H.-G. (2019). "Fréchet regression for random objects with Euclidean predictors." The Annals of Statistics, 47(2), 691--719.}
#' 
#' \cite{Gajardo, Á. and Müller, H.-G. (2022). "Cox Point Process Regression." IEEE Transactions on Information Theory, 68(2), 1133-1156.}
#' @importFrom quadprog solve.QP
#' @export

GloPointPrReg <- function(xin=NULL, tin=NULL,T0=NULL, xout=NULL, optns=list()) {
  
  if(is.null(xin)){
    stop("xin has no default and must be input")
  }
  if(!is.matrix(xin)){
    stop("xin must be a matrix")
  }
  if(is.null(xout)){
    xout=xin
  }
  if(!is.matrix(xout)){
    stop("xout must be a matrix")
  }
  if(ncol(xin)!=ncol(xout)){
    stop("xin and xout must have same number of columns")
  }
  if(is.null(tin)){
    stop("tin has no default and must be input")
  }
  if(is.null(T0)){
    stop("T0 has no default and must be input")
  }
  if(T0<max(unlist(tin))){
    stop("T0 cannot be smaller than any event time")
  }
  if(is.null(optns$L)){
    optns$L=1e10
  }
  if(is.null(optns$M)){
    optns$M=1e-10
  }
  if(optns$L<0 | optns$M<0 | optns$L<optns$M){
    stop("L and M must be positive with L>M")
  }
  if(is.null(xout)){
    xout <- xin
  }
  if(is.vector(xin)){
    xin = as.matrix(xin)
  }
  
  if(!is.null(optns$bwDen)){
    if(sum(!is.numeric(optns$bwDen) | !length(optns$bwDen)==1 | !(optns$bwDen>0))>0){
      stop("bwDen option must be a positive scalar")
    }
    optns$userBwMu=optns$bwDen
  }
  if(is.null(optns$nqSup)){
    optns$nqSup=500
  }else{
    if(sum(!is.numeric(optns$nqSup) | !length(optns$nqSup)==1 | !(optns$nqSup>0))>0){
      stop("nqSup must be a positive integer")
    }
    if(optns$nqSup<100){
      warning("nqSup option may be too small")
    }
  }
  
  if(!is.null(optns$dSup)){
    if(optns$dSup[1]!=0 | optns$dSup[length(optns$dSup)]!=T0){
      stop("dSup must be a vector with endpoints 0 and T0")
    }
  }
  
  n = nrow(xin)
  
  getNplus=function(x_predict){
    cov_X=var(xin)*(n-1)/n
    aux=xin-mean(xin)
    N_plus=max((t(Ni_n)%*%(1+aux%*%solve(cov_X)%*%(x_predict-mean(xin)))/n)/mean(Ni_n),0)
    return(N_plus)
  }
  
  gridQ=seq(0,1,length.out=optns$nqSup+2)[2:(optns$nqSup+1)]
  Qi_hat=matrix(0,nrow=n,ncol=length(gridQ))
  Ni_n=matrix(0,nrow=n,ncol=1)
  for(i in 1:n){
    Ni_n[i]=length(tin[[i]])
    if(Ni_n[i]==0){
      Qi_hat[i,]=gridQ #uniform quantile distribution function if there are no points
    }else{
      Qi_hat[i,]=as.vector(quantile(tin[[i]],gridQ))
    }
  }
  
  k = nrow(xout)
  m = ncol(Qi_hat)
  p=ncol(xin)
  xbar = colMeans(xin)
  Sigma = cov(xin) * (n-1) / n
  invSigma = solve(Sigma)
  
  A = cbind(diag(m), rep(0,m)) + cbind(rep(0,m), -diag(m))
  A=A[,c(1,ncol(A),2:(ncol(A)-1))]
  A=cbind(A,-A[,3:ncol(A)])
  A=cbind(c(-1,rep(0,m-1)),c(rep(0,m-1),1),c(rep(0,m-1),-1),A)
  b0 = c(-optns$L/(m+1),1-optns$L/(m+1),optns$M/(m+1)-1,optns$M/(m+1),-T0,rep(optns$M/(m+1),m-1))
  b0=c(b0,-rep(optns$L/(m+1),m-1))
  
  quantile_GFR = t(sapply(1:k, function(j){
    s = 1 + t(t(xin) - xbar) %*% invSigma %*% (xout[j,] - xbar)
    s = as.vector(s)/n
    gx = (s %*% Qi_hat)
    res = do.call(quadprog::solve.QP, list(diag(m), gx, A, b0))
    return(sort(res$solution))
  })) #each row contains the GFR quantile regression function on shape space at xout[j,]
  
  
  if(is.null(optns$dSup)){
    density_grid=as.vector(c(0,gridQ,T0))
  }else{
    density_grid=optns$dSup
    optns$dSup=NULL
  }
  
  intensityReg=sapply(1:k,function(j){
    qf2pdf(qf=as.vector(c(0,quantile_GFR[j,],T0)),prob=as.vector(c(0,gridQ,1)),optns=list(outputGrid=density_grid,infSupport=FALSE,userBwMu=optns$bwDen))$y*getNplus(xout[j,])
  })
  
  res <- list(xout = xout, dSup = density_grid, intensityReg = intensityReg, xin=xin, optns=optns)
  
  class(res) <- "PointPrReg"
  return(res)
}

#' @title Generalized Fréchet integrals of network 
#' @description Calculating generalized Fréchet integrals of networks (equipped with Frobenius norm of adjacency matrices with zero diagonal elements and non negative off diagonal elements.) 
#' @param phi An eigenfunction along which we want to project the network
#' @param t_out Support of \code{phi}
#' @param X A three dimensional array of dimension \code{length(t_out) x m x m}, where \code{X[i,,]} is an \code{m x m} network adjacency matrix. The diagonal elements of adjacency matrices are zero and the off diagonal entries lie between zero and \code{U}.
#' @param U Upper bound of off-diagonal entries
#' @return A list of the following:
#' \item{f}{An adjacency matrix which corresponds to the Fréchet integral of \code{X} along \code{phi}}
#' @examples 
#' \donttest{
#' set.seed(5)
#' n <- 100
#' N <- 50
#' t_out <- seq(0,1,length.out = N)
#' library(mpoly)
#' p2 <- as.function(mpoly::jacobi(2,4,3),silent=TRUE)
#' p4 <- as.function(mpoly::jacobi(4,4,3),silent=TRUE)
#' p6 <- as.function(mpoly::jacobi(6,4,3),silent=TRUE)
#' 
#' # first three eigenfunctions
#' phi1 <- function(t){
#' p2(2*t-1)*t^(1.5)*(1-t)^2 / (integrate(function(x) p2(2*x-1)^2*x^(3)*(1-x)^4,0,1))$value^(1/2)
#' }
#' phi2 <- function(t){
#' p4(2*t-1)*t^(1.5)*(1-t)^2 / (integrate(function(x) p4(2*x-1)^2*x^(3)*(1-x)^4,0,1))$value^(1/2)
#' }
#' phi3 <- function(t){
#' p6(2*t-1)*t^(1.5)*(1-t)^2 / (integrate(function(x) p6(2*x-1)^2*x^(3)*(1-x)^4,0,1))$value^(1/2)
#' }
#' 
#' # random component of adjacency matrices
#' P12 <- 0.1 ## edge between compunities
#' Score <- matrix(runif(n*4), nrow = n)
#' # edge within first community
#' P1_vec <- 0.5 + 0.4*Score[,1] %*% t(phi1(t_out)) + 0.1*Score[,2] %*% t(phi3(t_out)) 
#' # edge within second community
#' P2_vec <- 0.5 + 0.3*Score[,3] %*% t(phi2(t_out)) + 0.1*Score[,4] %*% t(phi3(t_out)) 
#' 
#' # create Network edge matrix
#' N_net1 <- 5 # first community number
#' N_net2 <- 5 # second community number
#' 
#' # I: four dimension array of n x n matrix of squared distances between the time point u 
#' # of the ith process and process and the time point v of the jth object process,
#' # e.g.: I[i,j,u,v] <- d_F^2(X_i(u) X_j(v)).
#' I <- array(0, dim = c(n,n,N,N))
#' for(u in 1:N){
#'   for(v in 1:N){
#'    #frobenius norm between two adjcent matrix
#'     I[,,u,v] <- outer(P1_vec[,u], P1_vec[,v], function(a1, a2) (a1-a2)^2*(N_net1^2-N_net1)) +
#'       outer(P2_vec[,u], P2_vec[,v], function(a1, a2) (a1-a2)^2*(N_net2^2-N_net2))
#'   }
#' }
#' 
#' 
#' # check ObjCov work
#' Cov_result <- ObjCov(t_out, I, 3, smooth=FALSE)
#' Cov_result$lambda  # 0.266 0.15 0.04
#' 
#' # sum((Cov_result$phi[,1] - phi1(t_out))^2) / sum(phi1(t_out)^2)
#' # sum((Cov_result$phi[,2] - phi2(t_out))^2) / sum(phi2(t_out)^2)
#' # sum((Cov_result$phi[,3] - phi3(t_out))^2) / sum(phi3(t_out)^2)
#' 
#' # e.g. subj 2
#' subj <- 2
#' # X_mat is the network for varying times with X[i,,] is the adjacency matrices 
#' # for the ith time point
#' X_mat <- array(0, c(N,(N_net1+N_net2), (N_net1+N_net2)))
#' for(i in 1:N){
#'   # edge between communities is P12
#'   Mat <- matrix(P12, nrow = (N_net1+N_net2), ncol = (N_net1+N_net2)) 
#'   # edge within the first communitiy is P1
#'   Mat[1:N_net1, 1:N_net1] <- P1_vec[subj, i] 
#'   # edge within the second community is P2
#'   Mat[(N_net1+1):(N_net1+N_net2), (N_net1+1):(N_net1+N_net2)] <- P2_vec[subj, i] 
#'   diag(Mat) <- 0 #diagonal element is 0
#'   X_mat[i,,] <- Mat
#' }
#' # output the functional principal network(adjacency matrice) of the second eigenfunction
#' NetFIntegral(Cov_result$phi[,2], t_out, X_mat, 2)
#' }
#' @references 
#' \cite{Dubey, P., & Müller, H. G. (2020). Functional models for time‐varying random objects. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2), 275-327.}
#' @export
#' @import fdapace


NetFIntegral <- function(phi, t_out, X, U){
  N <- length(phi)
  m <- dim(X)[2]
  if(dim(X)[1] != N){
    stop("length of first argument of X and length of phi are not consistenet ")
  }
  if(dim(X)[3] != dim(X)[2]){
    stop("X[i,,] should be a square matrix")
  }

  phi_out <- phi / fdapace::trapzRcpp(t_out, phi)
  g_mini <- matrix(0, nrow = m, ncol = m)
  for(i in 1:(m-1)){
    for(j in (i+1):m){
      g_mini[i,j] <- fdapace::trapzRcpp(t_out, phi_out * X[,i,j])
      g_mini[j,i] <- g_mini[i,j]
    }
    g_mini[i,i] <- fdapace::trapzRcpp(t_out, phi * X[,i,i])
  }
  g_mini[m,m] <- fdapace::trapzRcpp(t_out, phi * X[,m,m])
  # we project the global minimizer g_mini to the space of adjacency matrices with zero diagonal elements and 
  # non negative off diagonal elements. In the paper, the author assumes that the space comprises of all matrices 
  # whose weights are greater than or equal to 0 and less than or equal to U. For more complex representations, this step needs to be modified.
  
  g_mini[g_mini<0] <- 0
  g_mini[g_mini>U] <- U
  diag(g_mini) <- 0
  return(list(f = g_mini))
}



#'@title Fréchet mean of covariance matrices
#'@description Fréchet mean computation for covariance matrices.
#'@param M A q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{metric}{Metric type choice, \code{"frobenius"}, \code{"power"}, \code{"log_cholesky"}, \code{"cholesky"} - default: \code{"frobenius"} which corresponds to the power metric with \code{alpha} equal to 1.}
#' \item{alpha}{The power parameter for the power metric, which can be any non-negative number. Default is 1 which corresponds to Frobenius metric.}
#' \item{weights}{A vector of weights to compute the weighted barycenter. The length of \code{weights} is equal to the sample size n. Default is equal weights.}
#' }
#' @return A list containing the following fields:
#' \item{Mout}{A list containing the Fréchet mean of the covariance matrices in \code{M}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#'#Example M input
#'n=10 #sample size
#'m=5 # dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#' Fmean=CovFMean(M=M,optns=list(metric="frobenius"))
#'
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \item \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' \item \cite{Lin, Z. (2019). Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. Siam. J. Matrix. Anal, A. 40, 1353--1370.}
#' }
#' @export

CovFMean= function(M=NULL, optns = list()){
  if(is.list(M)){
    n=length(M)
  } else {
    if(!is.array(M)){
      stop('M must be an array or a list')
    }
    n=dim(M)[3]
  }
  if(n==1){
    stop("Sample size n should be at least 2")
  }
  x=matrix(1:n,nrow=n,ncol=1)
  xout=matrix((n+1)/2) #mean of x

  if (is.null(optns$metric)){
    metric="frobenius"
  } else {
    metric=optns$metric
  }
  if(!metric%in%c("frobenius","power","cholesky","log_cholesky")){
    stop("metric choice not supported.")
  }

  flagUnweighted=FALSE
  if(!is.null(optns$weights)){
    if(!is.vector(optns$weights)){
      stop("weights should be a vector")
    }
    if(n!=length(optns$weights)){
      stop("The length of weights cannot differ from the sample size")
    }
    if(sum(optns$weights<0)>0){
      stop("weights must be non-negative")
    }
    if(abs(sum(optns$weights)-1)>1e-15){
      stop("weights must sum to 1")
    }
    if(sum(abs(optns$weights-1/n))==0){
      #Case of equal weights requires different call to GFRCov function
      flagUnweighted=TRUE
    }else{
      if(metric=="frobenius"){
        res <- list(Mout=GFRCov(x=matrix(optns$weights), y=NULL,M=M,xout=matrix(sum(optns$weights^2)),optns = optns)$Mout,optns=optns)
      } else if(metric=="power"){
        res <- list(Mout=GFRCovPower(x=matrix(optns$weights), y=NULL,M=M,xout=matrix(sum(optns$weights^2)),optns = optns)$Mout,optns=optns)
      } else {
        if (is.null(M))
          stop("M must be input for Cholesky and log-Cholesky metrics; y does not apply.")
        res <- list(Mout=GFRCovCholesky(x=matrix(optns$weights), M=M, xout=matrix(sum(optns$weights^2)), optns = optns)$Mout,optns=optns)
      }
    }
  }
  
  if(flagUnweighted | is.null(optns$weights)){
    #If weights are provided and are equal (unweighted case) or weights not provided
    if(metric=="frobenius"){
      res <- list(Mout=GFRCov(x=x, y=NULL,M=M,xout=xout,optns = optns)$Mout,optns=optns)
    } else if(metric=="power"){
      res <- list(Mout=GFRCovPower(x=x, y=NULL,M=M,xout=xout,optns = optns)$Mout,optns=optns)
    } else {
      if (is.null(M))
        stop("M must be input for Cholesky and log-Cholesky metrics; y does not apply.")
      res <- list(Mout=GFRCovCholesky(x=x, M=M, xout=xout, optns = optns)$Mout,optns=optns)
    }
  }
  class(res) <- "covReg"
  return(res)
}
#'@title Geodesic distance on spheres.
#'@param y1,y2 Two unit vectors, i.e., with \eqn{L^2} norm equal to 1, of the same length.
#'@return A scalar holding the geodesic distance between \code{y1} and \code{y2}.
#'@examples
#'d <- 3
#'y1 <- rnorm(d)
#'y1 <- y1 / sqrt(sum(y1^2))
#'y2 <- rnorm(d)
#'y2 <- y2 / sqrt(sum(y2^2))
#'dist <- SpheGeoDist(y1,y2)
#'@export

SpheGeoDist <- function(y1,y2) {
  if (abs(length(y1) - length(y2)) > 0) {
    stop("y1 and y2 should be of the same length.")
  }
  if ( !isTRUE( all.equal(l2norm(y1),1) ) ) {
    stop("y1 is not a unit vector.")
  }
  if ( !isTRUE( all.equal(l2norm(y2),1) ) ) {
    stop("y2 is not a unit vector.")
  }
  y1 = y1 / l2norm(y1)
  y2 = y2 / l2norm(y2)
  if (sum(y1 * y2) > 1){
    return(0)
  } else if (sum(y1*y2) < -1){
    return(pi)
  } else return(acos(sum(y1 * y2)))
}
#'@noRd
#'@import pracma
#'@import fdapace
#'@importFrom utils getFromNamespace

getSmoothCov <- function(C, tgrid, method, kern, n){
  t = length(tgrid)
  t1 = tgrid
  cyy = t(as.vector(C))
  cxxn = cyy
  xygrid = pracma::meshgrid(t1)
  xx = xygrid$X
  yy = xygrid$Y
  tpairn = cbind(as.vector(xx), as.vector(yy))
  win = pracma::ones(1, length(cxxn))
  indx = c()
  count = c()
  rcov = list(tpairn = tpairn, 
              cxxn = cxxn,
              indx, indx, 
              win = win,
              cyy = cyy,
              count = count)
  GCVLwls2DV2 <- utils::getFromNamespace("GCVLwls2DV2", "fdapace")
  if (method %in% c('GCV', 'GMeanAndGCV')){
    gcvObj = GCVLwls2DV2(tgrid, tgrid, kern = kern, rcov = rcov, t = lapply(1:n, function(o) tgrid))
    bwCov <- gcvObj$h
    if (method == 'GMeanAndGCV') {
      bwCov <- sqrt(bwCov * gcvObj$minBW)
    } 
  }
  sC = fdapace::Lwls2D(bwCov, "gauss", xin = cbind(rep(tgrid, times = t), rep(tgrid, each = t)), yin = as.vector(C), xout1 = tgrid, xout2 = tgrid)
  
  return(sC)
}
#' @title Plots for Fréchet regression for univariate densities.
#' @param x A \code{denReg} object, result of \code{\link{DenFMean}}, \code{\link{GloDenReg}} or \code{\link{LocDenReg}}.
#' @param obj An integer indicating which output to be plotted; 1, 2, 3, 4, and 5 for \code{dout}, \code{qout}, \code{din}, \code{qin}, and reference chart for \code{qout}, respectively - default: 1.
#' @param prob A vector specifying the probability levels for reference chart if \code{obj} is set to 5. Default: \code{c(0.05,0.25,0.5,0.75,0.95)}.
#' @param xlab Character holding the label for x-axis; default: \code{"Probability"} when \code{obj} is 2 or 4, \code{""} when \code{obj} is 1 or 3, \code{"x"} when \code{obj} is 5.
#' @param ylab Character holding the label for y-axis; default: \code{"Quantile"} when \code{obj} is 2, 4, or 5, and \code{"Density"} when \code{obj} is 1 or 3.
#' @param main Character holding the plot title; default: \code{NULL}.
#' @param xlim A numeric vector of length 2 holding the range of the x-axis to be drawn; default: automatically determined by the input \code{x}.
#' @param ylim A numeric vector of length 2 holding the range of the y-axis to be drawn; default: automatically determined by the input \code{x}.
#' @param col.bar A logical variable indicating whether a color bar is presented on the right of the plot - default: \code{TRUE}.
#' @param widrt A scalar giving the width ratio between the main plot and the color bar - default: 4.
#' @param col.lab A character giving the color bar label.
#' @param nticks An integer giving the number of ticks used in the axis of color bar.
#' @param ticks A numeric vector giving the locations of ticks used in the axis of color bar; it overrides \code{nticks}.
#' @param add Logical; only works when \code{obj} is 5. If \code{TRUE} add to an already existing plot. Taken as \code{FALSE} (with a warning if a different value is supplied) if no graphics device is open.
#' @param pos.prob \code{FALSE} or a scalar less than 0 or larger than 1. FALSE: no probability levels will be labeled on the quantile curves; a scalar between 0 and 1: indicating where to put the probability levels along the curves on growth charts: 0 and 1 correspond to left and right ends, respectively. Default: 0.9.
#' @param colPalette A function that takes an integer argument (the required number of colors) and returns a character vector of colors interpolating the given sequence
#' (e.g., \code{\link{heat.colors}}, \code{\link{terrain.colors}} and functions created by \code{\link{colorRampPalette}}).
#' Default is \code{colorRampPalette(colors = c("pink","royalblue"))} for more than one curves and \code{"black"} otherwise.
#' @param ... Can set up \code{lty}, \code{lwd}, etc.
#' @return No return value.
#' @note see \code{\link{DenFMean}}, \code{\link{GloDenReg}} and \code{\link{LocDenReg}} for example code.
#' @export

plot.denReg <- function(x, obj = NULL, prob = NULL,
                        xlab = NULL, ylab = NULL, main = NULL,
                        ylim = NULL, xlim = NULL,
                        col.bar = TRUE, widrt = 4, col.lab = NULL,
                        nticks = 5, ticks = NULL,
                        add = FALSE, pos.prob = 0.9,
                        colPalette = NULL,...) {
  if (is.null(obj)) obj <- 1
  if(! obj %in% 1:5) stop("obj is mis-specified.")

  if (is.null(colPalette))
    colPalette <- colorRampPalette(colors = c("pink","royalblue"))
  if (obj == 5) {
    # growth chart
    if (is.null(prob))
      prob <- c(0.05,0.25,0.5,0.75,0.95)
    plotObj <- sapply(apply(x$qout, 1, approx, x=x$qSup, xout=prob), with, y)
    plotGrid <- x$xout
    if (!is.vector(plotGrid)) {
      plotGrid <- plotGrid[,1]
      warning("x$xout is not a vector. Only the first column is used.")
    }
    if (is.null(ylim)) ylim <- range(plotObj)
    if (is.null(xlim)) xlim <- range(plotGrid)
    if(is.null(ylab)) ylab <- "Quantile"
    if(is.null(xlab)) xlab <- "x"

    n <- nrow(plotObj)
    if (n == 1)
      colPalette <- function(num) rep("black",num)
    i <- 1
    if (add) {
      lines(plotGrid, plotObj[i,], col = colPalette(n)[i], ...)
    } else {
      if (add != FALSE) warning("add is mis-specified and is taken to be FALSE.")
      plot(plotGrid, plotObj[i,], type='l', col = colPalette(n)[i],
           ylim = ylim, xlim = xlim, xlab = xlab, ylab = ylab, main = main, ...)
    }
    if (is.numeric(pos.prob) & pos.prob >= 0 & pos.prob <= 1) {
      textidx <- which.min(abs(plotGrid - (xlim[1]*(1-pos.prob)+xlim[2]*pos.prob)))
      text(x = plotGrid[textidx], y = plotObj[i,textidx], labels = prob[i], col = colPalette(n)[i], cex=0.7)
    }
    if (n > 1) {
      for (i in 2:n) {
        lines(plotGrid, plotObj[i,], col = colPalette(n)[i], ...)
        if (is.numeric(pos.prob) & pos.prob >= 0 & pos.prob <= 1) {
          textidx <- which.min(abs(plotGrid - (xlim[1]*(1-pos.prob)+xlim[2]*pos.prob)))
          text(x = plotGrid[textidx], y = plotObj[i,textidx], labels = prob[i], col = colPalette(n)[i], cex=0.7)
        }
      }
    }

  } else {

    if(is.null(ylab)) ylab <- ifelse(obj %in% c(1,3), "Density", "Quantile")
    if(is.null(xlab)) xlab <- ifelse(obj %in% c(1,3), "", "Probability")

    if ((obj == 1 & is.list(x$dout)) | obj == 3) {
      if (obj == 1) {
        tmp <- x$dout
      } else tmp <- x$din
      n <- length(tmp)

      if (n == 1) {
        colPalette <- function(num) rep("black",num)
      } else {
        if (col.bar) {
          oldpar <- par(no.readonly = TRUE)
          on.exit(par(oldpar))
          
          #def.par <- par(no.readonly = TRUE)
          layout(t(1:2), widths = c(widrt,1))
        }
      }

      if (is.null(ylim)) {
        ylim <- sapply(tmp, function(d) range(d$y))
        ylim <- c(min(ylim[1,]), max(ylim[2,]))
      }
      if (is.null(xlim)) {
        xlim <- sapply(tmp, function(d) range(d$x))
        xlim <- c(min(xlim[1,]), max(xlim[2,]))
      }

      i <- 1
      plot(tmp[[i]]$x, tmp[[i]]$y, col=colPalette(n)[i], type='l',
           xlim = xlim, ylim = ylim, xlab = xlab, ylab = ylab, main = main, ...)
      if (n > 1) {
        for (i in 2:n) {
          lines(tmp[[i]]$x, tmp[[i]]$y, col=colPalette(n)[i], ...)
        }
      }
    } else {
      if (obj == 1) {
        plotObj <- x$dout
        plotGrid <- x$dSup
      } else if (obj == 2) {
        plotObj <- x$qout
        plotGrid <- x$qSup
      } else {
        plotObj <- x$qin
        plotGrid <- x$qSup
      }
      if (is.null(ylim)) ylim <- range(plotObj)
      if (is.null(xlim)) xlim <- range(plotGrid)

      if (is.vector(plotObj)) {
        plotObj <- matrix(plotObj, nrow = 1)
      }
      n <- nrow(plotObj)
      
      if (n == 1) {
        colPalette <- function(num) rep("black",num)
      } else {
        if (col.bar) {
          oldpar <- par(no.readonly = TRUE)
          on.exit(par(oldpar))
          
          layout(t(1:2), widths = c(widrt,1))
        }
      }

      i <- 1
      plot(plotGrid, plotObj[i,], type='l', col = colPalette(n)[i],
           ylim = ylim, xlim = xlim, xlab = xlab, ylab = ylab, main = main, ...)
      if (n > 1) {
        for (i in 2:n) {
          lines(plotGrid, plotObj[i,], col = colPalette(n)[i], ...)
        }
      }
    }
    if (obj %in% c(1,2)) {
      colVal <- x$xout
    } else colVal <- x$xin
    if (!is.vector(colVal)) {
      colVal <- colVal[,1]
      warning(ifelse(obj %in% c(1,2), "x$xout","x$xin")," is not a vector. Only the first column is used.")
    }

    if (n > 1) {
      if (col.bar) {
        color.bar(colVal = colVal, lut = colPalette(length(colVal)), nticks = nticks, ticks = ticks, title = col.lab)
        #par(def.par)
      }
    }
  }
}
# using trust package and perturbation for initial value
#' @noRd
#' @import trust trust
#' 
LocSpheGeoReg <- function(xin, yin, xout, optns = list()) {
  k = length(xout)
  n = length(xin)
  m = ncol(yin)
  
  bw <- optns$bw
  ker <- kerFctn(optns$kernel)
  
  yout = sapply(1:k, function(j){
    mu0 = mean(ker((xout[j] - xin) / bw))
    mu1 = mean(ker((xout[j] - xin) / bw) * (xin - xout[j]))
    mu2 = mean(ker((xout[j] - xin) / bw) * (xin - xout[j])^2)
    s = ker((xout[j] - xin) / bw) * (mu2 - mu1 * (xin - xout[j])) /
      (mu0 * mu2 - mu1^2)
    
    # initial guess
    y0 = colMeans(yin*s)
    y0 = y0 / l2norm(y0)
    if (sum(sapply(1:n, function(i) sum(yin[i,]*y0))[ker((xout[j] - xin) / bw)>0] > 1-1e-8)){
    #if (sum( is.infinite (sapply(1:n, function(i) (1 - sum(yin[i,]*y0)^2)^(-0.5) )[ker((xout[j] - xin) / bw)>0] ) ) +
    #   sum(sapply(1:n, function(i) 1 - sum(yin[i,] * y0)^2 < 0)) > 0){
      # return(y0)
      y0 = y0 + rnorm(3) * 1e-3
      y0 = y0 / l2norm(y0)
    }
    
    objFctn = function(y){
      # y <- y / l2norm(y)
      if ( ! isTRUE( all.equal(l2norm(y),1) ) ) {
        return(list(value = Inf))
      }
      f = mean(s * sapply(1:n, function(i) SpheGeoDist(yin[i,], y)^2))
      g = 2 * colMeans(t(sapply(1:n, function(i) SpheGeoDist(yin[i,], y) * SpheGeoGrad(yin[i,], y))) * s)
      res = sapply(1:n, function(i){
        grad_i = SpheGeoGrad(yin[i,], y)
        return((grad_i %*% t(grad_i) + SpheGeoDist(yin[i,], y) * SpheGeoHess(yin[i,], y)) * s[i])
      }, simplify = "array")
      h = 2 * apply(res, 1:2, mean)
      return(list(value=f, gradient=g, hessian=h))
    }
    res = trust::trust(objFctn, y0, 0.1, 1e5)
    # res = trust::trust(objFctn, y0, 0.1, 1)
    return(res$argument / l2norm(res$argument))
  })
  return(t(yout))
}
#' @title Converting a quantile function to a histogram
#' @noRd
#' @param qf a numerical vector holding the values of a quantile function at a probability grid.
#' @param prob a numerical vector holding the probability grid at which the quantile function takes values. By default, \code{prob} is an equidistant sequence with the same length as \code{qf}.
#' @param breaks a numerical vector holding the breakpoints between histogram bins. By default, \code{breaks} is an equidistant sequence from the minimum to the maximum values of \code{qf}.

qf2hist <- function(qf=NULL, prob=NULL, breaks=NULL){#, tol=1e-2){
  if(is.null(qf))
    stop("qf is missing.")
  if(!is.vector(qf))
    stop("qf should be a vector.")
  if (!is.numeric(qf))
    stop("qf should be a numerical vector.")
  if (is.unsorted(qf))
    stop("qf should be an increasingly sorted numerical vector.")
  if(is.null(prob))
    prob = seq(0,1,length.out=length(qf))
  if(length(prob)!=length(qf))
    stop("The length of prob should be the same as qf.")
  if(!is.vector(prob) | !is.numeric(prob) | is.unsorted(prob))
    stop("prob should be an increasingly sorted numerical vector.")

  #if(min(prob)>tol | max(prob) < 1-tol)
  #  stop("prob should be a vector with minimum 0 and maximum 1.")
  if(is.null(breaks))
    breaks = seq(min(qf), max(qf), length.out = 1e3)
  if(!is.vector(breaks) | !is.numeric(breaks) | is.unsorted(breaks) )
    stop("breaks should be an increasingly sorted numerical vector.")
  if(min(breaks) > min(qf) | max(breaks) < max(qf))
    stop("The range of breaks should cover that of qf.")

  cdf = approx(x=qf, y=prob, xout=breaks, ties = mean)$y
  if (min(qf) > min(breaks)) cdf[breaks < min(qf)] <- 0
  if (max(breaks) > max(qf)) cdf[breaks > max(qf)] <- 1
  if (sum(cdf>1)) cdf[cdf>1] <- 1
  density = (cdf[-1] - cdf[-length(cdf)])
  counts = as.integer(density * 1e5)
  density = density / (breaks[-1] - breaks[-length(breaks)])
  mids = (breaks[-1] + breaks[-length(breaks)]) / 2
  hist = list(breaks=breaks, counts=counts, density=density, mids=mids)
  return(hist)
}
#' @title Local Fréchet Regression for Spherical Data
#' 
#' @description  Local Fréchet regression for spherical data with respect to the geodesic distance.
#' 
#' @param xin A vector of length n with input measurement points.
#' @param yin An n by m matrix holding the spherical data, of which the sum of squares of elements within each row is 1.
#' @param xout A vector of length k with output measurement points; Default: \code{xout = xin}.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{bw}{A scalar used as the bandwidth or \code{"CV"} (default).}
#' \item{kernel}{A character holding the type of kernel functions for local Fréchet regression for densities; \code{"rect"}, \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, \code{"quar"} - default: \code{"gauss"}.}
#' }
#' @return A list containing the following components:
#' \item{xout}{Input \code{xout}.}
#' \item{yout}{A k by m matrix holding the fitted responses, of which each row is a spherical vector, corresponding to each element in \code{xout}.}
#' \item{xin}{Input \code{xin}.}
#' \item{yin}{Input \code{yin}.}
#' \item{optns}{A list of control options used.}
#' 
#' @examples
#' \donttest{
#' set.seed(1)
#' n <- 200
#' # simulate the data according to the simulation in Petersen & Müller (2019)
#' xin <- runif(n)
#' err_sd <- 0.2
#' xout <- seq(0,1,length.out = 51)
#' 
#' phi_true <- acos(xin)
#' theta_true <- pi * xin
#' ytrue <- cbind(
#'   sin(phi_true) * cos(theta_true),
#'   sin(phi_true) * sin(theta_true),
#'   cos(phi_true)
#' )
#' basis <- list(
#'   b1 = cbind(
#'     cos(phi_true) * cos(theta_true),
#'     cos(phi_true) * sin(theta_true),
#'     -sin(phi_true)
#'   ),
#'   b2 = cbind(
#'     sin(theta_true),
#'     -cos(theta_true),
#'     0
#'   )
#' )
#' yin_tg <- basis$b1 * rnorm(n, mean = 0, sd = err_sd) + 
#'   basis$b2 * rnorm(n, mean = 0, sd = err_sd)
#' yin <- t(sapply(seq_len(n), function(i) {
#'   tgNorm <- sqrt(sum(yin_tg[i,]^2))
#'   if (tgNorm < 1e-10) {
#'     return(ytrue[i,])
#'   } else {
#'     return(sin(tgNorm) * yin_tg[i,] / tgNorm + cos(tgNorm) * ytrue[i,])
#'   }
#' }))
#' 
#' res <- LocSpheReg(xin=xin, yin=yin, xout=xout, optns = list(bw = 0.15, kernel = "epan"))
#' }
#' @references
#' \cite{Petersen, A., & Müller, H.-G. (2019). "Fréchet regression for random objects with Euclidean predictors." The Annals of Statistics, 47(2), 691--719.}
#' @export 

LocSpheReg <- function(xin=NULL, yin=NULL, xout=NULL, optns=list()){
  
  if (is.null(xin))
    stop ("xin has no default and must be input by users.")
  if (is.null(yin))
    stop ("yin has no default and must be input by users.")
  if (is.null(xout))
    xout <- xin
  if (!is.vector(xin) | !is.numeric(xin))
    stop("xin should be a numerical vector.")
  if (!is.matrix(yin) | !is.numeric(yin))
    stop("yin should be a numerical matrix.")
  if (!is.vector(xout) | !is.numeric(xout))
    stop("xout should be a numerical vector.")
  if (length(xin)!=nrow(yin))
    stop("The length of xin should be the same as the number of rows in yin.")
  if (sum(abs(rowSums(yin^2) - rep(1,nrow(yin))) > 1e-6)){
    yin = yin / sqrt(rowSums(yin^2))
    warning("Each row of yin has been standardized to enforce sum of squares equal to 1.")
  }
  
  if (is.null(optns$bw)){
    optns$bw <- "CV" #max(sort(xin)[-1] - sort(xin)[-length(xin)]) * 1.2
  }
  if (is.character(optns$bw)) {
    if (optns$bw != "CV") {
      warning("Incorrect input for optns$bw.")
    }
  } else if (!is.numeric(optns$bw)) {
    stop("Mis-specified optns$bw.")
  }
  if (length(optns$bw) > 1)
    stop("bw should be of length 1.")
  
  if (is.null(optns$kernel))
    optns$kernel <- "gauss"
  
  if (is.numeric(optns$bw)) {
    bwRange <- SetBwRange(xin = xin, xout = xout, kernel_type = optns$kernel)
    if (optns$bw < bwRange$min | optns$bw > bwRange$max) {
      optns$bw <- "CV"
      warning("optns$bw is too small or too large; reset to be chosen by CV.")
    }
  } 
  if (optns$bw == "CV") {
    optns$bw <- bwCV_sphe(xin = xin, yin = yin, xout = xout, optns = optns)
  }
  yout <- LocSpheGeoReg(xin = xin, yin = yin, xout = xout, optns = optns)
  res <- list(xout = xout, yout = yout, xin = xin, yin = yin, optns = optns)
  class(res) <- "spheReg"
  return(res)
}
#' @title Fréchet Variance for Networks
#' @description Obtain Fréchet variance for graph Laplacian matrices, 
#'   covariance matrices, or correlation matrices 
#'   with respect to the Frobenius distance.
#' @param Ly A list (length n) of m by m matrices or a m by m by n array where
#'   \code{Ly[, , i]} contains an m by m matrix, which can be either graph 
#'   Laplacian matrices or covariance matrices or correlation matrices.
#' @return A list containing the following fields:
#' \item{NetFVar}{A scalar holding the Fréchet variance.}
#' \item{NetFMean}{A matrix holding the Fréchet mean.}
#' @examples
#' set.seed(1)
#' n <- 100
#' U <- pracma::randortho(10)
#' Ly <- lapply(1:n, function(i) {
#'   U %*% diag(rexp(10, (1:10)/2)) %*% t(U)
#' })
#' res <- NetFVar(Ly)
#' res$NetFVar
#' @export

NetFVar <- function(Ly = NULL) {
  if (is.null(Ly)) {
    stop("requires the input of Ly")
  }
  if (!is.list(Ly)) {
    if (is.array(Ly)) {
      Ly <- lapply(seq(dim(Ly)[3]), function(i) Ly[, , i])
    } else {
      stop("Ly must be a list or an array")
    }
  }
  if (length(unique(sapply(Ly, length))) > 1) {
    stop("each matrix in Ly should be of the same dimension")
  }
  if (any(sapply(Ly, function(Lyi) nrow(Lyi) != ncol(Lyi)))) {
    stop("each matrix in Ly should be a square matrix")
  }
  n <- length(Ly)
  mup <- rowMeans(matrix(unlist(Ly), ncol = n))
  Vp <- mean(sapply(Ly, function(Lyi) sum((Lyi - mup)^2)))
  res <- list(NetFVar = Vp, NetFMean = mup)
  res
}
#' @title Global Fréchet regression of covariance matrices with Log-Cholesky and Cholesky metric
#' @noRd
#' @description Global Fréchet regression of covariance matrices with Euclidean predictors.
#' 
#' @param x an n by p matrix of predictors.
#' @param M an q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#' @param xout an m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are 
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{metric}{Metric type choice, "log_cholesky", "cholesky" - default: \code{log_cholesky} for log Cholesky metric}
#' }
#' 
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' 
#' @examples
#' n=30 #sample size
#' m=5  #dimension of covariance matrices
#' x=cbind(matrix(rnorm(n),n),matrix(rnorm(n),n)) #vector of predictor values
#' M <- array(0,c(m,m,n))
#' a = rnorm(m); b = rnorm(m)
#' A = diag(m)+a%*%t(a);
#' B = diag(m)+3*b%*%t(b);
#' for (i in 1:n){
#'   aux <- x[i,1]*A + x[i,2]**2*B
#'   M[,,i] <- aux %*% t(aux)
#' }
#' xout=cbind(runif(5),runif(5)) #output predictor levels
#' Covlist <- GFRCovCholesky(x=x, M=M, xout=xout)
#' 
#' @references
#' \cite{A Petersen and HG Müller (2019). "Fréchet regression for random objects with Euclidean predictors." An. Stat. 47, 691-719.}
#' \cite{Z Lin (2019). " Riemannian Geometry of Symmetric Positive Definite Matrices via Cholesky Decomposition." Siam. J. Matrix. Anal, A. 40, 1353–1370.}
#' @importFrom Matrix forceSymmetric

GFRCovCholesky <- function(x, M, xout, optns = list()){
  
  if(!is.matrix(x)&!is.vector(x)){
    stop('x must be a matrix or vector')
  }
  if(!is.matrix(xout)&!is.vector(xout)){
    stop('xout must be a matrix or vector')
  }
  if(is.vector(x)){x<- matrix(x,length(x)) }
  
  if(is.vector(xout)){xout<- matrix(xout,length(xout)) }
  
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have same number of columns')
  }

  
  if(is.null(M)){
    stop("M must be provided")
  }
  if(is.list(M)){
    M=array(as.numeric(unlist(M)), dim=c(dim(M[[1]])[1],dim(M[[1]])[1],length(M)))
  }else{
    if(!is.array(M)){
      stop('M must be an array or a list')
    } else if (length(dim(M))!=3) {
      stop('M must be an array or a list')
    }
  }
  if(nrow(x)!=dim(M)[3]){
    stop("the number of rows of x must be the same as the number of covariance matrices in M")
  }
  
  if(is.null(optns$corrOut)){
    corrOut = FALSE
  } else {
    corrOut = optns$corrOut
  }
  
  if(is.null(optns$metric)){
    metric = 'log_cholesky'
  } else {
    metric =  optns$metric
  }

 
  
  n = nrow(x)
  p = ncol(x)
  nout = nrow(xout)
  
  invVa = solve(var(x))
  mx = apply(x,2,mean)
  
  MM = list()
  if(is.array(M)){
    for (i in 1:n) {
      MM[[i]] = M[,,i]
    }
  } else {MM = M}
  
  M = lapply(MM, function(X) (X+t(X))/2)
  Mout = list()
  if(metric == 'log_cholesky'){
    LL = lapply(M, chol)
    L = lapply(LL, function(X) X - diag(diag(X)))
    D = lapply(LL, function(X) diag(X))
    
    for (j in 1:nout) {
      ss = 0
      U = 0
      E = 0
      s = array(0,n)
      for (i in 1:n) {
        s[i] = 1+(x[i,]-mx)%*%invVa%*%(xout[j,]-mx)
        ss = ss + s[i]
        U = U + s[i]*L[[i]]
        E = E + s[i]*log(D[[i]])
      }
      SS = U/ss + diag(exp(E/ss))
      Mout[[j]] = t(SS)%*%SS
    }
  } else{
    L = lapply(M, chol)
    for (j in 1:nout) {
      ss = 0
      U = 0
      s = array(0,n)
      for (i in 1:n) {
        s[i] = 1+(x[i,]-mx)%*%invVa%*%(xout[j,]-mx)
        ss = ss + s[i]
        U = U + s[i]*L[[i]]
      }
      Mout[[j]] = t(U/ss) %*% (U/ss)
    }
  }
  
  if(corrOut){
    for(j in 1:nrow(xout)){
      D=diag(1/sqrt(diag(Mout[[j]])))
      Mout[[j]]=D%*%Mout[[j]]%*%D
      Mout[[j]]=as.matrix(Matrix::forceSymmetric(Mout[[j]]))
    }
  }
  out = list(xout=xout, Mout=Mout, optns=list(corrOut=corrOut,metric=metric))
  return(out)
}
  
  



#' @description Helper function computing bootstrap replications of
#'   the Fréchet CPD test statistics for networks.
#' @noRd

NetCPDStatistic <- function(data, indices, cutOff, bootSize = length(indices)) {
  LyBoot <- data[indices[1:bootSize]] # booted sample. m see bootstrap scheme in the paper
  n <- length(LyBoot) # number of observations
  scope <- ceiling(cutOff * n):(n - ceiling(cutOff * n))
  nTn <- sapply(scope, function(i) {
    n1 <- i
    n2 <- n - i
    lambda <- n1 / n
    LyBoot1 <- LyBoot[1:n1]
    LyBoot2 <- LyBoot[(n1 + 1):n]
    mup <- rowMeans(matrix(unlist(LyBoot), ncol = n)) # overall Frechet mean
    mu1 <- rowMeans(matrix(unlist(LyBoot1), ncol = n1)) # first part Frechet mean
    mu2 <- rowMeans(matrix(unlist(LyBoot2), ncol = n2)) # second part Frechet mean
    V1 <- mean(sapply(LyBoot1, function(LyBoot1i) sum((LyBoot1i - mu1)^2)))
    V2 <- mean(sapply(LyBoot2, function(LyBoot2i) sum((LyBoot2i - mu2)^2)))
    V1C <- mean(sapply(LyBoot1, function(LyBoot1i) sum((LyBoot1i - mu2)^2)))
    V2C <- mean(sapply(LyBoot2, function(LyBoot2i) sum((LyBoot2i - mu1)^2)))
    Di <- sapply(LyBoot, function(LyBooti) sum((LyBooti - mup)^2))
    sigma2 <- mean(Di^2) - mean(Di)^2
    nTni <- n * lambda * (1 - lambda) * ((V1 - V2)^2 + (V1C - V1 + V2C - V2)^2) / sigma2 # formula 2.5 nTn
    nTni
  })
  c(maxnTn = max(nTn), tau = which.max(nTn)) # test statistic and location of change point
}
#' @title Fréchet means of densities.
#' @description Obtain Fréchet means of densities with respect to \eqn{L^2}-Wasserstein distance.
#' @param yin A matrix or list holding the sample of measurements for the observed distributions. If \code{yin} is a matrix, each row holds the measurements for one distribution.
#' @param hin A list holding the histograms of an observed distribution.
#' @param qin A matrix or list holding the quantile functions of the response. If \code{qin} is a matrix, each row holds the quantile function of an observed distribution taking values on \code{optns$qSup}.
#' Note that only one of the three \code{yin}, \code{hin}, and \code{qin} needs to be input.
#' If more than one of them are specified, \code{yin} overwrites \code{hin}, and \code{hin} overwrites \code{qin}.
#' @param optns A list of options control parameters specified by \code{list(name=value)}.
#' @details Available control options are \code{qSup}, \code{nqSup}, 
#' \code{bwDen}, \code{ndSup}, \code{dSup}, \code{delta}, 
#' \code{kernelDen}, \code{infSupport}, and \code{denLowerThreshold}. 
#' See \code{\link{LocDenReg}} for details.
#' \describe{
#' \item{weights}{A vector of weights to compute the weighted barycenter. The length of \code{weights} is equal to the sample size. Default is equal weights.}
#' }
#' @return A list containing the following components:
#' \item{dout}{A numeric vector holding the density of the Fréchet mean.}
#' \item{dSup}{A numeric vector giving the domain grid of \code{dout} when it is a matrix.}
#' \item{qout}{A numeric vector holding the quantile function of the Fréchet mean.}
#' \item{qSup}{A numeric vector giving the domain grid of \code{qout}.}
#' \item{optns}{A list of control options used.}
#' @examples
#' xin = seq(0,1,0.05)
#' yin = lapply(xin, function(x) {
#'   rnorm(100, rnorm(1,x + x^2,0.005), 0.05)
#' })
#' res <- DenFMean(yin=yin)
#' plot(res)
#'
#' @export

DenFMean <- function(yin=NULL, hin=NULL, qin=NULL, optns=list()) {
  if (!is.null(yin)) {
    if (is.list(yin)) {
      xin <- rep(1, length(yin))
    } else if (is.matrix(yin)) {
      xin <- rep(1, nrow(yin))
    }
  } else {
    if (!is.null(hin)) {
      xin <- rep(1, length(hin))
    } else {
      if (is.list(qin)) {
        xin <- rep(1, length(qin))
      } else if (is.matrix(qin)) {
        xin <- rep(1, nrow(qin))
      }
    }
  }
  if(!is.null(optns$weights)){
    if(!is.vector(optns$weights)){
      stop("weights should be a vector")
    }
    if(length(xin)!=length(optns$weights)){
      stop("The length of weights cannot differ from the sample size")
    }
    if(sum(optns$weights<0)>0){
      stop("weights must be non-negative")
    }
    if(abs(sum(optns$weights)-1)>1e-15){
      stop("weights must sum to 1")
    }
    if(sum(abs(optns$weights-1/length(xin)))==0){
      #Case of equal weights requires different call to GloDenReg function
      res <- GloDenReg(xin = xin, yin = yin, hin = hin, qin = qin, optns = optns)
    }else{
      res <- GloDenReg(xin = optns$weights, yin = yin, hin = hin, qin = qin,xout=sum(optns$weights^2), optns = optns)
    }
  }else{
    res <- GloDenReg(xin = xin, yin = yin, hin = hin, qin = qin, optns = optns)
  }
  if (!is.vector(res$qout)) {
    res$qout <- as.vector(res$qout)
  }
  if (is.list(res$dout)) {
    res$dSup <- res$dout[[1]]$x
    res$dout <- res$dout[[1]]$y
  } else if (is.matrix(res$dout)) {
    res$dout <- as.vector(res$dout)
  }
  #with(res, list(dout = dout, dSup = dSup, qout = c(qout), qSup = qSup, optns = optns))
  class(res) <- "denReg"
  res
}
#'@title Power distance for covariance matrices
#'@noRd
#'@description Power distance computation for covariance matrices.
#' @param A an p by p matrix
#' @param B an p by p matrix
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{metric}{Metric type choice, \code{"frobenius"}, \code{"power"} - default: \code{"frobenius"} which corresponds to the power metric with \code{alpha} equal to 1.}
#' \item{alpha}{The power parameter for the power metric, which can be any non-negative number. Default is 1 which corresponds to frobenius metric.}
#' }
#' @return A list containing the following fields:
#' \item{dist}{The distance between the two covariance matrices in \code{A} and \code{B}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#'#Example
#'m=5 # dimension of covariance matrices
#'M <- array(0,c(m,m,2))
#'for (i in 1:2){
#'  y0=rnorm(m)
#'  aux<-diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'A=M[,,1]
#'B=M[,,2]
#' covDistance=CovFPowerDist(A=A,B=B,optns=list(metric="frobenius"))
#'
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2016). Fréchet integration and adaptive metric selection for interpretable covariances of multivariate functional data. Biometrika, 103, 103--120.}
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \item \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' }

CovFPowerDist= function(A=NULL,B=NULL, optns = list()){
  
  if(!is.matrix(A) | !is.matrix(B) ){
    stop('A and B must be of matrix class')
  }
  if(nrow(A)!=ncol(A) | nrow(B)!=ncol(B)){
    stop('Both A and B must be square matrices')
  }
  if(sum(dim(A)!=dim(B))>0){
    stop('Both A and B must have the same dimension')
  }
  
  if (is.null(optns$metric)){
    metric="frobenius"
  } else {
    metric=optns$metric
    if(is.null(optns$alpha)){
      optns$alpha=1
      alpha=1
    }
    else{
      alpha=optns$alpha
    }
  }
  if(!metric%in%c("frobenius","power")){
    stop("metric choice not supported.")
  }
  
  M <- array(0,c(nrow(A),ncol(A),2))
  M[,,1]=A
  M[,,2]=B
  
  if(metric=="frobenius"){
    dist <- sqrt(sum(diag((M[,,1]-M[,,2])%*%(M[,,1]-M[,,2]))))
  } else if(metric=="power"){
    if(alpha==1){
      dist <- sqrt(sum(diag((M[,,1]-M[,,2])%*%(M[,,1]-M[,,2]))))
    }
    else{
      if(alpha>0){
        #Transform M1
        P=eigen(M[,,1])$vectors
        Lambd_alpha=diag(pmax(0,eigen(M[,,1])$values)**alpha)
        M_alpha_1=P%*%Lambd_alpha%*%t(P)
        #Transform M2
        P=eigen(M[,,2])$vectors
        Lambd_alpha=diag(pmax(0,eigen(M[,,2])$values)**alpha)
        M_alpha_2=P%*%Lambd_alpha%*%t(P)       
        dist <- sqrt(sum(diag((M_alpha_1-M_alpha_2)%*%(M_alpha_1-M_alpha_2))))/alpha
      }
      else{
        if(alpha<0){
          stop('alpha has to be non-negative')
        }
        #Transform M1
        P=eigen(M[,,1])$vectors
        Lambd_alpha=diag(log(pmax(1e-30,eigen(M[,,1])$values)))
        M_alpha_1=P%*%Lambd_alpha%*%t(P)
        #Transform M2
        P=eigen(M[,,2])$vectors
        Lambd_alpha=diag(log(pmax(1e-30,eigen(M[,,2])$values)))
        M_alpha_2=P%*%Lambd_alpha%*%t(P)
        dist <- sqrt(sum(diag((M_alpha_1-M_alpha_2)%*%(M_alpha_1-M_alpha_2))))
      }
    }
  }
  res <- list(dist=dist,optns=optns)
  return(res)
}
#' @title Fréchet Variance for Densities
#' @description Obtain Fréchet variance for densities with respect to
#'   \eqn{L^2}-Wasserstein distance.
#' @param yin A matrix or data frame or list holding the sample of measurements 
#'   for the observed distributions. If \code{yin} is a matrix or data frame, 
#'   each row holds the measurements for one distribution.
#' @param hin A list holding the histograms for the observed distributions. 
#' @param din A matrix or data frame or list holding the density functions. 
#'   If \code{din} is a matrix or data frame, each row of \code{din} holds 
#'   the density function for one distribution.
#' @param qin A matrix or data frame or list holding the quantile functions. 
#'   If \code{qin} is a matrix or data frame, each row of \code{qin} holds 
#'   the quantile function for one distribution.
#' Note that the input can be only one of the four \code{yin}, \code{hin}, 
#' \code{din}, and \code{qin}. If more than one of them are specified, 
#' \code{yin} overwrites \code{hin}, \code{hin} overwrites \code{din}, 
#' and \code{din} overwrites \code{qin}.
#' @param supin A matrix or data frame or list holding the support grids of 
#'   the density functions in \code{din} or the quantile functions in \code{qin}. 
#'   If \code{supin} is a matrix or data frame, each row of \code{supin} holds 
#'   the support grid of the corresponding density function or quantile function.
#'   Ignored if the input is \code{yin} or \code{hin}.
#'   It can also be a vector if all density functions in \code{din} or 
#'   all quantile functions in \code{qin} have the same support grid.
#' @param optns A list of control parameters specified by
#'   \code{list(name = value)}. See `Details`.
#' @details Available control options are 
#' \describe{
#' \item{nqSup}{A scalar giving the number of the support points for 
#'   quantile functions based on which the \eqn{L^2} Wasserstein distance 
#'   (i.e., the \eqn{L^2} distance between the quantile functions) is computed. 
#'   Default is 201.}
#' \item{qSup}{A numeric vector holding the support grid on [0, 1] based on 
#'   which the \eqn{L^2} Wasserstein distance (i.e., the \eqn{L^2} distance 
#'   between the quantile functions) is computed. It overrides \code{nqSup}.}
#' \item{bwDen}{The bandwidth value used in \code{CreateDensity()} for
#'   density estimation; positive numeric - default: determine automatically 
#'   based on the data-driven bandwidth selector proposed by 
#'   Sheather and Jones (1991).}
#' \item{ndSup}{A scalar giving the number of support points the kernel density 
#'   estimation used in \code{CreateDensity()}; numeric - default: 101.}
#' \item{dSup}{User defined output grid for the support of 
#'   kernel density estimation used in \code{CreateDensity()}, 
#'   it overrides \code{ndSup}.}
#' \item{delta}{A scalar giving the size of the bin to be used used in 
#'   \code{CreateDensity()}; numeric - default: \code{diff(range(y))/1000}. 
#'   It only works when the raw sample is available.}
#' \item{kernelDen}{A character holding the type of kernel functions used in 
#'   \code{CreateDensity()} for density estimation; \code{"rect"}, 
#'   \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, 
#'   \code{"quar"} - default: \code{"gauss"}.}
#' \item{infSupport}{logical if we expect the distribution to have 
#'   infinite support or not, used in \code{CreateDensity()} for 
#'   density estimation; logical - default: \code{FALSE}}
#' \item{denLowerThreshold}{\code{FALSE} or a positive value giving 
#'   the lower threshold of the densities used in \code{CreateDensity()}; 
#'   default: \code{0.001 * mean(qin[,ncol(qin)] - qin[,1])}.}
#' }
#' @return A list containing the following fields:
#' \item{DenFVar}{A scalar holding the Fréchet variance.}
#' \item{optns}{A list of control options used.}
#' @examples
#' set.seed(1)
#' n <- 100
#' mu <- rnorm(n, mean = 0, sd = 0.5)
#' qSup <- seq(0.01, 0.99, (0.99 - 0.01) / 50)
#' Ly <- lapply(1:n, function(i) qnorm(qSup, mu[i], sd = 1))
#' Lx <- qSup
#' res <- DenFVar(qin = Ly, supin = Lx)
#' res$DenFVar
#' @importFrom fdadensity dens2quantile
#' @importFrom pracma trapz
#' @export

DenFVar <- function(yin = NULL, hin = NULL, din = NULL, qin = NULL, 
                    supin = NULL, optns = list()) {
  if (is.null(yin) & is.null(qin) & is.null(hin) & is.null(din)) {
    stop ("one of the four arguments, yin, hin, din, and qin, should be inputted by users")
  }
  if (!is.null(yin)) {
    if (!(is.null(hin) & is.null(din) & is.null(qin))) {
      warning ("hin, din, and qin are redundant when yin is available")
    }
    tin <- 1 # type of input
    ls <- yin
  } else if (!is.null(hin)) {
    if (!(is.null(din) & is.null(qin))) {
      warning ("din and qin are redundant when hin is available")
    }
    tin <- 2 # type of input
    ls <- hin
  } else if (!is.null(din)) {
    if (!is.null(qin)) {
      warning ("qin is redundant when din is available")
    }
    tin <- 3 # type of input
    ls <- din
    if (!is.list(din)) {
      if (is.matrix(din) | is.data.frame(din)) {
        din <- lapply(1:nrow(din), function(i) din[i, ])
      } else {
        stop("din must be a matrix or data frame or list")
      }
    }
  } else {
    tin <- 4 # type of input
    ls <- qin
  }
  if (tin == 2) {
    if (!is.list(ls)) {
      stop("hin must be a list")
    }
    for (histogram in ls) {
      if (!is.list(histogram))
        stop("each element of hin must be a list")
      if (is.null(histogram$breaks) & is.null(histogram$mids))
        stop("each element of hin must be a list with at least one of the components breaks or mids")
      if (is.null(histogram$counts))
        stop("each element of hin must be a list with component counts")
    }
  } else {
    if (!is.list(ls)) {
      if (is.matrix(ls) | is.data.frame(ls)) {
        ls <- lapply(1:nrow(ls), function(i) ls[i, ])
      } else {
        if (tin == 1) {
          stop("yin must be a matrix or data frame or list")
        } else if (tin == 3) {
          stop("din must be a matrix or data frame or list")
        } else {
          stop("qin must be a matrix or data frame or list")
        }
      }
    }
  }
  n <- length(ls)
  if (tin > 2) {# if din or qin
    if (!is.null(supin)) {
      if (!is.list(supin)) {
        if (is.matrix(supin) | is.data.frame(supin)) {
          supin <- lapply(1:nrow(supin), function(i) supin[i, ])
        } else if (is.vector(supin)) {
          supin <- rep(list(supin), n)
        } else {
          stop("supin must be a vector or matrix or data frame or list")
        }
      }
      if (length(supin) != n) {
        if (tin == 3) {
          stop("the number of support grids in supin is not equal to the number of observed distributions in din")
        } else {
          stop("the number of support grids in supin is not equal to the number of observed distributions in qin")
        }
      }
      if (sum(sapply(supin, length) - sapply(ls, length))) {
        if (tin == 3) {
          stop("the number of support points must be equal to the number of observations for each density function in din")
        } else {
          stop("the number of support points must be equal to the number of observations for each quantile function in qin")
        }
      }
    } else {
      if (tin == 3) {
        stop("requires the input of supin for din")
      } else {
        stop("requires the input of supin for qin")
      }
    }
  }
  if (!is.null(optns$qSup)) {
    if (min(optns$qSup) != 0 | max(optns$qSup) - 1 != 0)
      stop ("optns$qSup must have minimum 0 and maximum 1")
    if (sum(duplicated(optns$qSup)) > 0) {
      optns$qSup <- unique(optns$qSup)
      warning ("optns$qSup has duplicated elements which has been removed")
    }
    if (is.unsorted(optns$qSup)) {
      optns$qSup <- sort(optns$qSup)
      warning ("optns$qSup has been reordered to be increasing")
    }
  } else {
    if (tin < 4) {# if not qin
      if(is.null(optns$nqSup)) {
        optns$nqSup <- 201
      }
      optns$qSup <- seq(0,1,length.out = optns$nqSup)
    } else {# if qin
      diffSupp <- TRUE
      if (length(unique(sapply(supin, length))) == 1) {
        if (!sum(diff(matrix(unlist(supin), nrow = n, byrow = TRUE)))) {
          diffSupp <- FALSE
        }
      }
      if (diffSupp) {
        if(is.null(optns$nqSup)) {
          optns$nqSup <- 201
        }
        optns$qSup <- seq(0,1,length.out = optns$nqSup)
        ls <- lapply(1:n, function(i) approx(x = supin[[i]], y = ls[[i]], xout = optns$qSup)$y)
      } else {
        optns$qSup <- supin[[1]]
        optns$nqSup <- length(optns$qSup)
      }
    }
  }
  qSup <- optns$qSup
  if (tin == 3) {
    ls <- lapply(1:n, function(i) fdadensity::dens2quantile(ls[[i]], dSup = supin[[i]], qSup = qSup))
  }
  if (tin < 3) {
    optnsDen <- optns
    if (!is.null(optnsDen$kernelDen))
      names(optnsDen)[which(names(optnsDen) == "kernelDen")] <- "kernel"
    if (!is.null(optnsDen$bwDen))
      names(optnsDen)[which(names(optnsDen) == "bwDen")] <- "userBwMu"
    if (!is.null(optnsDen$ndSup))
      names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
    if (!is.null(optnsDen$dSup))
      names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"
    if (tin == 1) {
      den <- lapply(ls, CreateDensity, optns = optnsDen)
    } else {
      den <- lapply(ls, function(histogram) {
        CreateDensity(histogram = histogram, optns = optnsDen)
      })
    }
    ls <- lapply(den, function(deni) fdadensity::dens2quantile(deni$y, dSup = deni$x, qSup = qSup))
  }
  mup <- rowMeans(matrix(unlist(ls), nrow = length(qSup), ncol = n))
  Vp <- mean(sapply(ls, function(lsi) {
    pracma::trapz(qSup, (lsi - mup)^2)
  }))
  res <- list(DenFVar = Vp, optns = optns)
  res
}
#' @title the Log-Cholesky and Cholesky distance between symmetric positive definite 
#' @noRd
#' @description the Log-Cholesky and Cholesky distance between two matrices
#' @param A an p by p matrix
#' @param B an p by p matrix
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are 
#' \describe{
#' \item{metric}{Metric type choice, "log_cholesky", "cholesky" - default: \code{log_cholesky} for log Cholesky metric}
#' }
#' @return A list containing the following fields:
#' \item{dist}{the distance between matrices \code{A} and \code{B}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' p <- 3
#' a <- matrix(rnorm(p*p),p,p)
#' b <- matrix(rnorm(p*p),p,p)
#' A <- a %*% t(a)
#' B <- b %*% t(b)
#' res <- DistCholesky(A,B)

DistCholesky <- function(A=NULL,B=NULL,optns = list()){
  
  if(!is.matrix(A) | !is.matrix(B) ){
    stop('A and B must be of matrix class')
  }
  if(nrow(A)!=ncol(A) | nrow(B)!=ncol(B)){
    stop('Both A and B must be square matrices')
  }
  if(sum(dim(A)!=dim(B))>0){
    stop('Both A and B must have the same dimension')
  }

  
  if(is.null(optns$metric)){
    metric = 'log_cholesky'
  } else {
    metric =  optns$metric
  }
  
  p <- dim(A)[1]
  M.a <- chol(A)
  D.a <- diag(M.a)
  L.a <- M.a - diag(D.a)
  
  M.b <- chol(B)
  D.b <- diag(M.b)
  L.b <- M.b - diag(D.b)  
  
  # if( (D.a[p] <= 0) | (D.b[p] <= 0) | sum(A!=t(A))>0 |sum(B!=t(B))>0 ){
  #   stop('Both A and B must be symmetric positive definite matrices')
  # }
  # 
  if(metric == 'log_cholesky'){
    dist <- sqrt( sum((L.a-L.b)^2)+sum((log(D.a)-log(D.b))^2) ) 
  }
  if(metric == 'cholesky'){
    dist <- sqrt(sum((M.a-M.b)^2)) 
  }
  return(list(dist=dist,optns=list(metric=metric)))
}
#' @title Fréchet Change Point Detection for Densities
#' @description Fréchet change point detection for densities with respect to
#'   \eqn{L^2}-Wasserstein distance.
#' @param yin A matrix or data frame or list holding the sample of measurements
#'   for the observed distributions. If \code{yin} is a matrix or data frame,
#'   each row holds the measurements for one distribution.
#' @param hin A list holding the histograms for the observed distributions.
#' @param din A matrix or data frame or list holding the density functions.
#'   If \code{din} is a matrix or data frame, each row of \code{din} holds
#'   the density function for one distribution.
#' @param qin A matrix or data frame or list holding the quantile functions.
#'   If \code{qin} is a matrix or data frame, each row of \code{qin} holds
#'   the quantile function for one distribution.
#' Note that the input can be only one of the four \code{yin}, \code{hin},
#' \code{din}, and \code{qin}. If more than one of them are specified,
#' \code{yin} overwrites \code{hin}, \code{hin} overwrites \code{din},
#' and \code{din} overwrites \code{qin}.
#' @param supin A matrix or data frame or list holding the support grids of
#'   the density functions in \code{din} or the quantile functions in \code{qin}.
#'   If \code{supin} is a matrix or data frame, each row of \code{supin} holds
#'   the support grid of the corresponding density function or quantile function.
#'   Ignored if the input is \code{yin} or \code{hin}.
#'   It can also be a vector if all density functions in \code{din} or
#'   all quantile functions in \code{qin} have the same support grid.
#' @param optns A list of control parameters specified by
#'   \code{list(name = value)}. See `Details`.
#' @details Available control options are
#' \describe{
#' \item{cutOff}{A scalar between 0 and 1 indicating the interval,
#'   i.e., [cutOff, 1 - cutOff], in which candidate change points lie.}
#' \item{Q}{A scalar representing the number of Monte Carlo simulations to run
#'   while approximating the critical value (stardized Brownian bridge).
#'   Default is 1000.}
#' \item{boot}{Logical, also compute bootstrap \eqn{p}-value if \code{TRUE}.
#'   Default is \code{FALSE}.}
#' \item{R}{The number of bootstrap replicates. Only used when \code{boot}
#'   is \code{TRUE}. Default is 1000.}
#' \item{nqSup}{A scalar giving the number of the support points for
#'   quantile functions based on which the \eqn{L^2} Wasserstein distance
#'   (i.e., the \eqn{L^2} distance between the quantile functions) is computed.
#'   Default is 201.}
#' \item{qSup}{A numeric vector holding the support grid on [0, 1] based on
#'   which the \eqn{L^2} Wasserstein distance (i.e., the \eqn{L^2} distance
#'   between the quantile functions) is computed. It overrides \code{nqSup}.}
#' \item{bwDen}{The bandwidth value used in \code{CreateDensity()} for
#'   density estimation; positive numeric - default: determine automatically
#'   based on the data-driven bandwidth selector proposed by
#'   Sheather and Jones (1991).}
#' \item{ndSup}{A scalar giving the number of support points the kernel density
#'   estimation used in \code{CreateDensity()}; numeric - default: 101.}
#' \item{dSup}{User defined output grid for the support of
#'   kernel density estimation used in \code{CreateDensity()},
#'   it overrides \code{ndSup}.}
#' \item{delta}{A scalar giving the size of the bin to be used used in
#'   \code{CreateDensity()}; numeric - default: \code{diff(range(y))/1000}.
#'   It only works when the raw sample is available.}
#' \item{kernelDen}{A character holding the type of kernel functions used in
#'   \code{CreateDensity()} for density estimation; \code{"rect"},
#'   \code{"gauss"}, \code{"epan"}, \code{"gausvar"},
#'   \code{"quar"} - default: \code{"gauss"}.}
#' \item{infSupport}{logical if we expect the distribution to have
#'   infinite support or not, used in \code{CreateDensity()} for
#'   density estimation; logical - default: \code{FALSE}}
#' \item{denLowerThreshold}{\code{FALSE} or a positive value giving
#'   the lower threshold of the densities used in \code{CreateDensity()};
#'   default: \code{0.001 * mean(qin[,ncol(qin)] - qin[,1])}.}
#' }
#' @return A \code{DenCPD} object --- a list containing the following fields:
#' \item{tau}{a scalar holding the estimated change point.}
#' \item{pvalAsy}{a scalar holding the asymptotic \eqn{p}-value.}
#' \item{pvalBoot}{a scalar holding the bootstrap \eqn{p}-value.
#'   Returned if \code{optns$boot} is TRUE.}
#' \item{optns}{the control options used.}
#' @examples
#' \donttest{
#' set.seed(1)
#' n1 <- 100
#' n2 <- 200
#' delta <- 0.75
#' qSup <- seq(0.01, 0.99, (0.99 - 0.01) / 50)
#' mu1 <- rnorm(n1, mean = delta, sd = 0.5)
#' mu2 <- rnorm(n2, mean = 0, sd = 0.5)
#' Y1 <- lapply(1:n1, function(i) {
#'   qnorm(qSup, mu1[i], sd = 1)
#' })
#' Y2 <- lapply(1:n2, function(i) {
#'   qnorm(qSup, mu2[i], sd = 1)
#' })
#' Ly <- c(Y1, Y2)
#' Lx <- qSup
#' res <- DenCPD(qin = Ly, supin = Lx, optns = list(boot = TRUE))
#' res$tau # returns the estimated change point
#' res$pvalAsy # returns asymptotic pvalue
#' res$pvalBoot # returns bootstrap pvalue
#' } 
#' @references
#' \itemize{
#' \item \cite{Dubey, P. and Müller, H.G., 2020. Fréchet change-point detection. The Annals of Statistics, 48(6), pp.3312-3335.}
#' }
#' @importFrom e1071 rbridge
#' @importFrom fdadensity dens2quantile
#' @importFrom boot boot
#' @export

DenCPD <- function(yin = NULL, hin = NULL, din = NULL, qin = NULL,
                   supin = NULL, optns = list()) {
  if (is.null(yin) & is.null(qin) & is.null(hin) & is.null(din)) {
    stop("one of the four arguments, yin, hin, din, and qin, should be inputted by users")
  }
  if (!is.null(yin)) {
    if (!(is.null(hin) & is.null(din) & is.null(qin))) {
      warning("hin, din, and qin are redundant when yin is available")
    }
    tin <- 1 # type of input
    ls <- yin
  } else if (!is.null(hin)) {
    if (!(is.null(din) & is.null(qin))) {
      warning("din and qin are redundant when hin is available")
    }
    tin <- 2 # type of input
    ls <- hin
  } else if (!is.null(din)) {
    if (!is.null(qin)) {
      warning("qin is redundant when din is available")
    }
    tin <- 3 # type of input
    ls <- din
    if (!is.list(din)) {
      if (is.matrix(din) | is.data.frame(din)) {
        din <- lapply(1:nrow(din), function(i) din[i, ])
      } else {
        stop("din must be a matrix or data frame or list")
      }
    }
  } else {
    tin <- 4 # type of input
    ls <- qin
  }
  if (tin == 2) {
    if (!is.list(ls)) {
      stop("hin must be a list")
    }
    for (histogram in ls) {
      if (!is.list(histogram)) {
        stop("each element of hin must be a list")
      }
      if (is.null(histogram$breaks) & is.null(histogram$mids)) {
        stop("each element of hin must be a list with at least one of the components breaks or mids")
      }
      if (is.null(histogram$counts)) {
        stop("each element of hin must be a list with component counts")
      }
    }
  } else {
    if (!is.list(ls)) {
      if (is.matrix(ls) | is.data.frame(ls)) {
        ls <- lapply(1:nrow(ls), function(i) ls[i, ])
      } else {
        if (tin == 1) {
          stop("yin must be a matrix or data frame or list")
        } else if (tin == 3) {
          stop("din must be a matrix or data frame or list")
        } else {
          stop("qin must be a matrix or data frame or list")
        }
      }
    }
  }
  n <- length(ls)
  if (is.null(optns$cutOff)) {
    optns$cutOff <- 0.1
  }
  if (is.null(optns$Q)) {
    optns$Q <- 1000
  }
  if (is.null(optns$boot)) {
    optns$boot <- FALSE
  }
  if (optns$boot) {
    if (is.null(optns$R)) {
      optns$R <- 1000
    }
  }
  if (tin > 2) { # if din or qin
    if (!is.null(supin)) {
      if (!is.list(supin)) {
        if (is.matrix(supin) | is.data.frame(supin)) {
          supin <- lapply(1:nrow(supin), function(i) supin[i, ])
        } else if (is.vector(supin)) {
          supin <- rep(list(supin), n)
        } else {
          stop("supin must be a vector or matrix or data frame or list")
        }
      }
      if (length(supin) != n) {
        if (tin == 3) {
          stop("the number of support grids in supin is not equal to the number of observed distributions in din")
        } else {
          stop("the number of support grids in supin is not equal to the number of observed distributions in qin")
        }
      }
      if (sum(sapply(supin, length) - sapply(ls, length))) {
        if (tin == 3) {
          stop("the number of support points must be equal to the number of observations for each density function in din")
        } else {
          stop("the number of support points must be equal to the number of observations for each quantile function in qin")
        }
      }
    } else {
      if (tin == 3) {
        stop("requires the input of supin for din")
      } else {
        stop("requires the input of supin for qin")
      }
    }
  }
  if (!is.null(optns$qSup)) {
    if (min(optns$qSup) != 0 | max(optns$qSup) - 1 != 0) {
      stop("optns$qSup must have minimum 0 and maximum 1")
    }
    if (sum(duplicated(optns$qSup)) > 0) {
      optns$qSup <- unique(optns$qSup)
      warning("optns$qSup has duplicated elements which has been removed")
    }
    if (is.unsorted(optns$qSup)) {
      optns$qSup <- sort(optns$qSup)
      warning("optns$qSup has been reordered to be increasing")
    }
  } else {
    if (tin < 4) { # if not qin
      if (is.null(optns$nqSup)) {
        optns$nqSup <- 201
      }
      optns$qSup <- seq(0, 1, length.out = optns$nqSup)
    } else { # if qin
      diffSupp <- TRUE
      if (length(unique(sapply(supin, length))) == 1) {
        if (!sum(diff(matrix(unlist(supin), nrow = n, byrow = TRUE)))) {
          diffSupp <- FALSE
        }
      }
      if (diffSupp) {
        if (is.null(optns$nqSup)) {
          optns$nqSup <- 201
        }
        optns$qSup <- seq(0, 1, length.out = optns$nqSup)
        ls <- lapply(1:n, function(i) approx(x = supin[[i]], y = ls[[i]], xout = optns$qSup)$y)
      } else {
        optns$qSup <- supin[[1]]
        optns$nqSup <- length(optns$qSup)
      }
    }
  }
  qSup <- optns$qSup
  if (tin == 3) {
    ls <- lapply(1:n, function(i) fdadensity::dens2quantile(ls[[i]], dSup = supin[[i]], qSup = qSup))
  }
  if (tin < 3) {
    optnsDen <- optns
    if (!is.null(optnsDen$kernelDen)) {
      names(optnsDen)[which(names(optnsDen) == "kernelDen")] <- "kernel"
    }
    if (!is.null(optnsDen$bwDen)) {
      names(optnsDen)[which(names(optnsDen) == "bwDen")] <- "userBwMu"
    }
    if (!is.null(optnsDen$ndSup)) {
      names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
    }
    if (!is.null(optnsDen$dSup)) {
      names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"
    }
    if (tin == 1) {
      den <- lapply(ls, CreateDensity, optns = optnsDen)
    } else {
      den <- lapply(ls, function(histogram) {
        CreateDensity(histogram = histogram, optns = optnsDen)
      })
    }
    ls <- lapply(den, function(deni) fdadensity::dens2quantile(deni$y, dSup = deni$x, qSup = qSup))
  }

  nc <- ceiling(n * optns$cutOff)
  sbb <- sapply(1:optns$Q, function(i) { # standardized Brownian bridge
    bu <- e1071::rbridge(frequency = n)[nc:(n - nc)]
    u <- (nc:(n - nc)) / n
    max(bu^2 / (u * (1 - u)))
  })
  tTau <- DenCPDStatistic(ls, 1:n, optns$cutOff, qSup, n)
  maxnTn <- tTau[1] # the test statistic
  tau <- tTau[2] + nc - 1 # estimated change point
  pvalAsy <- length(which(sbb > maxnTn)) / optns$Q
  if (optns$boot) {
    bootSize <- n # check bootstrap scheme in the paper
    bootRes <- boot::boot(
      data = ls, statistic = DenCPDStatistic,
      R = optns$R, cutOff = optns$cutOff, qSup = qSup, bootSize = bootSize)
    pvalBoot <- length(which(bootRes$t[, 1] > bootRes$t0[1])) / optns$R
    res <- list(tau = tau, pvalAsy = pvalAsy, pvalBoot = pvalBoot, optns = optns)
  } else {
    res <- list(tau = tau, pvalAsy = pvalAsy, optns = optns)
  }
  class(res) <- "DenCPD"
  res
}
#' @description Helper function computing bootstrap replications of
#'   the Fréchet ANOVA test statistics for networks.
#' @noRd

NetANOVAStatistic <- function(data, indices, sizes) {
  k <- length(sizes) # number of groups
  n <- sum(sizes) # total number of observations
  LyBoot <- data[indices] # booted sample
  lambda <- sizes / n # ratio for each group
  groupData <- split(LyBoot, rep(1:k, sizes))
  mup <- rowMeans(matrix(unlist(LyBoot), ncol = n))
  Vp <- mean(sapply(LyBoot, function(LyBooti) sum((LyBooti - mup)^2)))
  V <- rep(0, k)
  sigma2 <- rep(0, k)
  for (i in 1:k) {
    mui <- rowMeans(matrix(unlist(groupData[[i]]), ncol = sizes[i]))
    Di <- sapply(groupData[[i]], function(Lyi) sum((Lyi - mui)^2))
    V[i] <- mean(Di)
    sigma2[i] <- mean(Di^2) - (mean(Di))^2
  }
  Fn <- Vp - sum(lambda * V)
  Un <- 0
  for (i in 1:(k - 1)) {
    for (j in (i + 1):k) {
      Un <- Un + lambda[i] * lambda[j] * (V[i] - V[j])^2 / (sigma2[i] * sigma2[j])
    }
  }
  Tn <- n * Un / sum(lambda / sigma2) + n * Fn^2 / sum(lambda^2 * sigma2)
  Tn
}
#' @title Distance between covariance matrices 
#'  
#' @description Distance computation between two covariance matrices
#' @param A an p by p matrix
#' @param B an p by p matrix
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are 
#' \describe{
#' \item{metric}{Metric type choice, \code{"frobenius"}, \code{"power"}, \code{"log_cholesky"} and \code{"cholesky"} - default: \code{"frobenius"}, which corresponds to the power metric with \code{alpha} equal to 1.}
#' \item{alpha}{The power parameter for the power metric, which can be any non-negative number. Default is 1 which corresponds to Frobenius metric.}
#' }
#' @return A list containing the following fields:
#' \item{dist}{the distance between covariance matrices \code{A} and \code{B}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples 
#'# M input as array
#' m <- 5 # dimension of covariance matrices
#' M <- array(0,c(m,m,2))
#' for (i in 1:2) {
#'  y0 <- rnorm(m)
#'  aux <- diag(m) + y0 %*% t(y0)
#'  M[,,i] <- aux
#' }
#' A <- M[,,1]
#' B <- M[,,2]
#' frobDist <- dist4cov(A=A, B=B, optns=list(metric="frobenius"))
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2016). Fréchet integration and adaptive metric selection for interpretable covariances of multivariate functional data. Biometrika, 103, 103--120.}
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \item \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' }
#' @export

dist4cov= function(A=NULL,B=NULL, optns = list()){
  if(!is.matrix(A) | !is.matrix(B) ){
    stop('A and B must be of matrix class')
  }
  if(nrow(A)!=ncol(A) | nrow(B)!=ncol(B)){
    stop('Both A and B must be square matrices')
  }
  if(sum(dim(A)!=dim(B))>0){
    stop('Both A and B must have the same dimension')
  }
  
  if (is.null(optns$metric)){
    metric="frobenius"
  } else {
    metric=optns$metric
    if(metric%in%c("power")){
      if(is.null(optns$alpha)){
        optns$alpha=1
      }
    }
  }
  if(!metric%in%c("frobenius","power","log_cholesky","cholesky")){
    stop("metric choice is not supported.")
  }
  if(metric%in%c("frobenius","power")){
    res=CovFPowerDist(A=A,B=B,optns=optns)
  }
  else{
    res=DistCholesky(A=A,B=B,optns=optns)
  }
  return(res)
}
  
  
  
  
  
  #' @title Local density regression.
#' @description Local Fréchet regression for densities with respect to \eqn{L^2}-Wasserstein distance.
#' @param xin An n by p matrix or a vector of length n if p=1 holding the n observations of the predictor.
#' @param yin A matrix or list holding the sample of observations of the response. If \code{yin} is a matrix, each row holds the observations of the response corresponding to a predictor value in the corresponding row of \code{xin}.
#' @param hin A list holding the histograms of the response corresponding to each predictor value in the corresponding row of \code{xin}.
#' @param qin A matrix or list holding the quantile functions of the response. If \code{qin} is a matrix, the support of the quantile functions should be the same (i.e., \code{optns$qSup}), and each row of \code{qin} holds the quantile function corresponding to a predictor value in the corresponding row of \code{xin}. If the quantile functions are evaluated on different grids, then \code{qin} should be a list, each element consisting of two components \code{x} and \code{y} holding the support grid and the corresponding values of the quantile functions, respectively.
#' Note that only one of the three \code{yin}, \code{hin}, and \code{qin} needs to be input.
#' If more than one of them are specified, \code{yin} overwrites \code{hin}, and \code{hin} overwrites \code{qin}.
#' @param xout An m by p matrix or a vector of length m if p=1 holding the m output predictor values. Default is \code{xin}.
#' @param optns A list of control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{bwReg}{A vector of length p used as the bandwidth for the Fréchet regression or \code{"CV"} (default), i.e., a data-adaptive selection done by cross-validation.}
#' \item{kernelReg}{A character holding the type of kernel functions for local Fréchet regression for densities; \code{"rect"}, \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, \code{"quar"} - default: \code{"gauss"}.}
#' \item{qSup}{A numeric vector holding the grid on [0,1] quantile functions take value on. Default is an equidistant grid.}
#' \item{nqSup}{A scalar giving the length of \code{qSup}. Default is 201.}
#' \item{lower}{A scalar with the lower bound of the support of the distribution. Default is \code{NULL}.}
#' \item{upper}{A scalar with the upper bound of the support of the distribution. Default is \code{NULL}.}
#' \item{bwRange}{A 2 by p matrix whose columns contain the bandwidth selection range for each corresponding dimension of the predictor \code{xin} for the case when \code{bwReg} equals \code{"CV"}. Default is \code{NULL} and is automatically chosen by a data-adaptive method.}
#' \item{bwDen}{The bandwidth value used in \code{CreateDensity()} for density estimation; positive numeric - default: determine automatically based on the data-driven bandwidth selector proposed by Sheather and Jones (1991).}
#' \item{ndSup}{The number of support points the kernel density estimation uses in \code{CreateDensity()}; numeric - default: 101.}
#' \item{dSup}{User defined output grid for the support of kernel density estimation used in \code{CreateDensity()}, it overrides \code{nRegGrid}; numeric - default: \code{NULL}}
#' \item{delta}{The size of the bin to be used used in \code{CreateDensity()}; numeric - default: \code{diff(range(y))/1000}. It only works when the raw sample is available.}
#' \item{kernelDen}{A character holding the type of kernel functions used in \code{CreateDensity()} for density estimation; \code{"rect"}, \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, \code{"quar"} - default: \code{"gauss"}.}
#' \item{infSupport}{logical if we expect the distribution to have infinite support or not, used in \code{CreateDensity()} for density estimation; logical - default: \code{FALSE}}
#' \item{denLowerThreshold}{\code{FALSE} or a positive value giving the lower threshold of the densities used in \code{CreateDensity()}; default: \code{0.001 * mean(qin[,ncol(qin)] - qin[,1])}.}
#' }
#' @return A list containing the following components:
#' \item{xout}{Input \code{xout}.}
#' \item{dout}{A matrix or list holding the output densities corresponding to \code{xout}. If \code{dout} is a matrix, each row gives a density and the domain grid is given in \code{dSup}. If \code{dout} is a list, each element is a list of two components, \code{x} and \code{y}, giving the domain grid and density function values, respectively.}
#' \item{dSup}{A numeric vector giving the domain grid of \code{dout} when it is a matrix.}
#' \item{qout}{A matrix holding the quantile functions of the output densities. Each row corresponds to a value in \code{xout}.}
#' \item{qSup}{A numeric vector giving the domain grid of \code{qout}.}
#' \item{xin}{Input \code{xin}.}
#' \item{din}{Densities corresponding to the input \code{yin}, \code{hin} or \code{qin}.}
#' \item{qin}{Quantile functions corresponding to the input \code{yin}, \code{hin} or \code{qin}.}
#' \item{optns}{A list of control options used.}
#'
#' @examples
#' \donttest{
#' xin = seq(0,1,0.05)
#' yin = lapply(xin, function(x) {
#'   rnorm(100, rnorm(1,x + x^2,0.005), 0.05)
#' })
#' qSup = seq(0,1,0.02)
#' xout = seq(0,1,0.1)
#' res1 <- LocDenReg(xin=xin, yin=yin, xout=xout, optns = list(bwReg = 0.12, qSup = qSup))
#' plot(res1)
#' xout <- xin
#' hin = lapply(yin, function(y) hist(y, breaks = 50))
#' res2 <- LocDenReg(xin=xin, hin=hin, xout=xout, optns = list(qSup = qSup))
#' plot(res2)
#'}
#' @references
#' \cite{Petersen, A., & Müller, H.-G. (2019). "Fréchet regression for random objects with Euclidean predictors." The Annals of Statistics, 47(2), 691--719.}
#' @export
#' @importFrom fdadensity dens2quantile
#' @importFrom pracma trapz

LocDenReg <- function(xin=NULL, yin=NULL, hin=NULL, qin=NULL, xout=NULL, optns=list()) {
  
  if (is.null(xin))
    stop ("xin has no default and must be input by users.")
  
  if(!is.matrix(xin)&!is.vector(xin)){
    stop('xin must be a matrix or vector')
  }
  xinVec <- xoutVec <- FALSE
  if(is.vector(xin)){
    xinVec <- TRUE
    xin<- matrix(xin,length(xin))
  }
  if (is.null(xout)){
    xout <- xin
  }
  if(!is.matrix(xout)&!is.vector(xout)){
    stop('xout must be a matrix or vector')
  }
  if(is.vector(xout)){
    xoutVec <- TRUE
    xout<- matrix(xout,length(xout))
  }
  if(ncol(xin) != ncol(xout)){
    stop('xin and xout must have the same number of columns')
  }
  if(ncol(xin)>2){
    stop('The dimension p of the predictor must be at most two')
  }
  if(is.matrix(qin)){
    if(nrow(qin)!=nrow(xin)){
      stop('qin (as matrix) and xin must have the same number of rows')
    }
  }
  if(is.list(qin)){
    if(length(qin)!=nrow(xin)){
      stop('qin (as list) and xin must have the same number of elements and rows, respectively')
    }
  }
  if(is.matrix(yin)){
    if(nrow(yin)!=nrow(xin)){
      stop('yin (as matrix) and xin must have the same number of rows')
    }
  }
  if(is.list(yin)){
    if(length(yin)!=nrow(xin)){
      stop('yin (as list) and xin must have the same number of elements and rows, respectively')
    }
  }
  if(!is.null(optns$bwRange)){
    if(!is.matrix(optns$bwRange)&!is.vector(optns$bwRange)){
      stop('bwRange must be a matrix or vector')
    }
    if(is.vector(optns$bwRange)){
      optns$bwRange<- matrix(optns$bwRange,length(optns$bwRange))
      if(ncol(xin)>1){
        stop('bwRange must be a matrix')
      }else{
        if(nrow(optns$bwRange)!=2){
          stop('bwRange must have the lower and upper bound for the bandwidth range')
        }
      }
    }
    else{
      if(ncol(optns$bwRange)!=ncol(xin)){
        stop('bwRange must have the same number of columns as xin')
      }
      if(nrow(optns$bwRange)!=2){
        stop('bwRange must have two rows')
      }
    }
    if(sum(optns$bwReg<=0)>0){
      stop('bwReg must contain positive bandwidths')
    }
  }
  
  if (is.null(yin) & is.null(qin) & is.null(hin))
    stop ("One of the three arguments, yin, hin and qin, should be input by users.")
  
  if (!is.null(optns$qSup)) {
    if (min(optns$qSup) != 0 | max(optns$qSup) - 1 != 0)
      stop ("optns$qSup must have minimum 0 and maximum 1.")
    if (sum(duplicated(optns$qSup)) > 0) {
      optns$qSup <- unique(optns$qSup)
      warning ("optns$qSup has duplicated elements which has been removed.")
    }
    if (is.unsorted(optns$qSup)) {
      optns$qSup <- sort(optns$qSup)
      warning ("optns$qSup has been reordered to be increasing.")
    }
  } else {
    if (!(is.null(yin) & is.null(hin))) {
      if(is.null(optns$nqSup)) {
        optns$nqSup <- 201
      }
      optns$qSup <- seq(0,1,length.out = optns$nqSup)
    } else {
      if (is.matrix(qin)) {
        optns$qSup <- seq(0,1,length.out = ncol(qin))
        warning ("optns$qSup is missing and is set by default as an equidistant grid on [0,1] with length equal to the number of columns in matrix qin.")
      } else {
        if(is.null(optns$nqSup)) {
          optns$nqSup <- 201
        }
        optns$qSup <- seq(0,1,length.out = optns$nqSup)
      }
    }
  }
  qSup <- optns$qSup
  
  optnsRegIdx <- match(c("bwReg","kernelReg","lower","upper","qSup","nqSup","bwRange"), names(optns))
  optnsRegIdx <- optnsRegIdx[!is.na(optnsRegIdx)]
  optnsReg <- optns[optnsRegIdx]
  if (is.null(optnsReg$kernelReg))
    optnsReg$kernelReg <- "gauss"
  names(optnsReg)[which(names(optnsReg) == "kernelReg")] <- "ker"
  if (!is.null(optnsReg$bwReg))
    names(optnsReg)[which(names(optnsReg) == "bwReg")] <- "bw"

  optnsDen <- optns[-optnsRegIdx]
  if (!is.null(optnsDen$kernelDen))
    names(optnsDen)[which(names(optnsDen) == "kernelDen")] <- "kernel"
  if (!is.null(optnsDen$bwDen))
    names(optnsDen)[which(names(optnsDen) == "bwDen")] <- "userBwMu"
  # moved to just the last step transforming output quantile to densities
  # don't want a common support before F reg
  #if (!is.null(optnsDen$ndSup))
  #  names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
  #if (!is.null(optnsDen$dSup))
  #  names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"

  if (!(is.null(yin) & is.null(hin))) {
    #require(fdadensity)

    if (!is.null(yin)) {
      if (!is.null(hin) | !is.null(qin))
        warning ("hin and qin are redundant when yin is available.")
      if (is.matrix(yin))
        yin <- as.data.frame(t(yin))
      if (!is.list(yin))
        stop ("yin must be a matrix or list.")
      den <- lapply(yin, CreateDensity, optns = optnsDen)
    } else if (!is.null(hin)) {
      if (!is.null(qin))
        warning ("qin is redundant when hin is available.")
      if (!is.list(hin) | length(hin) != length(xin))
        stop ("hin must be a list of the same length as xin.")
      for (histogram in hin) {
        if (!is.list(histogram))
          stop ("Each element of hin must be a list.")
        if (is.null(histogram$breaks) & is.null(histogram$mids))
          stop ("Each element of hin must be a list with at least one of the components breaks or mids.")
        if (is.null(histogram$counts))
          stop ("Each element of hin must be a list with component counts.")
      }
      den <- lapply(hin, function(histogram) {
        CreateDensity(histogram = histogram, optns = optnsDen)
      })
    }
    qin <- sapply(den, function(deni) {
      fdadensity::dens2quantile(dens = deni$y, dSup = deni$x, qSup = qSup)
    })
    qin <- t(qin)
  } else {
    #if (!is.matrix(qin))
    #  stop ("qin must be a matrix, of which each row holding the values of a quantile function evaluated on a common grid from 0 to 1.")
    if (!is.matrix(qin)) {
      if (!is.list(qin))
        stop ("qin must be a matrix or list.")
      for (qt in qin) {
        if (!is.list(qt)) {
          stop ("If qin is a list, each element must also be a list with two components, x and y.")
        } else if (is.null(qt$x) | is.null(qt$y)) {
          stop ("If qin is a list, each element must also be a list with two components, x and y.")
        }
      }
      qin <- sapply(qin, function(q) {
        approx(x = q$x, y = q$y, xout = qSup, rule = 2)$y
      })
      qin <- t(qin)
    }
    den <- apply(qin, 1, function(q) qf2pdf(qf = sort(q),prob = qSup))
  }

  if (is.null(optns$denLowerThreshold)) {
    optns$denLowerThreshold <- 0.001 * mean(qin[,ncol(qin)] - qin[,1])
  } else if (optns$denLowerThreshold) {
    if(!is.numeric(optns$denLowerThreshold) | optns$denLowerThreshold < 0)
      optns$denLowerThreshold <- 0.001 * mean(qin[,ncol(qin)] - qin[,1])
  }

  if (optns$denLowerThreshold) {
    # density thresholding from below
    if (sum(sapply(den, function(d) sum(d$y < optns$denLowerThreshold/diff(range(d$x))))) > 0) {
      den <- lapply(den, function(d) {
        lower <- optns$denLowerThreshold/diff(range(d$x))
        if (sum(d$y < lower) > 0) {
          d$y[d$y < lower] <- lower
          d$y <- d$y / pracma::trapz(d$x,d$y)
        }
        list(x=d$x, y=d$y)
      })
      qin <- sapply(den, function(deni) {
        fdadensity::dens2quantile(dens = deni$y, dSup = deni$x, qSup = qSup)
      })
      qin <- t(qin)
    }
  }

  if (!"bw" %in% names(optnsReg)) {
    optnsReg$bw <- "CV"
  }
  if (!is.numeric(optnsReg$bw)) {
    if (optnsReg$bw == "CV") {
      optnsReg$bw <- bwCV(xin=xin, qin=qin, xout=xout, optns=optnsReg)
    } else {
      warning("optns$bwReg was mis-specified and is reset to be chosen by CV.")
      optnsReg$bw <- bwCV(xin=xin, qin=qin, xout=xout, optns=optnsReg)
    }
    #if (optnsReg$bw < diff(range(xin))*0.1)
    #  optnsReg$bw <- diff(range(xin))*0.1
  } else {
    if(ncol(xin)==1){
      if (optnsReg$bw[1] < max(diff(sort(xin[,1]))) & !is.null(optnsReg$ker)) {
        if(optnsReg$ker %in% c("rect","quar","epan")) {
          warning("optns$bwReg was set too small and is reset to be chosen by CV.")
          optnsReg$bw <- bwCV(xin=xin, qin=qin, xout=xout, optns=optnsReg)
        }
      }
    }else{
      if (optnsReg$bw[1] < max(diff(sort(xin[,1]))) & optnsReg$bw[2] < max(diff(sort(xin[,2]))) & !is.null(optnsReg$ker)) {
        if(optnsReg$ker %in% c("rect","quar","epan")) {
          warning("optns$bwReg was set too small and is reset to be chosen by CV.")
          optnsReg$bw <- bwCV(xin=xin, qin=qin, xout=xout, optns=optnsReg)
        }
      }
    }
  }
  optns$bwReg <- optnsReg$bw
  qout <- LocWassReg(xin = xin, qin = qin, xout = xout, optns = optnsReg)
  
  if (!is.null(optnsDen$ndSup))
    names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
  if (!is.null(optnsDen$dSup))
    names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"
  
  if (xoutVec) xout <- c(xout)
  if (xinVec) xin <- c(xin)
  if (is.null(optnsDen$outputGrid)) {
    dout <- apply(qout, 1, qf2pdf, prob = qSup, optns = optnsDen)
    dout <- lapply(dout, function(d) d[c("x","y")])
    res <- list(xout = xout, dout = dout, qout = qout, qSup = qSup, xin=xin, din=den, qin=qin, optns=optns)
  } else {
    dSup <- optnsDen$outputGrid
    dout <- apply(qout, 1, function(q) qf2pdf(q, prob = qSup, optns = optnsDen)$y)
    #dout <- apply(qout, 1, qnt2dens, qSup = qSup, dSup = optnsDen$outputGrid)
    dout <- t(dout)
    res <- list(xout = xout, dout = dout, dSup = dSup, qout = qout, qSup = qSup, xin=xin, din=den, qin=qin, optns=optns)
  }
  class(res) <- "denReg"
  return(res)
}

# set up bandwidth range
SetBwRange <- function(xin, xout, kernel_type) {
  xinSt <- unique(sort(xin))
  bw.min <- max(diff(xinSt), xinSt[2] - min(xout), max(xout) - xinSt[length(xin)-1])*1.1 / (ifelse(kernel_type == "gauss", 3, 1) * ifelse(kernel_type == "gausvar", 2.5, 1))
  bw.max <- diff(range(xin))/3 #/ (ifelse(kernel_type == "gauss", 3, 1) * ifelse(kernel_type == "gausvar", 2.5, 1))
  if (bw.max < bw.min) {
    if (bw.min > bw.max*3/2) {
      #warning("Data is too sparse.")
      bw.max <- bw.min*1.01
    } else bw.max <- bw.max*3/2
  }
  return(list(min=bw.min, max = bw.max))
}

# bandwidth selection via cross validation
bwCV <- function(xin, qin, xout, optns) {
  p=ncol(xin)
  if(p==1){
    compareRange <- (xin[,1] > min(xin[,1]) + diff(range(xin[,1]))/5) & (xin[,1] < max(xin[,1]) - diff(range(xin[,1]))/5)
  }else{
    compareRange <- (xin[,1] > min(xin[,1]) + diff(range(xin[,1]))/5) & (xin[,1] < max(xin[,1]) - diff(range(xin[,1]))/5) & (xin[,2] > min(xin[,2]) + diff(range(xin[,2]))/5) & (xin[,2] < max(xin[,2]) - diff(range(xin[,2]))/5)
  }
  
  # k-fold
  objFctn <- function(bw) {
    optns1 <- optns
    optns1$bw <- bw
    folds <- numeric(nrow(xin))
    nn <- sum(compareRange)
    numFolds <- ifelse(nn > 30, 10, sum(compareRange))

    tmp <- c(sapply(1:ceiling(nn/numFolds), function(i)
      sample(x = seq_len(numFolds), size = numFolds, replace = FALSE)))
    tmp <- tmp[1:nn]
    repIdx <- which(diff(tmp) == 0)
    for (i in which(diff(tmp) == 0)) {
      s <- tmp[i]
      tmp[i] <- tmp[i-1]
      tmp[i-1] <- s
    }
    #tmp <- cut(1:n,breaks = seq(0,n,length.out = numFolds+1), labels=FALSE)
    #tmp <- tmp[sample(seq_len(n), n)]

    folds[compareRange] <- tmp

    qfit <- lapply(seq_len(numFolds), function(foldidx) {
      testidx <- which(folds == foldidx)
      res <- LocWassReg(xin = matrix(xin[-testidx,],ncol=p,byrow=TRUE), qin = matrix(qin[-testidx,],ncol=ncol(qin),byrow=TRUE), xout = matrix(xin[testidx,],ncol=p,byrow=TRUE),
                        optns = optns1)
      res # each row is a qt function
    })
    qfit <- do.call(rbind, qfit)
    mean(apply((qfit - qin[which(compareRange)[order(tmp)],])^2, 1, pracma::trapz, x = optns1$qSup))
  }
  
  if(p==1){
    aux=SetBwRange(xin = xin[,1], xout = xout[,1], kernel_type = optns$ker)
    bwRange <- matrix(c(aux$min,aux$max),nrow=2,ncol=1)
  }else{
    aux=SetBwRange(xin = xin[,1], xout = xout[,1], kernel_type = optns$ker)
    aux2=SetBwRange(xin = xin[,2], xout = xout[,2], kernel_type = optns$ker)
    bwRange <- as.matrix(cbind(c(aux$min,aux$max),c(aux2$min,aux2$max)))
  }
  if(!is.null(optns$bwRange)){
    if(p==1){
      if (min(optns$bwRange) < min(bwRange)) {
        message("Minimum bandwidth is too small and has been reset.")
      }else{
        bwRange[1,1] <- min(optns$bwRange)
      }
      if (max(optns$bwRange) >  min(bwRange)) {
        bwRange[2,1] <- max(optns$bwRange)
      }else {
        message("Maximum bandwidth is too small and has been reset.")
      }
    }else{
      #Check for first dimension of the predictor
      if (min(optns$bwRange[,1]) < min(bwRange[,1])) {
        message("Minimum bandwidth of first predictor dimension is too small and has been reset.")
      }else{
        bwRange[1,1] <- min(optns$bwRange[,1])
      }
      if (max(optns$bwRange[,1]) >  min(bwRange[,1])) {
        bwRange[2,1] <- max(optns$bwRange[,1])
      } else {
        message("Maximum bandwidth of first predictor dimension is too small and has been reset.")
      }
      #Check for second dimension of the predictor
      if (min(optns$bwRange[,2]) < min(bwRange[,2])) {
        message("Minimum bandwidth of second predictor dimension is too small and has been reset.")
      }else{
        bwRange[1,2] <- min(optns$bwRange[,2])
      }
      if (max(optns$bwRange[,2]) >  min(bwRange[,2])) {
        bwRange[2,2] <- max(optns$bwRange[,2])
      }else{
        message("Maximum bandwidth of second predictor dimension is too small and has been reset.")
      }
    }
  }
  if(p==1){
    res <- optimize(f = objFctn, interval = bwRange[,1])$minimum
  }else{
    res <- optim(par=colMeans(bwRange),fn=objFctn,lower=bwRange[1,],upper=bwRange[2,],method='L-BFGS-B')$par
  }
  res
}





# if(0){
#   bwCV <- function(xin, qin, optns) {
#     compareRange <- (xin > min(xin) + diff(range(xin))/5) & (xin < max(xin) - diff(range(xin))/5)
#     # leave-one-out cv
#     objFctn <- function(bw) {
#       optns1 <- optns
#       optns1$bw <- bw
#       qfit <- sapply(seq_along(xin)[compareRange], function(i) {
#         res <- LocWassReg(xin = xin[-i], qin = qin[-i,], xout = xin[i], optns = optns1)
#         as.vector(res)
#       })
#       mean(apply((qfit - t(qin[compareRange,]))^2, 2, pracma::trapz, x = optns1$qSup))
#     }
#     bwRange <- SetBwRange(xin = xin, ker = optns$ker)
#     res <- optimize(f = objFctn, interval = c(bwRange$min, bwRange$max))
#     res$minimum
#   }
# }

# # bandwidth selection via GCV implemented for the linear combination of quantile functions before optimization
# # GCV = n^{-1}\sum_i \|g(x_i) - q(x_i)\|^2_{L^2} / (1 - n^{-1}\sum_i s_i(x_i))^2
# # S = [s_j(x_i)]
# # g(x) = \sum_i s_i(x) q(x_i)
# bwGCV <- function(xin, qin, xout, optns) {
#   compareRange <- (xin > min(xin) + diff(range(xin))/5) & (xin < max(xin) - diff(range(xin))/5)
#   objFctn <- function(bw) {
#     optns1 <- optns
#     optns1$bw <- bw
#     S <- LocWt(xin = xin[compareRange], qin = qin[compareRange,], xout = xin[compareRange], optns = optns1)
#     g <- S %*% qin[compareRange,]
#     mean(apply((g - qin[compareRange,])^2, 1, pracma::trapz, x = optns1$qSup)) /
#       (1 - mean(diag(S)))^2
#   }
#   bwRange <- SetBwRange(xin = xin, xout = xout, kernel_type = optns$ker)
#   if (!is.null(optns$bwRange)) {
#     if (min(optns$bwRange) < bwRange$min) {
#       message("Minimum bandwidth is too small and has been reset.")
#     } else bwRange$min <- min(optns$bwRange)
#     if (max(optns$bwRange) >  bwRange$min) {
#       bwRange$max <- max(optns$bwRange)
#     } else {
#       message("Maximum bandwidth is too small and has been reset.")
#     }
#   }
#   res <- optimize(f = objFctn, interval = c(bwRange$min, bwRange$max))
#   res$minimum
# }

# # Weight used in local Wasserstein regression
# LocWt = function(xin, qin, xout, optns = list()){
# 
#   if (is.null(optns$bw))
#     stop ("optns$bw has no default values and must be input by users.")
#   bw <- optns$bw
#   if(!is.numeric(optns$bw) | (length(optns$bw)>1))
#     stop("optns$bw should be a numerical vector of length 1.")
#   if(!is.vector(xin) | !is.numeric(xin))
#     stop("xin should be a numerical vector.")
#   if(!is.matrix(qin) | !is.numeric(qin))
#     stop("qin should be a numerical matrix.")
#   if(length(xin)!=nrow(qin))
#     stop("The length of xin should be the same as the number of rows of qin.")
# 
#   if (is.null(optns$ker)) {
#     optns$ker <- 'gauss'
#   }
#   ker <- kerFctn(optns$ker)
# 
#   n = length(xout)
#   #m = ncol(qin)
# 
#   S = sapply(1:n, function(j){
#     mu0 = mean(ker((xout[j] - xin) / bw))
#     mu1 = mean(ker((xout[j] - xin) / bw) * (xin - xout[j]))
#     mu2 = mean(ker((xout[j] - xin) / bw) * (xin - xout[j])^2)
#     s = ker((xout[j] - xin) / bw) * (mu2 - mu1 * (xin - xout[j])) /
#       (mu0 * mu2 - mu1^2)
#     #gx = colMeans(qin * s)
#     return(s)
#   })
#   S = t(S)/n
#   return(S)
# }
#' @title Object Covariance
#' @description Calculating covariance for time varying object data
#' @param tgrid Time grid for the time varying object data and covariance function
#' @param I A four dimension array of \code{n} x \code{n} matrix of squared distances between the time point u of the ith process and process and the time point v of the jth object process, 
#' e.g.: \eqn{I[i,j,u,v] = d^2(X_i(u) X_j(v))}
#' @param K Numbers of principal components 
#' @param smooth Logical indicating if the smoothing is enabled when calculating the eigenvalues and eigenfunctions
#' @return A list of the following:
#' \item{C}{Estimated object covariance (non-smooth) on the 2D grid of dimension \code{length(tgrid)} X \code{length(tgrid)}}
#' \item{sC}{Estimated object covariance (smooth) on the 2D grid of dimension \code{length(tgrid)} X \code{length(tgrid)}}
#' \item{tgrid}{Time grid for the time varying object data and covariance function}
#' \item{K}{Numbers of principal components}
#' \item{phi}{Matrix of smooth eigenfunctions (dimension: \code{length(tgrid)} X \code{K})}
#' \item{lambda}{Vector of eigenvalues of dimension \code{K} }
#' @examples 
#' \donttest{
#' ### functional covariate
#' phi1 <- function(x) -cos(pi*x/10)/sqrt(5)
#' phi2 <- function(x)  sin(pi*x/10)/sqrt(5)
#' 
#' lambdaX <- c(4,2)
#' # training set
#' n <- 100
#' N <- 50
#' tgrid <- seq(0,10,length.out = N)
#' 
#' Xi <- matrix(rnorm(2*n),nrow=n,ncol=2)
#' CovX <- lambdaX[1] * phi1(tgrid) %*% t(phi1(tgrid)) + lambdaX[2] * phi2(tgrid) %*% t(phi2(tgrid))
#' comp1 = lambdaX[1]^(1/2) * Xi[,1] %*% t(phi1(tgrid))
#' comp2 = lambdaX[2]^(1/2) * Xi[,2] %*% t(phi2(tgrid))
#' SampleX <- comp1 + comp2
#' 
#' I <- array(0, c(n,n,N,N))
#' for (u in 1:N){
#'   for (v in 1:N){
#'     temp1 <- SampleX[,u]
#'    temp2 <- SampleX[,v]
#'     I[,,u,v] <- outer(temp1, temp2, function(v1,v2){
#'      (v1 - v2)^2
#'    })
#'  }
#' }
#' 
#' result_cov <- ObjCov(tgrid, I, 2)
#' result_cov$lambda #4 2
#' 
#' sC <- result_cov$sC
#' sum((sC-CovX)^2) / sum(sC^2)
#' sum((phi1(tgrid)-result_cov$phi[,1])^2)/sum(phi1(tgrid)^2)
#' }
#' @references 
#' \cite{Dubey, P., & Müller, H. G. (2020). Functional models for time‐varying random objects. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2), 275-327.}
#' @export
#' @import pracma
#' @import fdapace
#' @importFrom utils getFromNamespace


ObjCov <- function(tgrid, I, K, smooth=TRUE){
  if(length(dim(I))!=4){
    stop("I must be a four dimensional array")
  }
  n = dim(I)[1]
  N = length(tgrid)
  if(dim(I)[2]!=n){
    stop("length of the first and second arguments of array I are inconsistent")
  }
  if(dim(I)[3]!=dim(I)[4]){
    stop("length of the third and fourth arguments of array I are inconsistent")
  }
  if(dim(I)[3]!=N){
    stop("length of tgrid and the length of third arguments of array I are inconsistent")
  }
  t <- length(tgrid)
  C <- matrix(0, nrow = t, ncol = t)
  # calculate empirical metric autocovariance matrix C 
  for(u in 1:(t-1)){
    for(v in (u+1):t){
      C[u,v] <-  ((sum(I[,,u,v]) - sum(diag(I[,,u,v]))) - (n-1)*sum(diag(I[,,u,v]))) / (2*n*(n-1))
      C[v,u] <- C[u,v]
    } 
    C[u,u] <- sum(sum(I[,,u,u])) / (2*n*(n-1))
  }
  C[t,t] <- sum(sum(I[,,t,t])) / (2*n*(n-1))
  #obtain smooth covariance
  GetEigenAnalysisResults <- utils::getFromNamespace("GetEigenAnalysisResults", "fdapace")
  if(smooth == TRUE){
    sC <- getSmoothCov(C, tgrid, "GMeanAndGCV", "gauss",n)
    #derivat the eigenfunctions and eigenvalues
    eigResult <- GetEigenAnalysisResults(sC, tgrid, optns = list(maxK = K, verbose = FALSE, FVEthreshold = 0.9999))
  }else{
    sC <- getSmoothCov(C, tgrid, "GMeanAndGCV", "gauss",n)
    #derivat the eigenfunctions and eigenvalues
    eigResult <- GetEigenAnalysisResults(C, tgrid, optns = list(maxK = K, verbose = FALSE, FVEthreshold = 0.9999))
  }
  return(list(C=C, sC = sC, tgrid = tgrid, K = K, phi = eigResult$phi, lambda = eigResult$lambda))
}

#' @title Fréchet ANOVA for Densities
#' @description Fréchet analysis of variance for densities with respect to 
#'   \eqn{L^2}-Wasserstein distance.
#' @param yin A matrix or data frame or list holding the sample of measurements 
#'   for the observed distributions. If \code{yin} is a matrix or data frame, 
#'   each row holds the measurements for one distribution.
#' @param hin A list holding the histograms for the observed distributions.
#' @param din A matrix or data frame or list holding the density functions. 
#'   If \code{din} is a matrix or data frame, each row of \code{din} holds 
#'   the density function for one distribution.
#' @param qin A matrix or data frame or list holding the quantile functions. 
#'   If \code{qin} is a matrix or data frame, each row of \code{qin} holds 
#'   the quantile function for one distribution.
#' Note that the input can be only one of the four \code{yin}, \code{hin}, 
#' \code{din}, and \code{qin}. If more than one of them are specified, 
#' \code{yin} overwrites \code{hin}, \code{hin} overwrites \code{din}, 
#' and \code{din} overwrites \code{qin}.
#' @param supin A matrix or data frame or list holding the support grids of 
#'   the density functions in \code{din} or the quantile functions in \code{qin}. 
#'   If \code{supin} is a matrix or data frame, each row of \code{supin} holds 
#'   the support grid of the corresponding density function or quantile function.
#'   Ignored if the input is \code{yin} or \code{hin}.
#'   It can also be a vector if all density functions in \code{din} or 
#'   all quantile functions in \code{qin} have the same support grid.
#' @param group A vector containing the group memberships of the corresponding 
#'   observed distributions in \code{yin} or \code{hin} or \code{din} or \code{qin}.
#' @param optns A list of control parameters specified by
#'   \code{list(name = value)}. See `Details`.
#' @details Available control options are 
#' \describe{
#' \item{boot}{Logical, also compute bootstrap \eqn{p}-value if \code{TRUE}. 
#'   Default is \code{FALSE}.}
#' \item{R}{The number of bootstrap replicates. Only used when \code{boot} 
#'   is \code{TRUE}. Default is 1000.}
#' \item{nqSup}{A scalar giving the number of the support points for 
#'   quantile functions based on which the \eqn{L^2} Wasserstein distance 
#'   (i.e., the \eqn{L^2} distance between the quantile functions) is computed. 
#'   Default is 201.}
#' \item{qSup}{A numeric vector holding the support grid on [0, 1] based on 
#'   which the \eqn{L^2} Wasserstein distance (i.e., the \eqn{L^2} distance 
#'   between the quantile functions) is computed. It overrides \code{nqSup}.}
#' \item{bwDen}{The bandwidth value used in \code{CreateDensity()} for
#'   density estimation; positive numeric - default: determine automatically 
#'   based on the data-driven bandwidth selector proposed by 
#'   Sheather and Jones (1991).}
#' \item{ndSup}{A scalar giving the number of support points the kernel density 
#'   estimation used in \code{CreateDensity()}; numeric - default: 101.}
#' \item{dSup}{User defined output grid for the support of 
#'   kernel density estimation used in \code{CreateDensity()}, 
#'   it overrides \code{ndSup}.}
#' \item{delta}{A scalar giving the size of the bin to be used used in 
#'   \code{CreateDensity()}; numeric - default: \code{diff(range(y))/1000}. 
#'   It only works when the raw sample is available.}
#' \item{kernelDen}{A character holding the type of kernel functions used in 
#'   \code{CreateDensity()} for density estimation; \code{"rect"}, 
#'   \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, 
#'   \code{"quar"} - default: \code{"gauss"}.}
#' \item{infSupport}{logical if we expect the distribution to have 
#'   infinite support or not, used in \code{CreateDensity()} for 
#'   density estimation; logical - default: \code{FALSE}}
#' \item{denLowerThreshold}{\code{FALSE} or a positive value giving 
#'   the lower threshold of the densities used in \code{CreateDensity()}; 
#'   default: \code{0.001 * mean(qin[,ncol(qin)] - qin[,1])}.}
#' }
#' @return A \code{DenANOVA} object --- a list containing the following fields:
#' \item{pvalAsy}{a scalar holding the asymptotic \eqn{p}-value.}
#' \item{pvalBoot}{a scalar holding the bootstrap \eqn{p}-value.
#'   Returned if \code{optns$boot} is TRUE.}
#' \item{optns}{the control options used.}
#' @examples
#' \donttest{
#' set.seed(1)
#' n1 <- 100
#' n2 <- 100
#' delta <- 1
#' qSup <- seq(0.01, 0.99, (0.99 - 0.01) / 50)
#' mu1 <- rnorm(n1, mean = 0, sd = 0.5)
#' mu2 <- rnorm(n2, mean = delta, sd = 0.5)
#' Y1 <- lapply(1:n1, function(i) {
#'   qnorm(qSup, mu1[i], sd = 1)
#' })
#' Y2 <- lapply(1:n2, function(i) {
#'   qnorm(qSup, mu2[i], sd = 1)
#' })
#' Ly <- c(Y1, Y2)
#' Lx <- qSup
#' group <- c(rep(1, n1), rep(2, n2))
#' res <- DenANOVA(qin = Ly, supin = Lx, group = group, optns = list(boot = TRUE))
#' res$pvalAsy # returns asymptotic pvalue
#' res$pvalBoot # returns bootstrap pvalue
#' }
#' @references
#' \itemize{
#' \item \cite{Dubey, P. and Müller, H.G., 2019. Fréchet analysis of variance for random objects. Biometrika, 106(4), pp.803-821.}
#' }
#' @importFrom fdadensity dens2quantile
#' @importFrom boot boot
#' @importFrom stats pchisq
#' @export

DenANOVA <- function(yin = NULL, hin = NULL, din = NULL, qin = NULL, 
                     supin = NULL, group = NULL, optns = list()) {
  if (is.null(yin) & is.null(qin) & is.null(hin) & is.null(din)) {
    stop ("one of the four arguments, yin, hin, din, and qin, should be inputted by users")
  }
  if (is.null(group)) {
    stop("requires the input of group")
  }
  if (!is.null(yin)) {
    if (!(is.null(hin) & is.null(din) & is.null(qin))) {
      warning ("hin, din, and qin are redundant when yin is available")
    }
    tin <- 1 # type of input
    ls <- yin
  } else if (!is.null(hin)) {
    if (!(is.null(din) & is.null(qin))) {
      warning ("din and qin are redundant when hin is available")
    }
    tin <- 2 # type of input
    ls <- hin
  } else if (!is.null(din)) {
    if (!is.null(qin)) {
      warning ("qin is redundant when din is available")
    }
    tin <- 3 # type of input
    ls <- din
    if (!is.list(din)) {
      if (is.matrix(din) | is.data.frame(din)) {
        din <- lapply(1:nrow(din), function(i) din[i, ])
      } else {
        stop("din must be a matrix or data frame or list")
      }
    }
  } else {
    tin <- 4 # type of input
    ls <- qin
  }
  if (tin == 2) {
    if (!is.list(ls)) {
      stop("hin must be a list")
    }
    for (histogram in ls) {
      if (!is.list(histogram))
        stop("each element of hin must be a list")
      if (is.null(histogram$breaks) & is.null(histogram$mids))
        stop("each element of hin must be a list with at least one of the components breaks or mids")
      if (is.null(histogram$counts))
        stop("each element of hin must be a list with component counts")
    }
  } else {
    if (!is.list(ls)) {
      if (is.matrix(ls) | is.data.frame(ls)) {
        ls <- lapply(1:nrow(ls), function(i) ls[i, ])
      } else {
        if (tin == 1) {
          stop("yin must be a matrix or data frame or list")
        } else if (tin == 3) {
          stop("din must be a matrix or data frame or list")
        } else {
          stop("qin must be a matrix or data frame or list")
        }
      }
    }
  }
  n <- length(ls)
  if (length(group) != n) {
    stop("the number of elements in group must equal the number of observed distributions")
  }
  if (is.null(optns$boot)) {
    optns$boot <- FALSE
  }
  if (optns$boot) {
    if (is.null(optns$R)) {
      optns$R <- 1000
    }
  }
  if (tin > 2) {# if din or qin
    if (!is.null(supin)) {
      if (!is.list(supin)) {
        if (is.matrix(supin) | is.data.frame(supin)) {
          supin <- lapply(1:nrow(supin), function(i) supin[i, ])
        } else if (is.vector(supin)) {
          supin <- rep(list(supin), n)
        } else {
          stop("supin must be a vector or matrix or data frame or list")
        }
      }
      if (length(supin) != n) {
        if (tin == 3) {
          stop("the number of support grids in supin is not equal to the number of observed distributions in din")
        } else {
          stop("the number of support grids in supin is not equal to the number of observed distributions in qin")
        }
      }
      if (sum(sapply(supin, length) - sapply(ls, length))) {
        if (tin == 3) {
          stop("the number of support points must be equal to the number of observations for each density function in din")
        } else {
          stop("the number of support points must be equal to the number of observations for each quantile function in qin")
        }
      }
    } else {
      if (tin == 3) {
        stop("requires the input of supin for din")
      } else {
        stop("requires the input of supin for qin")
      }
    }
  }
  if (!is.null(optns$qSup)) {
    if (min(optns$qSup) != 0 | max(optns$qSup) - 1 != 0)
      stop ("optns$qSup must have minimum 0 and maximum 1")
    if (sum(duplicated(optns$qSup)) > 0) {
      optns$qSup <- unique(optns$qSup)
      warning ("optns$qSup has duplicated elements which has been removed")
    }
    if (is.unsorted(optns$qSup)) {
      optns$qSup <- sort(optns$qSup)
      warning ("optns$qSup has been reordered to be increasing")
    }
  } else {
    if (tin < 4) {# if not qin
      if(is.null(optns$nqSup)) {
        optns$nqSup <- 201
      }
      optns$qSup <- seq(0,1,length.out = optns$nqSup)
    } else {# if qin
      diffSupp <- TRUE
      if (length(unique(sapply(supin, length))) == 1) {
        if (!sum(diff(matrix(unlist(supin), nrow = n, byrow = TRUE)))) {
          diffSupp <- FALSE
        }
      }
      if (diffSupp) {
        if(is.null(optns$nqSup)) {
          optns$nqSup <- 201
        }
        optns$qSup <- seq(0,1,length.out = optns$nqSup)
        ls <- lapply(1:n, function(i) approx(x = supin[[i]], y = ls[[i]], xout = optns$qSup)$y)
      } else {
        optns$qSup <- supin[[1]]
        optns$nqSup <- length(optns$qSup)
      }
    }
  }
  qSup <- optns$qSup
  if (tin == 3) {
    ls <- lapply(1:n, function(i) fdadensity::dens2quantile(ls[[i]], dSup = supin[[i]], qSup = qSup))
  }
  if (tin < 3) {
    optnsDen <- optns
    if (!is.null(optnsDen$kernelDen))
      names(optnsDen)[which(names(optnsDen) == "kernelDen")] <- "kernel"
    if (!is.null(optnsDen$bwDen))
      names(optnsDen)[which(names(optnsDen) == "bwDen")] <- "userBwMu"
    if (!is.null(optnsDen$ndSup))
      names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
    if (!is.null(optnsDen$dSup))
      names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"
    if (tin == 1) {
      den <- lapply(ls, CreateDensity, optns = optnsDen)
    } else {
      den <- lapply(ls, function(histogram) {
        CreateDensity(histogram = histogram, optns = optnsDen)
      })
    }
    ls <- lapply(den, function(deni) fdadensity::dens2quantile(deni$y, dSup = deni$x, qSup = qSup))
  }
  sizes <- as.vector(table(group))
  k <- length(sizes) # number of groups
  data <- ls[order(group)]
  # data <- unlist(split(ls, group), recursive = FALSE)
  if (optns$boot) {
    bootRes <- boot::boot(data = data, statistic = DenANOVAStatistic, 
                          R = optns$R, sizes = sizes, qSup = qSup)
    pvalBoot <- length(which(bootRes$t > bootRes$t0)) / optns$R
    pvalAsy <- 1 - pchisq(bootRes$t0, df = k - 1)
    res <- list(pvalAsy = pvalAsy, pvalBoot = pvalBoot, optns = optns)
  } else {
    t0 <- DenANOVAStatistic(data, 1:length(group), sizes, qSup)
    pvalAsy <- 1 - pchisq(t0, df = k - 1)
    res <- list(pvalAsy = pvalAsy, optns = optns)
  }
  class(res) <- "DenANOVA"
  res
}
#' @title Fréchet ANOVA for Networks
#' @description Fréchet analysis of variance for graph Laplacian matrices, 
#'   covariance matrices, or correlation matrices 
#'   with respect to the Frobenius distance.
#' @param Ly A list (length n) of m by m matrices or a m by m by n array where
#'   \code{Ly[, , i]} contains an m by m matrix, which can be either graph 
#'   Laplacian matrices or covariance matrices or correlation matrices.
#' @param group A vector containing the group memberships of the corresponding 
#'   matrices in \code{Ly}.
#' @param optns A list of control parameters specified by 
#'   \code{list(name = value)}. See `Details`.
#' @details Available control options are:
#' \describe{
#' \item{boot}{Logical, also compute bootstrap \eqn{p}-value if \code{TRUE}. 
#'   Default is \code{FALSE}.}
#' \item{R}{The number of bootstrap replicates. Only used when \code{boot} 
#'   is \code{TRUE}. Default is 1000.}
#' }
#' @return A \code{NetANOVA} object --- a list containing the following fields:
#' \item{pvalAsy}{A scalar holding the asymptotic \eqn{p}-value.}
#' \item{pvalBoot}{A scalar holding the bootstrap \eqn{p}-value. 
#'   Returned if \code{optns$boot} is TRUE.}
#' \item{optns}{The control options used.}
#' @examples
#' \donttest{
#' set.seed(1)
#' n1 <- 100
#' n2 <- 100
#' gamma1 <- 2
#' gamma2 <- 3
#' Y1 <- lapply(1:n1, function(i) {
#'   igraph::laplacian_matrix(igraph::sample_pa(n = 10, power = gamma1, 
#'                                              directed = FALSE), 
#'                            sparse = FALSE)
#' })
#' Y2 <- lapply(1:n2, function(i) {
#'   igraph::laplacian_matrix(igraph::sample_pa(n = 10, power = gamma2, 
#'                                              directed = FALSE), 
#'                            sparse = FALSE)
#' })
#' Ly <- c(Y1, Y2)
#' group <- c(rep(1, n1), rep(2, n2))
#' res <- NetANOVA(Ly, group, optns = list(boot = TRUE))
#' res$pvalAsy # returns asymptotic pvalue
#' res$pvalBoot # returns bootstrap pvalue
#' }
#' @references
#' \itemize{
#' \item \cite{Dubey, P. and Müller, H.G., 2019. Fréchet analysis of variance for random objects. Biometrika, 106(4), pp.803-821.}
#' }
#' @importFrom boot boot
#' @importFrom stats pchisq
#' @export

NetANOVA <- function(Ly = NULL, group = NULL, optns = list()) {
  if (is.null(Ly) | is.null(group)) {
    stop("requires the input of both Ly and group")
  }
  if (!is.list(Ly)) {
    if (is.array(Ly)) {
      Ly <- lapply(seq(dim(Ly)[3]), function(i) Ly[, , i])
    } else {
      stop("Ly must be a list or an array")
    }
  }
  if (length(Ly) != length(group)) {
    stop("Ly and group should have the same length")
  }
  if (length(unique(sapply(Ly, length))) > 1) {
    stop("each matrix in Ly should be of the same dimension")
  }
  if (any(sapply(Ly, function(Lyi) nrow(Lyi) != ncol(Lyi)))) {
    stop("each matrix in Ly should be a square matrix")
  }
  if (is.null(optns$boot)) {
    optns$boot <- FALSE
  }
  if (is.null(optns$R)) {
    optns$R <- 1000
  }
  n <- length(Ly)
  sizes <- as.vector(table(group))
  k <- length(sizes) # number of groups
  data <- Ly[order(group)]
  # data <- unlist(split(Ly, group), recursive = FALSE)
  if (optns$boot) {
    bootRes <- boot::boot(data = data, statistic = NetANOVAStatistic, 
                          R = optns$R, sizes = sizes)
    pvalBoot <- length(which(bootRes$t > bootRes$t0)) / optns$R
    pvalAsy <- 1 - pchisq(bootRes$t0, df = k - 1)
    res <- list(pvalAsy = pvalAsy, pvalBoot = pvalBoot, optns = optns)
  } else {
    t0 <- NetANOVAStatistic(data, 1:length(group), sizes)
    pvalAsy <- 1 - pchisq(t0, df = k - 1)
    res <- list(pvalAsy = pvalAsy, optns = optns)
  }
  class(res) <- "NetANOVA"
  res
}
#'@title Local Fréchet regression of conditional mean
#'@noRd
#'@description Local Fréchet regression of conditional mean function with functional response and vector predictors. Input must be regular.
#'@param x An n by p matrix of predictors, p should be at most 3.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.
#'@param xout An m by p matrix of output predictor levels
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{bwMean}{A vector of length p holding the bandwidths for conditional mean estimation if \code{y} is provided. If \code{bwMean} is not provided, it is chosen by cross validation.}
#' \item{kernel}{Name of the kernel function to be chosen from 'gauss', 'rect', 'epan', 'gausvar' and 'quar'. Default is 'gauss'.}
#' }
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{mean_out}{A list of estimated conditional mean vectors at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' \donttest{
#'# Example
#'n=200             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Mean_rst = mean4LocCovReg(x=x,y=y,xout=xout,list(bwMean=0.1))
#'}
#' @references
#' \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}

mean4LocCovReg = function(x, y, xout,optns = list()){
  if(is.null(optns$kernel)){
    kernel='gauss'
  } else{
    kernel=optns$kernel
  }
  if(is.null(optns$bwMean)){
    bwMean=NA
  } else{
    bwMean=optns$bwMean
  }
  if(!is.matrix(x)){
    stop('x must be a matrix')
  }
  if(!is.matrix(y)){
    stop('y must be a matrix')
  }
  if(!is.matrix(xout)){
    stop('xout must be a matrix')
  }
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have the same number of columns')
  }
  if(nrow(x) != nrow(y)){
    stop('x and y must have the same number of rows')
  }

  Kern=kerFctn(kernel)
  K = function(x,h){
    x=matrix(x,nrow=1)
    k = 1
    for(i in 1:p){
      k=k*Kern(x[,i]/h[i])
    }
    return(as.numeric(k))
  }

  n = nrow(y)
  p = ncol(x)
  m = nrow(xout)
  nGrid = ncol(y)
  cmh = matrix(0,m,nGrid)

  if(p > 3){
    warning('Local method is designed to work in low dimensional case, the result might be unstable.')
  }

  if(is.na(sum(bwMean))){
      hs = matrix(0,p,20)
      for(l in 1:p){
        hs[l,] = exp(seq(log(n^(-1/(1+p))*(max(x[,l])-min(x[,l]))/10),log(5*n^(-1/(1+p))*(max(x[,l])-min(x[,l]))),length.out =  20))
      }
      cv = array(0,20^p)
      for(k in 0:(20^p-1)){
        h = array(0,p)
        for(l in 1:p){
          kl = floor((k %% (20^l)) / (20^(l-1))) + 1
          h[l] = hs[l,kl]
        }
        yfit = matrix(0,n,nGrid)
        for(j in 1:n){
          a = x[j,]
          sL = array(0,n)
          mu0 = 0
          mu1 = 0
          mu2 = 0
          for(i in (1:n)[-j]){
            mu0 = mu0 + K(x[i,]-a,h) /n
            mu1 = mu1 + K(x[i,]-a,h)*(x[i,]-a) /n
            mu2 = mu2 + K(x[i,]-a,h)*((x[i,]-a) %*% t(x[i,]-a))/n
          }
          for(i in (1:n)[-j]){
            sL[i] =K(x[i,]-a,h)*(1-t(mu1)%*%solve(mu2)%*%(x[i,]-a))
          }
          s = sum(sL)
          for(i in (1:n)[-j]){
            yfit[j,] = yfit[j,]+sL[i]*y[i,]/s
          }
        }
        cv[k+1] = sum((y - yfit)^2)/n/nGrid
      }
      bwi = which.min(cv)
      bwMean = array(0,p)
      for(l in 1:p){
        kl = floor((bwi %% (20^l)) / (20^(l-1))) + 1
        bwMean[l] = hs[l,kl]
      }
  }

  for(j in 1:m){
    if(length(bwMean) != p){
      stop('Dimension of bandwidth does not agree with that of Euclidean predictor X')
    }
    a = xout[j,]
    sL = array(0,n)
    mu0 = 0
    mu1 = 0
    mu2 = 0
    for(i in 1:n){
      mu0 = mu0 + K(x[i,]-a,bwMean) /n
      mu1 = mu1 + K(x[i,]-a,bwMean)*(x[i,]-a) /n
      mu2 = mu2 + K(x[i,]-a,bwMean)*(x[i,]-a) %*% t(x[i,]-a)/n
    }
    for(i in 1:n){
      sL[i] =K(x[i,]-a,bwMean)*(1-t(mu1)%*%solve(mu2)%*%(x[i,]-a))
    }
    s = sum(sL)
    if(s == 0){
      stop('Bandwidth too small')
    }
    for(i in 1:n){
      cmh[j,] = cmh[j,]+sL[i]*y[i,]/s
    }
  }
  optns$kernel=kernel
  optns$bwMean=bwMean
  return(list(xout=xout,mean_out = cmh,optns=optns))
}








#' @title Local Wasserstein Regression
#' @noRd
#' @description  Local Fréchet regression for probability distributions with respect to the Wasserstein distance.
#'
#' @param xin A n by p matrix holding the n observations of the predictor.
#' @param qin An n by m matrix with values of quantile functions of which each row holds the quantile function values on an equispaced grid on [0, 1] of length m.
#' @param xout A k by p matrix holding the k output predictor values.
#' @param optns A list of control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{bw}{A vector of length p containing the bandwidths of each predictor dimension.}
#' \item{ker}{A character holding the type of kernel functions.}
#' \item{lower}{A scalar with the lower bound of the support of the distribution. Default is \code{NULL}.}
#' \item{upper}{A scalar with the upper bound of the support of the distribution. Default is \code{NULL}.}
#' }
#' @importFrom osqp solve_osqp osqpSettings

LocWassReg = function(xin, qin, xout, optns = list()){

  if(!is.matrix(xin)&!is.vector(xin)){
    stop('xin must be a matrix or vector')
  }
  if(is.vector(xin)){
    xin<- matrix(xin,length(xin))
  }
  if (is.null(xout)){
    xout <- xin
  }
  if(!is.matrix(xout)&!is.vector(xout)){
    stop('xout must be a matrix or vector')
  }
  if(is.vector(xout)){
    xout<- matrix(xout,length(xout))
  }
  if(ncol(xin) != ncol(xout)){
    stop('xin and xout must have the same number of columns')
  }
  if(is.null(optns$bw)){
    stop ("optns$bw has no default values and must be input by user.")
  }
  if(!is.numeric(optns$bw) | (length(optns$bw)!=ncol(xin))){
    stop("optns$bw should be a numerical vector of length p.")
  }
  if(!is.matrix(qin) | !is.numeric(qin)){
    stop("qin should be a numerical matrix.")
  }
  if(nrow(xin)!=nrow(qin)){
    stop("The number of rows of xin should be the same as the number of rows of qin.")
  }
  if(is.null(optns$ker)){
    optns$ker <- 'gauss'
  }
  ker <- kerFctn(optns$ker)
  
  K = function(x,h){
    k = 1
    for(i in 1:p){
      k=k*ker(x[,i]/h[i])
    }
    return(as.numeric(k))
  }
  
  k <- nrow(xout)
  n <- nrow(xin)
  m <- ncol(qin)
  p <- ncol(xin)
  
  getLFRweights=function(x0){
    #x0 is a vector in R^p that corresponds to the covariate value for which we want to predict
    aux=K(xin-matrix(t(x0),nrow=n,ncol=length(x0),byrow=TRUE),optns$bw)
    mu0 = mean(aux)
    mu1 = colMeans(aux*(xin - matrix(t(x0),nrow=n,ncol=length(x0),byrow=TRUE)))
    mu2=0
    for(i in 1:n){
      mu2 = mu2 + aux[i]*(xin[i,]-x0) %*% t(xin[i,]-x0)/n
    }
    sL = array(0,n)
    for(i in 1:n){
      sL[i] =aux[i]*(1-t(mu1)%*%solve(mu2)%*%(xin[i,]-x0))
    }
    s = sum(sL)
    return(sL/s)
  }
  
  # if lower & upper are neither NULL
  A <- cbind(diag(m), rep(0,m)) + cbind(rep(0,m), -diag(m))
  if (!is.null(optns$upper) & !is.null(optns$lower)) {
    b0 <- c(optns$lower, rep(0,m-1), -optns$upper)
  } else if(!is.null(optns$upper)) {
    A <- A[,-1]
    b0 <- c(rep(0,m-1), -optns$upper)
  } else if(!is.null(optns$lower)) {
    A <- A[,-ncol(A)]
    b0 <- c(optns$lower,rep(0,m-1))
  } else {
    A <- A[,-c(1,ncol(A))]
    b0 <- rep(0,m-1)
  }
  Pmat <- as(diag(m), "sparseMatrix")
  Amat <- as(t(A), "sparseMatrix")
  
  qout <- sapply(1:k, function(j){
    s=getLFRweights(xout[j,])
    s=as.vector(s)
    gx <- colMeans(qin * s)*n
    #res = do.call(quadprog::solve.QP, list(diag(m), gx, A, b0))
    #return(sort(res$solution))
    res <- do.call(osqp::solve_osqp,
                   list(P=Pmat, q= -gx, A=Amat, l=b0, pars = osqp::osqpSettings(verbose = FALSE)))
    return(sort(res$x))
  })
  qout = t(qout)
  return(qout)
}
# L2 norm
l2norm <- function(x){
  #sqrt(sum(x^2))
  as.numeric(sqrt(crossprod(x)))
}#' Generate a "natural" frame (orthonormal basis)
#' @description Generate a "natural" frame (orthonormal basis) for the tangent space at \code{x} on the unit sphere.
#' @param x A unit vector of length \eqn{d}.
#' @return A \eqn{d}-by-\eqn{(d-1)} matrix where columns hold the orthonormal basis of the tangent space at \code{x} on the unit sphere.
#' @details The first \eqn{(i+1)} elements of the \eqn{i}th basis vector are given by 
#' \eqn{\sin\theta_i\prod_{j=1}^{i-1}\cos\theta_j}, \eqn{\sin\theta_i\sin\theta_1 \prod_{j=2}^{i-1}\cos\theta_j},
#' \eqn{\sin\theta_i\sin\theta_2 \prod_{j=3}^{i-1}\cos\theta_j}, \eqn{\dots}, \eqn{\sin\theta_i\sin\theta_{i-1}}, \eqn{-\cos\theta_i}, respectively.
#' The rest elements (if any) of the \eqn{i}th basis vector are all zero.
#' @examples
#' frameSphere(c(1,0,0,0))
#' @export
frameSphere <- function(x) {
  theta <- car2pol(x)[-1]
  d <- length(x)
  frm <- matrix(numeric(d), ncol = 1)
  frm[1:2,1] <- c(sin(theta[1]), -cos(theta[1]))
  if (d == 2) return(frm)
  
  # product of (sin(theta[1]), cos(theta[2]), ..., cos(theta[l-1]), sin(theta[l]))
  sinfirstlast <- function(theta) {
    len <- length(theta)
    if (len < 2) stop("theta must have at least two elements.")
    res <- prod(sin(theta[c(1,len)]))
    if (len > 2) res <- res * prod(cos(theta[-c(1,len)]))
    return(res)
  }
  
  frm <- cbind(
    frm,
    sapply(2:(d-1), function(j) {
      tmp <- rep(0,d)
      tmp[1] <- sin(theta[j]) * prod(cos(theta[1:(j-1)]))
      tmp[2:j] <- sapply(1:(j-1), function(k) {
        sinfirstlast(theta[k:j])
      })
      tmp[j+1] <- -cos(theta[j])
      return(tmp)
    })
  )
  return(frm)
}
#'@title Local Fréchet regression of covariance matrices
#'@description Local Fréchet regression of covariance matrices with Euclidean predictors.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.  See 'metric' option in 'Details' for more details.
#'@param M A q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q. See 'metric' option in 'Details' for more details.
#'@param xout An m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{metric}{Metric type choice, \code{"frobenius"}, \code{"power"}, \code{"log_cholesky"}, \code{"cholesky"} - default: \code{"frobenius"} which corresponds to the power metric with \code{alpha} equal to 1.
#' For power (and Frobenius) metrics, either \code{y} or \code{M} must be input; \code{y} would override \code{M}. For Cholesky and log-Cholesky metrics, \code{M} must be input and \code{y} does not apply.}
#' \item{alpha}{The power parameter for the power metric. Default is 1 which corresponds to Frobenius metric.}
#' \item{bwMean}{A vector of length p holding the bandwidths for conditional mean estimation if \code{y} is provided. If \code{bwMean} is not provided, it is chosen by cross validation.}
#' \item{bwCov}{A vector of length p holding the bandwidths for conditional covariance estimation. If \code{bwCov} is not provided, it is chosen by cross validation.}
#' \item{kernel}{Name of the kernel function to be chosen from \code{"rect"}, \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, \code{"quar"}. Default is \code{"gauss"}.}
#' }
#' @return A \code{covReg} object --- a list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance or correlation matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' \donttest{
#' #Example y input
#'n=30             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Cov_est=LocCovReg(x=x,y=y,xout=xout,optns=list(corrOut=FALSE,metric="power",alpha=3))
#'
#'#Example M input
#'n=30 #sample size
#'m=30 #dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-15*diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'x=matrix(rnorm(n),n)
#'xout = matrix(c(0.25,0.5,0.75),3) #output predictor levels
#'Cov_est=LocCovReg(x=x,M=M,xout=xout,optns=list(corrOut=FALSE,metric="power",alpha=0))
#'}
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \item \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' \item \cite{Lin, Z. (2019). Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. Siam. J. Matrix. Anal, A. 40, 1353--1370.}
#' }
#' @export

LocCovReg= function(x,y=NULL,M=NULL,xout,optns = list()){
  if(is.null(optns$metric)){
    metric <- "frobenius"
  } else {
    metric <- optns$metric
  }
  if(!metric %in% c("frobenius","power","cholesky","log_cholesky")){
    stop("metric choice not supported.")
  }
  if(metric=="frobenius") {#Faster implementation using LFRCov instead of LFRCovPower for Frobenius case
    res <- LFRCov(x=x, y=y,M=M,xout=xout,optns = optns)
  } else if(metric=="power") {
    res <- LFRCovPower(x=x, y=y,M=M,xout=xout,optns = optns)
  } else{
    if (is.null(M))
      stop("M must be input for Cholesky and log-Cholesky metrics; y does not apply.")
    res <- LFRCovCholesky(x=x, M=M, xout=xout, optns = optns)
  }
  class(res) <- "covReg"
  return(res)
}
# using CV to choose bw for local Fréchet regression on a unit hypersphere.
bwCV_sphe <- function(xin, yin, xout, optns) {
  yin <- yin[order(xin),]
  xin <- sort(xin)
  compareRange <- (xin > min(xin) + diff(range(xin))/5) & (xin < max(xin) - diff(range(xin))/5)
  
  # k-fold
  objFctn <- function(bw) {
    optns1 <- optns
    optns1$bw <- bw
    folds <- numeric(length(xin))
    n <- sum(compareRange)
    numFolds <- ifelse(n > 30, 10, sum(compareRange))
    
    tmp <- c(sapply(1:ceiling(n/numFolds), function(i)
      sample(x = seq_len(numFolds), size = numFolds, replace = FALSE)))
    tmp <- tmp[1:n]
    repIdx <- which(diff(tmp) == 0)
    for (i in which(diff(tmp) == 0)) {
      s <- tmp[i]
      tmp[i] <- tmp[i-1]
      tmp[i-1] <- s
    }
    #tmp <- cut(1:n,breaks = seq(0,n,length.out = numFolds+1), labels=FALSE)
    #tmp <- tmp[sample(seq_len(n), n)]
    
    folds[compareRange] <- tmp
    
    yout <- lapply(seq_len(numFolds), function(foldidx) {
      testidx <- which(folds == foldidx)
      res <- LocSpheGeoReg(xin = xin[-testidx], yin = yin[-testidx,], xout = xin[testidx], optns = optns1)
      res # each row is a spherical vector
    })
    yout <- do.call(rbind, yout)
    yinMatch <- yin[which(compareRange)[order(tmp)],]
    mean(sapply(1:nrow(yout), function(i) SpheGeoDist(yout[i,], yinMatch[i,])^2))
  }
  bwRange <- SetBwRange(xin = xin, xout = xout, kernel_type = optns$ker)
  #if (!is.null(optns$bwRange)) {
  #  if (min(optns$bwRange) < bwRange$min) {
  #    message("Minimum bandwidth is too small and has been reset.")
  #  } else bwRange$min <- min(optns$bwRange)
  #  if (max(optns$bwRange) >  bwRange$min) {
  #    bwRange$max <- max(optns$bwRange)
  #  } else {
  #    message("Maximum bandwidth is too small and has been reset.")
  #  }
  #}
  res <- optimize(f = objFctn, interval = c(bwRange$min, bwRange$max))
  res$minimum
}
#' Compute a log map for a unit hypersphere.
#' @param base A unit vector of length \eqn{m} holding the base point of the tangent space.
#' @param x A unit vector of length \eqn{m} which the log map is taken.
#' @return A tangent vector of length \eqn{m}.
#' @export
logSphere <- function(base, x) {
  tg <- (x - sum(x * base) * base)
  tgNorm <- l2norm(tg)
  if ( !is.na(tgNorm) & isTRUE(all.equal(tgNorm, 0)) ) {
    rep( 0,length(base) )
  } else {
    tg / l2norm(tg) * SpheGeoDist(base,x)
  }
}#' Compute gradient w.r.t. y of the geodesic distance \eqn{\arccos(x^\top y)} on a unit hypersphere
#' @param x,y Two unit vectors.
#' @return A vector holding gradient w.r.t. \code{y} of the geodesic distance between \code{x} and \code{y}.
#' @export
SpheGeoGrad <- function(x,y) { 
  tmp <- 1 - sum(x * y) ^ 2
  return(- (tmp) ^ (-0.5) * x)
  # if (tmp < tol) {
  #   return(- Inf * x)
  # } else {
  #   return(- (tmp) ^ (-0.5) * x)
  # }
}#'@title Global Fréchet regression of conditional mean
#'@noRd
#'@description Global Fréchet regression of conditional mean function with functional response and vector predictors, input must be regular.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.
#'@param xout An m by p matrix of output predictor levels
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{mean_out}{An m by l matrix of estimated conditional means at \code{xout}.}
#' @examples
#' \donttest{
#'#Example
#'n=200             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'   theta1[i] = rnorm(1,x[i],x[i]^2)
#'   theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Mean_rst = mean4GloCovReg(x,y,xout)
#'}
#' @references
#' \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}

mean4GloCovReg  = function(x, y, xout){
  if(!is.matrix(x)){
    stop('x must be a matrix')
  }
  if(!is.matrix(y)){
    stop('y must be a matrix')
  }
  if(!is.matrix(xout)){
    stop('y must be a matrix')
  }
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have the same number of columns')
  }
  if(nrow(x) != nrow(y)){
    stop('x and y must have the same number of columns')
  }
  n = nrow(y)
  invVa = solve(var(x))
  p = ncol(invVa)
  mx = apply(x,2,mean)
  nGrid = ncol(y)
  m = nrow(xout)
  cm = matrix(0,m,nGrid)

  for(j in 1:m){
    a=xout[j,]
    sL = array(0,n)
    for(i in 1:n){
      sL[i] = 1-(x[i,]-mx)%*%invVa %*% (mx-a)
    }
    for(i in 1:n){
      cm[j,] = cm[j,]+sL[i]*y[i,]/n
    }
  }
  return(list(xout=xout,mean_out = cm))
}
#' @title Create density functions from raw data, histogram objects or frequency tables with bins
#'
#' @description Create kernel density estimate along the support of the raw data using the HADES method.
#'
#' @param y A vector of raw readings.
#' @param histogram A \code{histogram} object in R. Use this option when histogram object is only available, but not the raw data \code{y}. The default is \code{NULL}.
#' @param freq A frequency vector. Use this option when frequency table is only available, but not the raw sample or the histogram object. The corresponding \code{bin} should be provided together. The default is \code{NULL}.
#' @param bin A bin vector having its length with \code{length(freq)+1}. Use this option when frequency table is only available, but not the raw sample or the histogram object. The corresponding \code{freq} should be provided together.The default is \code{NULL}.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{userBwMu}{The bandwidth value for the smoothed mean function; positive numeric - default: determine automatically based on the data-driven bandwidth selector proposed by Sheather and Jones (1991)}
#' \item{nRegGrid}{The number of support points the KDE; numeric - default: 101.}
#' \item{delta}{The size of the bin to be used; numeric - default: \code{diff(range(y))/1000}. It only works when the raw sample is available.}
#' \item{kernel}{smoothing kernel choice, \code{"rect"}, \code{"gauss"}, \code{"epan"}, \code{"gausvar"}, \code{"quar"} - default: \code{"gauss"}.}
#' \item{infSupport}{logical if we expect the distribution to have infinite support or not; logical - default: \code{FALSE}.}
#' \item{outputGrid}{User defined output grid for the support of the KDE, it overrides \code{nRegGrid}; numeric - default: \code{NULL}.}
#' }
#'
#' @return A list containing the following fields:
#' \item{bw}{The bandwidth used for smoothing.}
#' \item{x}{A vector of length \code{nRegGrid} with the values of the KDE's support points.}
#' \item{y}{A vector of length \code{nRegGrid} with the values of the KDE at the support points.}
#'
#' @examples
#'
#' ### compact support case
#'
#' # input: raw sample
#' set.seed(100)
#' n <- 100
#' x0 <-seq(0,1,length.out=51)
#' Y <- rbeta(n,3,2)
#' f1 <- CreateDensity(y=Y,optns = list(outputGrid=x0))
#'
#' # input: histogram
#' histY <- hist(Y)
#' f2 <- CreateDensity(histogram=histY,optns = list(outputGrid=x0))
#'
#' # input: frequency table with unequally spaced (random) bins
#' binY <- c(0,sort(runif(9)),1)
#' freqY <- c()
#' for (i in 1:(length(binY)-1)) {
#'   freqY[i] <- length(which(Y>binY[i] & Y<=binY[i+1]))
#' }
#' f3 <- CreateDensity(freq=freqY, bin=binY,optns = list(outputGrid=x0))
#'
#' # plot
#' plot(f1$x,f1$y,type='l',col=2,lty=2,lwd=2,
#'      xlim=c(0,1),ylim=c(0,2),xlab='domain',ylab='density')
#' points(f2$x,f2$y,type='l',col=3,lty=3,lwd=2)
#' points(f3$x,f3$y,type='l',col=4,lty=4,lwd=2)
#' points(x0,dbeta(x0,3,2),type='l',lwd=2)
#' legend('topleft',
#'        c('true','raw sample','histogram','frequency table (unequal bin)'),
#'        col=1:4,lty=1:4,lwd=3,bty='n')
#'
#' ### infinite support case
#'
#' # input: raw sample
#' set.seed(100)
#' n <- 200
#' x0 <-seq(-3,3,length.out=101)
#' Y <- rnorm(n)
#' f1 <- CreateDensity(y=Y,optns = list(outputGrid=x0))
#'
#' # input: histogram
#' histY <- hist(Y)
#' f2 <- CreateDensity(histogram=histY,optns = list(outputGrid=x0))
#'
#' # input: frequency table with unequally spaced (random) bins
#' binY <- c(-3,sort(runif(9,-3,3)),3)
#' freqY <- c()
#' for (i in 1:(length(binY)-1)) {
#'   freqY[i] <- length(which(Y>binY[i] & Y<=binY[i+1]))
#' }
#' f3 <- CreateDensity(freq=freqY, bin=binY,optns = list(outputGrid=x0))
#'
#' # plot
#' plot(f1$x,f1$y,type='l',col=2,lty=2,lwd=2,
#'      xlim=c(-3,3),ylim=c(0,0.5),xlab='domain',ylab='density')
#' points(f2$x,f2$y,type='l',col=3,lty=3,lwd=2)
#' points(f3$x,f3$y,type='l',col=4,lty=4,lwd=2)
#' points(x0,dnorm(x0),type='l',lwd=2)
#' legend('topright',
#'        c('true','raw sample','histogram','frequency table (unequal bin)'),
#'        col=1:4,lty=1:4,lwd=3,bty='n')
#'
#' @references
#' \itemize{
#' \item \cite{H.-G. Müller, J.L. Wang and W.B. Capra (1997). "From lifetables to hazard rates: The transformation approach." Biometrika 84, 881--892.}
#' \item \cite{S.J. Sheather and M.C. Jones (1991). "A reliable data-based bandwidth selection method for kernel density estimation." JRSS-B 53, 683--690.}
#' \item \cite{H.-G. Müller, U. Stadtmüller, and T. Schmitt. (1987) "Bandwidth choice and confidence intervals for derivatives of noisy data." Biometrika 74, 743--749.}
#' }
#' @export
#' @importFrom fdapace Lwls1D
#' @importFrom pracma trapz

CreateDensity <- function(y=NULL, histogram=NULL, freq=NULL, bin=NULL, optns = list()){

  if (is.null(y)==TRUE) {

    if (is.null(histogram)==FALSE) {
      if (is.null(histogram$breaks)) {
        mids <- histogram$mids
        bin <- mids[1]-(mids[2]-mids[1])/2
        for (i in 2:length(mids)) {
          bin[i] <- (mids[i-1]+mids[i])/2
        }
        bin[length(mids)+1] <- mids[length(mids)]+(mids[length(mids)]-mids[length(mids)-1])/2
      } else bin <- histogram$breaks

      freq <- histogram$counts
    }

    if (is.null(freq)==TRUE) {

      if ((length(freq)+1)!=length(bin)) {
        stop('length(bin) should equal to length(freq)+1.')
      }

      # mids <- c()
      # for (i in 1:length(freq)) {
      #   mids[i] <- (bin[i]+bin[i+1])/2
      # }
    }

    y <- c()
    for (i in 1:length(freq)) {
      if (freq[i]!=0) {
        yTmp <- seq(bin[i],bin[i+1],length.out=(freq[i]+2))
        y <- c(y,yTmp[-c(1,length(yTmp))])
      }
    }
  }

  if(is.null(optns$kernel)){
    kernel = 'gauss'
  } else {
    kernel =  optns$kernel
  }

  if(is.null(optns$delta)){
    delta = diff(range(y))/1000
    #delta = max(c( diff(range(y))/1000, min(diff(sort(unique(y)))) ))
  } else {
    delta = optns$delta
  }

  if(is.null(optns$nRegGrid)){
    nRegGrid = 101
  } else {
    nRegGrid = optns$nRegGrid
  }

  if(is.null(optns$outputGrid)){
    outputGrid = NULL
  } else {
    outputGrid = optns$outputGrid
  }

  if(is.null(optns$infSupport)){
    infSupport = FALSE
  } else {
    infSupport = optns$infSupport
  }

  N = length(y)
  #histgrid = seq( min(y)-delta*0.5, max(y)+delta*0.5, by = delta );
  histgrid = c(seq( min(y), max(y), by = delta) - delta*0.5, max(y)+delta*0.5)
  if(  (max(y)+delta*0.5) > histgrid[length(histgrid)] ){
    histgrid[length(histgrid)] =  max(y)+delta*0.5;
  }
  M = length(histgrid)
  histObj =  hist(y, breaks = histgrid, plot = FALSE);
  yin = histObj$density; xin = histObj$mids
  #yin = histObj$counts[1:M-1] / N / delta;
  #xin = seq( min(y), max(y), by = delta);

  if( is.null(optns$userBwMu)){

    densTmp <- density(y,kernel='epanechnikov')
    bw <- 2*densTmp$bw
    #bw <- 2*densTmp$bw
    
  } else {
    bw = optns$userBwMu
  }

  densObj <- list()
  densObj$bw <- bw
  densObj$x <- outputGrid
  if( infSupport ){
    if(is.null(outputGrid)){
      densObj$x <- seq(min(y)-delta, max(y)+delta, length.out = nRegGrid)
    }
    qpadding = 10
    mu = fdapace::Lwls1D(bw = bw, kernel_type = kernel, win = rep(1,M+(-1+2*qpadding)),
                         xin = c( min(y) - 11:2 *delta, xin, max(y) + 2:11 * delta), yin = c(rep(0,qpadding),yin,rep(0,qpadding)), xout = densObj$x)
  } else {
    if(is.null(outputGrid)){
      densObj$x <- outputGrid <- seq(min(y), max(y), length.out = nRegGrid)
    }
    if (min(xin) > outputGrid[1]) {
      qpad1 <- seq(outputGrid[1],min(xin),by=delta)
    } else qpad1 <- numeric()
    if (max(xin) < outputGrid[length(outputGrid)]) {
      qpad2 <- seq(outputGrid[length(outputGrid)],max(xin),by=-delta)
      qpad2 <- qpad2[length(qpad2):1]
    } else qpad2 <- numeric()
    xinNew <- c(qpad1,xin,qpad2)
    yinNew <- c(rep(0,length(qpad1)),yin,rep(0,length(qpad2)))

    # note that Lwls1D requires xin to be at least of length 6
    mu = fdapace::Lwls1D(bw = bw, kernel_type = kernel, win = rep(1,length(xinNew)), xin = xinNew, yin = yinNew, xout = densObj$x)
  }
  mu[mu<0] = 0
  #densObj$y = mu / fdapace:::trapzRcpp(densObj$x, mu)
  densObj$y = mu / pracma::trapz(densObj$x, mu)

  return(densObj)
}






#' @title Fréchet Change Point Detection for Networks
#' @description Fréchet change point detection for graph Laplacian matrices, 
#'   covariance matrices, or correlation matrices 
#'   with respect to the Frobenius distance.
#' @param Ly A list (length n) of m by m matrices or a m by m by n array where
#'   \code{Ly[, , i]} contains an m by m matrix, which can be either graph 
#'   Laplacian matrices or covariance matrices or correlation matrices.
#' @param optns A list of control parameters specified by 
#'   \code{list(name = value)}. See `Details`.
#' @details Available control options are:
#' \describe{
#' \item{cutOff}{A scalar between 0 and 1 indicating the interval,
#'   i.e., [cutOff, 1 - cutOff], in which candidate change points lie.}
#' \item{Q}{A scalar representing the number of Monte Carlo simulations to run
#'   while approximating the critical value (stardized Brownian bridge).
#'   Default is 1000.}
#' \item{boot}{Logical, also compute bootstrap \eqn{p}-value if \code{TRUE}. 
#'   Default is \code{FALSE}.}
#' \item{R}{The number of bootstrap replicates. Only used when \code{boot} 
#'   is \code{TRUE}. Default is 1000.}
#' }
#' @return A \code{NetCPD} object --- a list containing the following fields:
#' \item{tau}{a scalar holding the estimated change point.}
#' \item{pvalAsy}{A scalar holding the asymptotic \eqn{p}-value.}
#' \item{pvalBoot}{A scalar holding the bootstrap \eqn{p}-value. 
#'   Returned if \code{optns$boot} is TRUE.}
#' \item{optns}{The control options used.}
#' @examples
#' \donttest{
#' set.seed(1)
#' n1 <- 100
#' n2 <- 100
#' gamma1 <- 2
#' gamma2 <- 3
#' Y1 <- lapply(1:n1, function(i) {
#'   igraph::laplacian_matrix(igraph::sample_pa(n = 10, power = gamma1, 
#'                                              directed = FALSE), 
#'                            sparse = FALSE)
#' })
#' Y2 <- lapply(1:n2, function(i) {
#'   igraph::laplacian_matrix(igraph::sample_pa(n = 10, power = gamma2, 
#'                                              directed = FALSE), 
#'                            sparse = FALSE)
#' })
#' Ly <- c(Y1, Y2)
#' res <- NetCPD(Ly, optns = list(boot = TRUE))
#' res$tau # returns the estimated change point
#' res$pvalAsy # returns asymptotic pvalue
#' res$pvalBoot # returns bootstrap pvalue
#' }
#' @references
#' \itemize{
#' \item \cite{Dubey, P. and Müller, H.G., 2020. Fréchet change-point detection. The Annals of Statistics, 48(6), pp.3312-3335.}
#' }
#' 
#' @importFrom e1071 rbridge
#' @importFrom boot boot
#' @export

NetCPD <- function(Ly = NULL, optns = list()) {
  if (is.null(Ly)) {
    stop("requires the input of Ly")
  }
  if (!is.list(Ly)) {
    if (is.array(Ly)) {
      Ly <- lapply(seq(dim(Ly)[3]), function(i) Ly[, , i])
    } else {
      stop("Ly must be a list or an array")
    }
  }
  if (is.null(optns$cutOff)) {
    optns$cutOff <- 0.1
  }
  if (is.null(optns$Q)) {
    optns$Q <- 1000
  }
  if (is.null(optns$boot)) {
    optns$boot <- FALSE
  }
  if (is.null(optns$R)) {
    optns$R <- 1000
  }
  n <- length(Ly)
  nc <- ceiling(n * optns$cutOff)
  sbb <- sapply(1:optns$Q, function(i) { # standardized Brownian bridge
    bu <- e1071::rbridge(frequency = n)[nc:(n - nc)]
    u <- (nc:(n - nc)) / n
    max(bu^2 / (u * (1 - u)))
  })
  tTau <- NetCPDStatistic(Ly, 1:n, optns$cutOff, n)
  maxnTn <- tTau[1] # the test statistic
  tau <- tTau[2] + nc - 1 # estimated change point
  pvalAsy <- length(which(sbb > maxnTn)) / optns$Q
  if (optns$boot) {
    bootSize <- n # check bootstrap scheme in the paper
    bootRes <- boot::boot(
      data = Ly, statistic = NetCPDStatistic,
      R = optns$R, cutOff = optns$cutOff, bootSize = bootSize)
    pvalBoot <- length(which(bootRes$t[, 1] > bootRes$t0[1])) / optns$R
    res <- list(tau = tau, pvalAsy = pvalAsy, pvalBoot = pvalBoot, optns = optns)
  } else {
    res <- list(tau = tau, pvalAsy = pvalAsy, optns = optns)
  }
  class(res) <- "NetCPD"
  res
}
#' Transform Cartesian to polar coordinates
#' @param x A vector holding the Cartesian coordinates.
#' @return A vector holding the polar coordinates. See \code{\link{pol2car}} for details.
#' @examples 
#' car2pol(c(-1,0,0)) # should equal c(1, pi, 0)
#' car2pol(c(1,0,1)/sqrt(2)) # should equal c(1, 0, pi/4)
#' @noRd
car2pol <- function(x, tol = 1e-10) {
  d <- length(x)
  if (d <= 1) stop("x must have at least 2 elements.")
  r <- sqrt(sum(x^2))
  if (abs(r) < tol) {
    message("Input x has norm 0.")
    return(rep(0,d)) # angles can be any feasible values; return all zeros here
  }
  existsZeroCos <- FALSE
  theta <- asin(x[d] / r)
  if (d > 2) {
    for (i in 1:(d-2)) {
      if (prod(cos(theta)) == 0) {
        existsZeroCos <- TRUE
        theta[(i+1):(d-1)] <- 0 # can be any feasible values; return all zeros here
        break
      } else {
        tmp_sintheta <- x[d-i] / (r * prod(cos(theta)))
        if (tmp_sintheta > 1) {
          theta[i+1] <- pi/2
        } else if (tmp_sintheta < -1) {
          theta[i+1] <- -pi/2
        } else {
          theta[i+1] <- asin(tmp_sintheta)
        }
      }
    }
    theta <- theta[(d-1):1]
    if (!existsZeroCos) {
      theta[1] <- abs(theta[1])
      costheta1 <- x[1] / (r * prod(cos(theta[-1])))
      sintheta1 <- x[2] / (r * prod(cos(theta[-1])))
    }
  } else {
    costheta1 <- x[1] / r
    sintheta1 <- x[2] / r
  }
  if (sintheta1 < 0) {
    if (costheta1 > 0) {
      theta[1] <- 2 * pi - theta[1]
    } else {
      theta[1] <- theta[1] + pi
    }
  } else if (costheta1 < 0) {
    theta[1] <- pi - theta[1]
  }
  return(c(r,theta))
}
#' @title Glocal Wasserstein Regression
#' @noRd
#' @description  Glocal Frechet regression with respect to the Wasserstein distance.
#'
#' @param xin An n by p matrix or a vector of length n (if p=1) with input measurements of the predictors.
#' @param qin An n by m matrix with values of quantile functions of which each row holds the quantile function values on an equispaced grid on [0, 1].
#' @param xout A k by p matrix or a vector of length k (if p=1) with output measurements of the predictors.
#' @param optns A list of control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{lower}{A scalar with the lower bound of the support of the distribution. Default is \code{NULL}.}
#' \item{upper}{A scalar with the upper bound of the support of the distribution. Default is \code{NULL}.}
#' \item{Rsquared}{A logical variable indicating whether R squared would be returned. Default is \code{FALSE}.}
#' \item{qSup}{A numerical vector of length m holding the probability grid on [0, 1] at which the input quantile functions take values. If \code{optns$Rsquared} is TRUE, \code{qSup} is needed. Default is \code{seq(1,2*m,2)/2/m}.}
#' }
#' @importFrom osqp solve_osqp osqpSettings
#' @importFrom pracma trapz

GloWassReg <- function(xin, qin, xout, optns=list()){
  if (is.null(optns$Rsquared)) optns$Rsquared <- FALSE

  if(is.vector(xin)){
    xin <- as.matrix(xin)
  }
  if(is.vector(xout)){
    xout <- as.matrix(xout)
  }
  if(nrow(xin)!=nrow(qin))
    stop("The numbers of observations in xin and qin are not the same.")
  if(ncol(xin)!=ncol(xout))
    stop("The numbers of variables in xin and xout are not the same.")
  if(optns$Rsquared & is.null(optns$qSup)){
    warning("optns$qSup is missing and taking the default value.")
  }

  k <- nrow(xout)
  n <- nrow(xin)
  m <- ncol(qin)
  xbar <- colMeans(xin)
  Sigma <- cov(xin) * (n-1) / n
  invSigma <- solve(Sigma)

  # if lower & upper are neither NULL
  A <- cbind(diag(m), rep(0,m)) + cbind(rep(0,m), -diag(m))
  if(!is.null(optns$upper) & !is.null(optns$lower)){
    b0 <- c(optns$lower, rep(0,m-1), -optns$upper)
  }else if(!is.null(optns$upper)){
    A <- A[,-1]
    b0 <- c(rep(0,m-1), -optns$upper)
  }else if(!is.null(optns$lower)){
    A <- A[,-ncol(A)]
    b0 <- c(optns$lower,rep(0,m-1))
  }else{
    A <- A[,-c(1,ncol(A))]
    b0 <- rep(0,m-1)
  }
  Pmat <- as(diag(m), "sparseMatrix")
  Amat <- as(t(A), "sparseMatrix")

  qout <- sapply(1:k, function(j){
    s <- 1 + t(t(xin) - xbar) %*% invSigma %*% (xout[j,] - xbar)
    s <- as.vector(s)
    gx <- colMeans(qin * s)

    #res <- do.call(quadprog::solve.QP, list(diag(m), gx, A, b0))
    #return(sort(res$solution)) #return(res$solution)


    res <- do.call(osqp::solve_osqp,
                   list(P=Pmat, q= -gx, A=Amat, l=b0, pars = osqp::osqpSettings(verbose = FALSE)))
    return(sort(res$x))
  })
  qout <- t(qout)

  if (!optns$Rsquared) {
    return(list(qout=qout))
  } else {
    qMean <- colMeans(qin)
    if (k == n) {
      if (sum(abs(xout-xin)) > 1e-10*length(xout))
        qin.est <- qout
    } else {
      qin.est <- sapply(1:n, function(j){
        s <- 1 + t(t(xin) - xbar) %*% invSigma %*% (xin[j,] - xbar)
        s <- as.vector(s)
        gx <- colMeans(qin * s)

        #res <- do.call(quadprog::solve.QP, list(diag(m), gx, A, b0))
        #return(sort(res$solution)) #return(res$solution)

        res <- do.call(osqp::solve_osqp,
                       list(P=Pmat, q= -gx, A=Amat, l=b0, pars = osqp::osqpSettings(verbose = FALSE)))
        return(sort(res$x))
      })
      qin.est <- t(qin.est)
    }
    Rsq <- ifelse(
      is.null(optns$qSup),
      1 - sum(t(qin - qin.est)^2) / sum((t(qin) - qMean)^2),
      1 - pracma::trapz(x=optns$qSup, y=colSums((qin - qin.est)^2)) /
        pracma::trapz(x=optns$qSup, y=rowSums((t(qin) - qMean)^2))
    )
    if(Rsq < 0) Rsq <- 0
    return(list(qout=qout, R.squared=Rsq))
  }
}
#'@title Global Fréchet regression of covariance matrices
#'@description Global Fréchet regression of covariance matrices with Euclidean predictors.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed. See 'metric' option in 'Details' for more details.
#'@param M A q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.  See 'metric' option in 'Details' for more details.
#'@param xout An m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{metric}{Metric type choice, \code{"frobenius"}, \code{"power"}, \code{"log_cholesky"}, \code{"cholesky"} - default: \code{"frobenius"} which corresponds to the power metric with \code{alpha} equal to 1.
#' For power (and Frobenius) metrics, either \code{y} or \code{M} must be input; \code{y} would override \code{M}. For Cholesky and log-Cholesky metrics, \code{M} must be input and \code{y} does not apply.}
#' \item{alpha}{The power parameter for the power metric. Default is 1 which corresponds to Frobenius metric.}
#' }
#' @return A \code{covReg} object --- a list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance or correlation matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#'#Example y input
#'n=50             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Cov_est=GloCovReg(x=x,y=y,xout=xout,optns=list(corrOut=FALSE,metric="power",alpha=3))
#'#Example M input
#'n=10 #sample size
#'m=5 # dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'x=cbind(matrix(rnorm(n),n),matrix(rnorm(n),n)) #vector of predictor values
#'xout=cbind(runif(3),runif(3)) #output predictor levels
#'Cov_est=GloCovReg(x=x,M=M,xout=xout,optns=list(corrOut=FALSE,metric="power",alpha=3))
#'
#' @references
#' \itemize{
#' \item \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \item \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' \item \cite{Lin, Z. (2019). Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. Siam. J. Matrix. Anal, A. 40, 1353--1370.}
#' }
#' @export

GloCovReg= function(x,y=NULL,M=NULL,xout,optns = list()){
  if (is.null(optns$metric)){
    metric="frobenius"
  } else {
    metric=optns$metric
  }
  if(!metric%in%c("frobenius","power","cholesky","log_cholesky")){
    stop("metric choice not supported.")
  }
  if(metric=="frobenius"){
    res <- GFRCov(x=x, y=y,M=M,xout=xout,optns = optns)
  } else if(metric=="power"){
    res <- GFRCovPower(x=x, y=y,M=M,xout=xout,optns = optns)
  } else {
    if (is.null(M))
      stop("M must be input for Cholesky and log-Cholesky metrics; y does not apply.")
    res <- GFRCovCholesky(x=x, M=M, xout=xout, optns = optns)
  }
  class(res) <- "covReg"
  return(res)
}
#' @title Global density regression.
#' @description Global Fréchet regression for densities with respect to \eqn{L^2}-Wasserstein distance.
#' @param xin An n by p matrix or a vector of length n (if p=1) with input measurements of the predictors.
#' @param yin A matrix or list holding the sample of observations of the response. If \code{yin} is a matrix, each row holds the observations of the response corresponding to a row in \code{xin}.
#' @param hin A list holding the histograms of the response corresponding to each row in \code{xin}.
#' @param qin A matrix or list holding the quantile functions of the response. If \code{qin} is a matrix, each row holds the quantile function of the response taking values on \code{optns$qSup} corresponding to a row in \code{xin}.
#' Note that only one of the three \code{yin}, \code{hin}, and \code{qin} needs to be input.
#' If more than one of them are specified, \code{yin} overwrites \code{hin}, and \code{hin} overwrites \code{qin}.
#' @param xout A k by p matrix or a vector of length k (if p=1) with output measurements of the predictors. Default is \code{xin}.
#' @param optns A list of control parameters specified by \code{list(name=value)}.
#' @details Available control options are \code{qSup}, \code{nqSup}, 
#' \code{lower}, \code{upper}, \code{Rsquared}, \code{bwDen}, \code{ndSup}, \code{dSup}, 
#' \code{delta}, \code{kernelDen}, \code{infSupport}, and \code{denLowerThreshold}. 
#' \code{Rsquared} is explained as follows and see \code{\link{LocDenReg}} for the other options.
#' \describe{
#' \item{Rsquared}{A logical variable indicating whether R squared would be returned. Default is \code{FALSE}.}
#' }
#' @return A list containing the following components:
#' \item{xout}{Input \code{xout}.}
#' \item{dout}{A matrix or list holding the output densities corresponding to \code{xout}. If \code{dout} is a matrix, each row gives a density and the domain grid is given in \code{dSup}. If \code{dout} is a list, each element is a list of two components, \code{x} and \code{y}, giving the domain grid and density function values, respectively.}
#' \item{dSup}{A numeric vector giving the domain grid of \code{dout} when it is a matrix.}
#' \item{qout}{A matrix holding the quantile functions of the output densities. Each row corresponds to a value in \code{xout}.}
#' \item{qSup}{A numeric vector giving the domain grid of \code{qout}.}
#' \item{xin}{Input \code{xin}.}
#' \item{din}{Densities corresponding to the input \code{yin}, \code{hin} or \code{qin}.}
#' \item{qin}{Quantile functions corresponding to the input \code{yin}, \code{hin} or \code{qin}.}
#' \item{Rsq}{A scalar giving the R squared value if \code{optns$Rsquared = TRUE}.}
#' \item{optns}{A list of control options used.}
#'
#' @examples
#' xin = seq(0,1,0.05)
#' yin = lapply(xin, function(x) {
#'   rnorm(100, rnorm(1,x,0.005), 0.05)
#' })
#' qSup = seq(0,1,0.02)
#' xout = seq(0,1,0.25)
#' res1 <- GloDenReg(xin=xin, yin=yin, xout=xout, optns = list(qSup = qSup))
#' plot(res1)
#'\donttest{
#' hin = lapply(yin, function(y) hist(y, breaks = 50, plot=FALSE))
#' res2 <- GloDenReg(xin=xin, hin=hin, xout=xout, optns = list(qSup = qSup))
#' plot(res2)
#'}
#' @references
#' \cite{Petersen, A., & Müller, H.-G. (2019). "Fréchet regression for random objects with Euclidean predictors." The Annals of Statistics, 47(2), 691--719.}
#' @export
#' @importFrom fdadensity dens2quantile
#' @importFrom pracma trapz

GloDenReg <- function(xin=NULL, yin=NULL, hin=NULL, qin=NULL, xout=NULL, optns=list()) {
  if (is.null(optns$Rsquared)) optns$Rsquared <- FALSE
  if (is.null(xin))
    stop ("xin has no default and must be input by users.")
  if (is.null(yin) & is.null(qin) & is.null(hin))
    stop ("One of the three arguments, yin, hin and qin, should be input by users.")
  if (is.null(xout))
    xout <- xin
  if (!is.null(optns$qSup)) {
    if (min(optns$qSup) != 0 | max(optns$qSup) - 1 != 0)
      stop ("optns$qSup must have minimum 0 and maximum 1.")
    if (sum(duplicated(optns$qSup)) > 0) {
      optns$qSup <- unique(optns$qSup)
      warning ("optns$qSup has duplicated elements which has been removed.")
    }
    if (is.unsorted(optns$qSup)) {
      optns$qSup <- sort(optns$qSup)
      warning ("optns$qSup has been reordered to be increasing.")
    }
  } else {
    if (!(is.null(yin) & is.null(hin))) {
      if(is.null(optns$nqSup)) {
        optns$nqSup <- 201
      }
      optns$qSup <- seq(0,1,length.out = optns$nqSup)
    } else {
      if (is.matrix(qin)) {
        optns$qSup <- seq(0,1,length.out = ncol(qin))
        warning ("optns$qSup is missing and is set by default as an equidistant grid on [0,1] with length equal to the number of columns in matrix qin.")
      } else {
        if(is.null(optns$nqSup)) {
          optns$nqSup <- 201
        }
        optns$qSup <- seq(0,1,length.out = optns$nqSup)
      }
    }
  }
  qSup <- optns$qSup

  optnsRegIdx <- match(c("Rsquared","lower","upper","qSup","nqSup"), names(optns))
  optnsRegIdx <- optnsRegIdx[!is.na(optnsRegIdx)]
  optnsReg <- optns[optnsRegIdx]

  optnsDen <- optns[-optnsRegIdx]
  if (!is.null(optnsDen$kernelDen))
    names(optnsDen)[which(names(optnsDen) == "kernelDen")] <- "kernel"
  if (!is.null(optnsDen$bwDen))
    names(optnsDen)[which(names(optnsDen) == "bwDen")] <- "userBwMu"
  # moved to just the last step transforming output quantile to densities
  # don't want a common support before F reg
  #if (!is.null(optnsDen$ndSup))
  #  names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
  #if (!is.null(optnsDen$dSup))
  #  names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"

  if (!(is.null(yin) & is.null(hin))) {
    #require(fdadensity)

    if (!is.null(yin)) {
      if (!is.null(hin) | !is.null(qin))
        warning ("hin and qin are redundant when yin is available.")
      if (is.matrix(yin))
        yin <- as.data.frame(t(yin))
      if (!is.list(yin))
        stop ("yin must be a matrix or list.")
      den <- lapply(yin, CreateDensity, optns = optnsDen)
    } else if (!is.null(hin)) {
      if (!is.null(qin))
        warning ("qin is redundant when hin is available.")
      for (histogram in hin) {
        if (!is.list(histogram))
          stop ("Each element of hin must be a list.")
        if (is.null(histogram$breaks) & is.null(histogram$mids))
          stop ("Each element of hin must be a list with at least one of the components breaks or mids.")
        if (is.null(histogram$counts))
          stop ("Each element of hin must be a list with component counts.")
      }
      den <- lapply(hin, function(histogram) {
        CreateDensity(histogram = histogram, optns = optnsDen)
      })
    }
    qin <- sapply(den, function(deni) {
      fdadensity::dens2quantile(dens = deni$y, dSup = deni$x, qSup = qSup)
    })
    qin <- t(qin)
  } else {
    #if (!is.matrix(qin))
    #  stop ("qin must be a matrix, of which each row holding the values of a quantile function evaluated on a common grid from 0 to 1.")
    if (!is.matrix(qin)) {
      if (!is.list(qin))
        stop ("qin must be a matrix or list.")
      for (qt in qin) {
        if (!is.list(qt)) {
          stop ("If qin is a list, each element must also be a list with two components, x and y.")
        } else if (is.null(qt$x) | is.null(qt$y)) {
          stop ("If qin is a list, each element must also be a list with two components, x and y.")
        }
      }
      qin <- sapply(qin, function(q) {
        approx(x = q$x, y = q$y, xout = qSup, rule = 2)$y
      })
      qin <- t(qin)
    }
    den <- apply(qin, 1, function(q) qf2pdf(qf = sort(q),prob = qSup))
  }

  if (is.null(optns$denLowerThreshold)) {
    optns$denLowerThreshold <- 0.001 * mean(qin[,ncol(qin)] - qin[,1])
  } else if (optns$denLowerThreshold) {
    if(!is.numeric(optns$denLowerThreshold) | optns$denLowerThreshold < 0)
      optns$denLowerThreshold <- 0.001 * mean(qin[,ncol(qin)] - qin[,1])
  }

  if (optns$denLowerThreshold) {
    # density thresholding from below
    if (sum(sapply(den, function(d) sum(d$y < optns$denLowerThreshold/diff(range(d$x))))) > 0) {
      den <- lapply(den, function(d) {
        lower <- optns$denLowerThreshold/diff(range(d$x))
        if (sum(d$y < lower) > 0) {
          d$y[d$y < lower] <- lower
          d$y <- d$y / pracma::trapz(d$x,d$y)
        }
        list(x=d$x, y=d$y)
      })
      qin <- sapply(den, function(deni) {
        fdadensity::dens2quantile(dens = deni$y, dSup = deni$x, qSup = qSup)
      })
      qin <- t(qin)
    }
  }

  if (sum(abs(xin - 1)) == 0) {
    # compute the Fréchet mean
    qout <- t(colMeans(qin))
  } else {
    regRes <- GloWassReg(xin = xin, qin = qin, xout = xout, optns = optnsReg)
    qout <- regRes$qout
  }

  if (!is.null(optnsDen$ndSup))
    names(optnsDen)[which(names(optnsDen) == "ndSup")] <- "nRegGrid"
  if (!is.null(optnsDen$dSup))
    names(optnsDen)[which(names(optnsDen) == "dSup")] <- "outputGrid"

  if (is.null(optnsDen$outputGrid)) {
    dout <- apply(qout, 1, qf2pdf, prob = qSup, optns = optnsDen)
    dout <- lapply(dout, function(d) d[c("x","y")])
    res <- list(xout = xout, dout = dout, qout = qout, qSup = qSup, xin=xin, din=den, qin=qin, optns=optns)
  } else {
    dSup <- optnsDen$outputGrid
    dout <- apply(qout, 1, function(q) qf2pdf(q, prob = qSup, optns = optnsDen)$y)
    #dout <- apply(qout, 1, qnt2dens, qSup = qSup, dSup = optnsDen$outputGrid)
    dout <- t(dout)
    res <- list(xout = xout, dout = dout, dSup = dSup, qout = qout, qSup = qSup, xin=xin, din=den, qin=qin, optns=optns)
  }
  if (optns$Rsquared & sum(abs(xin - 1)) > 0) res$Rsq <- regRes$R.squared
  class(res) <- "denReg"
  return(res)
}
#' Hessian \eqn{\partial^2/\partial y \partial y^\top} of the geodesic distance \eqn{\arccos(x^\top y)} on a unit hypersphere
#' @param x,y Two unit vectors.
#' @return A Hessian matrix.
#' @export
SpheGeoHess <- function(x,y) { #,tol = 1e-10){
  return(- sum(x * y) * (1 - sum(x * y) ^ 2) ^ (-1.5) * x %*% t(x))
}#'@title Global Fréchet regression of covariance matrices with power metric
#'@noRd
#'@description Global Fréchet regression of covariance matrices with Euclidean predictors and power metric.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.
#'@param M A q by q by n array (resp. a list of q by q matrices) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#'@param xout An m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' \item{alpha}{Non-negative parameter from the power metric. Default is 1 which corresponds to Frobenius metric.}
#' }
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance or correlation matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' \donttest{
#'#Example y input
#'n=200             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Cov_est=GFRCovPower(x=x,y=y,xout=xout,optns=list(alpha=3,corrOut=FALSE))
#'#Example M input
#'n=10 #sample size
#'m=5 # dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'x=cbind(matrix(rnorm(n),n),matrix(rnorm(n),n)) #vector of predictor values
#'xout=cbind(runif(3),runif(3)) #output predictor levels
#'Cov_est=GFRCovPower(x=x,M=M,xout=xout,optns=list(alpha=3,corrOut=FALSE))
#'}
#' @references
#' \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' @importFrom Matrix nearPD forceSymmetric


GFRCovPower  = function(x,y=NULL,M=NULL,xout,optns = list()){
  if(is.null(optns$corrOut)){
    corrOut=FALSE
  } else{
    corrOut=optns$corrOut
  }
  if(is.null(optns$alpha)){
    alpha=1
  } else{
    alpha=optns$alpha
  }

  if(!is.matrix(x)){
    stop('x must be a matrix')
  }
  if(!is.matrix(xout)){
    stop('xout must be a matrix')
  }
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have the same number of columns')
  }
  if(alpha<0){
    stop("alpha must be non-negative")
  }
  invVa = solve(var(x))
  mx = apply(x,2,mean)

  if(!is.null(y)){
    if(!is.matrix(y)){
      stop('y must be a matrix')
    }
    if(nrow(x) != nrow(y)){
      stop('x and y must have the same number of rows')
    }
    n = nrow(y)
    cm = mean4GloCovReg(x=x,y=y,xout=x)$mean_out
    #conditional covariance
    M=array(0,c(dim(y)[2], dim(y)[2], dim(y)[1]))
    for(i in 1:n){
      M[,,i] = (y[i,] - cm[i,]) %*% t(y[i,] - cm[i,])
    }
  } else{
    if(is.null(M)){
      stop("y or M must be provided.")
    }
    if(is.list(M)){
      M=array(as.numeric(unlist(M)), dim=c(dim(M[[1]])[1],dim(M[[1]])[1],length(M)))
    }else{
      if(!is.array(M)){
        stop('M must be an array or a list')
      } else if (length(dim(M))!=3) {
        stop('M must be an array or a list')
      }
    }
    if(nrow(x)!=dim(M)[3]){
      stop("The number of rows of x must be the same as the number of covariance matrices in M")
    }
  }
  M_hat=array(0,c(dim(M)[1],dim(M)[1],nrow(xout)))
  n=dim(x)[1]

  if(alpha>0){
    for(j in 1:nrow(xout)){
      s = array(0,n)
      for(i in 1:n){
        s[i] = 1+(x[i,]-mx)%*%invVa%*%(xout[j,]-mx)
      }
      for(i in 1:n){
        P=eigen(M[,,i])$vectors
        Lambd_alpha=diag(pmax(0,eigen(M[,,i])$values)**alpha)
        M_alpha=P%*%Lambd_alpha%*%t(P)
        M_hat[,,j]=M_hat[,,j]+s[i]*M_alpha/n
      }
      M_hat[,,j]=as.matrix(Matrix::nearPD(M_hat[,,j],corr = FALSE)$mat)
      P=eigen(M_hat[,,j])$vectors
      Lambd_alpha=diag(pmax(0,eigen(M_hat[,,j])$values)**(1/alpha))
      M_hat[,,j]=P%*%Lambd_alpha%*%t(P)
      M_hat[,,j]=as.matrix(Matrix::forceSymmetric(M_hat[,,j]))
    }
  } else{
    for(j in 1:nrow(xout)){
      s = array(0,n)
      for(i in 1:n){
        s[i] = 1+(x[i,]-mx)%*%invVa%*%(xout[j,]-mx)
      }
      for(i in 1:n){
        P=eigen(M[,,i])$vectors
        Lambd_alpha=diag(log(pmax(1e-30,eigen(M[,,i])$values)))
        M_alpha=P%*%Lambd_alpha%*%t(P)
        M_hat[,,j]=M_hat[,,j]+s[i]*M_alpha/n
      }
      M_hat[,,j]=as.matrix(Matrix::nearPD(M_hat[,,j],corr = FALSE)$mat)
      P=eigen(M_hat[,,j])$vectors
      Lambd_alpha=diag(exp(pmax(0,eigen(M_hat[,,j])$values)))
      M_hat[,,j]=P%*%Lambd_alpha%*%t(P)
      M_hat[,,j]=as.matrix(Matrix::forceSymmetric(M_hat[,,j]))
    }
  }
  if(corrOut){
    for(j in 1:nrow(xout)){
      D=diag(1/sqrt(diag(M_hat[,,j])))
      M_hat[,,j]=D%*%M_hat[,,j]%*%D
      M_hat[,,j]=as.matrix(Matrix::forceSymmetric(M_hat[,,j]))
    }
  }
  Mout=list()
  for(j in 1:nrow(xout)){
    Mout=c(Mout,list(M_hat[,,j]))
  }
  optns$corrOut=corrOut
  optns$alpha=alpha
  return(list(xout=xout, Mout=Mout, optns=optns))
}






#' @title Generalized Fréchet integrals of 1D distribution 
#' @description Calculating generalized Fréchet integrals of 1D distribution (equipped with Wasserstein distance) 
#' @param phi An eigenfunction along which we want to project the distribution
#' @param t_out Support of \code{phi}
#' @param Q A \code{length(t_out)} X \code{length(Qout)} matrix whose jth row corresponds to the quantile function on grid \code{Qout} for the jth time point.   
#' @param Qout Support of the quantile valued process
#' @return A list of the following:
#' \item{f}{Quantile function corresponding to the frechet integral of \code{Q} along \code{phi}}
#' @examples 
#' #simulation as in the paper Dubey, P., & Müller, H. G. (2020).
#' # Functional models for time‐varying random objects. 
#' # JRSSB, 82(2), 275-327.
#' \donttest{
#' n <- 100
#' N <- 50
#' t_out <- seq(0,1,length.out = N)
#' 
#' phi1 <- function(t){
#'   (t^2-0.5)/0.3416
#' }
#' phi2 <- function(t){
#'   sqrt(3)*t
#' }
#' phi3 <- function(t){
#'   (t^3 - 0.3571*t^2 - 0.6*t + 0.1786)/0.0895
#' }
#' 
#' Z <- cbind(rnorm(n)*sqrt(12), rnorm(n), runif(n)*sqrt(72), runif(n)*sqrt(9))
#' mu_vec <- 1 + Z[,1] %*% t(phi1(t_out)) + Z[,2] %*% t(phi3(t_out))
#' sigma_vec <- 3 + Z[,3] %*% t(phi2(t_out)) + Z[,4] %*% t(phi3(t_out))
#' 
#' # grids of quantile function
#' Nq <- 40
#' eps <- 0.00001
#' Qout <- seq(0+eps,1-eps,length.out=Nq)
#' 
#' # I: four dimension array of n x n matrix of squared distances 
#' # between the time point u of the ith process and 
#' # process and the time point v of the jth object process, 
#' # e.g.: I[i,j,u,v] <- d_w^2(X_i(u) X_j(v)).
#' I <- array(0, dim = c(n,n,N,N))
#' for(i in 1:n){
#'   for(j in 1:n){
#'     for(u in 1:N){
#'       for(v in 1:N){
#'         #wasserstein distance between distribution X_i(u) and X_j(v) 
#'         I[i,j,u,v] <- (mu_vec[i,u] - mu_vec[j,v])^2 + (sigma_vec[i,u] - sigma_vec[j,v])^2
#'       }
#'     }
#'   }
#' }
#' 
#' # check ObjCov work 
#' Cov_result <- ObjCov(t_out, I, 3)
#' #Cov_result$lambda #12 6 1.75
#' 
#' # calculate Q 
#' i <- 6 # for the ith subject
#' Q <- t(sapply(1:N, function(t){
#'   qnorm(Qout, mean = mu_vec[i,t], sd = sigma_vec[i,t])
#' }))
#' 
#' score_result <- WassFIntegral(Cov_result$phi[,1], t_out, Q, Qout)
#' score_result$f
#' }
#' @references 
#' \cite{Dubey, P., & Müller, H. G. (2020). Functional models for time‐varying random objects. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2), 275-327.}
#' @export
#' @import fdapace
#' @import quadprog
#' @import pracma

WassFIntegral <- function(phi, t_out, Q, Qout){

  m <- length(Qout)
  if(m<2){
    stop("length of Q out too small, Please increase ")
  }
  if(length(phi)!=length(t_out)){
    stop("length of t_out and phi inconsistent")
  }
  if(dim(Q)[2]!=length(Qout)){
    stop("column length of Qout and length Qout inconsistent")
  }
  if(dim(Q)[1]!=length(t_out)){
    stop("row length of Qout and length t_out inconsistent")
  }
  g_mini <- rep(0, m)
  phi_out <- phi / fdapace::trapzRcpp(t_out, phi)
  #g_mini = \int Q(S(t))(u)phi(t)dt
  for (i in 1:m){
    g_mini[i] <- fdapace::trapzRcpp(t_out, phi_out * Q[,i])
  }
  D <- pracma::Toeplitz(a=c(-1,rep(0,m-2)),
                       b=c(-1,1, rep(0,m-2)))
  f <- quadprog::solve.QP(diag(m),g_mini,t(D),rep(0,m-1))
  return(list(f=f$solution))
}




#' Compute an exponential map for a unit hypersphere.
#' @param base A unit vector of length \eqn{m} holding the base point of the tangent space.
#' @param tg A vector of length \eqn{m} of which the exponential map is taken.
#' @return A unit vector of length \eqn{m}.
#' @export
expSphere <- function(base,tg) {
  tgNorm <- l2norm(tg)
  if (!is.na(tgNorm) & isTRUE(all.equal(tgNorm,0)) ) {
    base
  } else {
    sin(tgNorm) * tg / tgNorm + cos(tgNorm) * base
  }
}#' @description Helper function computing bootstrap replications of
#'   the Fréchet ANOVA test statistics for densities/quantiles.
#' @importFrom pracma trapz
#' @noRd

DenANOVAStatistic <- function(data, indices, sizes, qSup) {
  k <- length(sizes) # number of groups
  n <- sum(sizes) # total number of observations
  LyBoot <- data[indices] # booted sample
  lambda <- sizes / n # ratio for each group
  groupData <- split(LyBoot, rep(1:k, sizes))
  mup <- rowMeans(matrix(unlist(LyBoot), ncol = n))
  Vp <- mean(sapply(LyBoot, function(LyBooti) {
    pracma::trapz(qSup, (LyBooti - mup)^2)
  }))
  V <- rep(0, k)
  sigma2 <- rep(0, k)
  for (i in 1:k) {
    mui <- rowMeans(matrix(unlist(groupData[[i]]), ncol = sizes[i]))
    Di <- sapply(groupData[[i]], function(Lyi) {
      pracma::trapz(qSup, (Lyi - mui)^2)
    })
    V[i] <- mean(Di)
    sigma2[i] <- mean(Di^2) - (mean(Di))^2
  }
  Fn <- Vp - sum(lambda * V)
  Un <- 0
  for (i in 1:(k - 1)) {
    for (j in (i + 1):k) {
      Un <- Un + lambda[i] * lambda[j] * (V[i] - V[j])^2 / (sigma2[i] * sigma2[j])
    }
  }
  Tn <- n * Un / sum(lambda / sigma2) + n * Fn^2 / sum(lambda^2 * sigma2)
  Tn
}
#' Transform polar to Cartesian coordinates
#' @param p A vector of length \eqn{d} \eqn{(d\ge 2)} with the first element being the radius and the others being the angles,
#' where \code{p[2]} takes values in \eqn{[0,2\pi]} and \code{p[i]} takes values in \eqn{[-\pi/2,\pi/2]}, for all \eqn{i>2} if any.
#' @return A vector of length \eqn{d} holding the corresponding Cartesian coordinates 
#' \deqn{\left(r\prod_{i=1}^{d-1}\cos\theta_i, r\sin\theta_1\prod_{i=2}^{d-1}\cos\theta_i, r\sin\theta_2\prod_{i=3}^{d-1}\cos\theta_i,\dots, r\sin\theta_{d-2}\cos\theta_{d-1}, r\sin\theta_{d-1}\right),}
#' where \eqn{r} is given by \code{p[1]} and \eqn{\theta_i} is given by \code{p[i+1]} for \eqn{i=1,\dots,d-1}.
#' @examples
#' pol2car(c(1, 0, pi/4)) # should equal c(1,0,1)/sqrt(2)
#' pol2car(c(1, pi, 0)) # should equal c(-1,0,0)
#' @export
pol2car <- function(p) {
  d <- length(p)
  if (d < 2) stop("p must have at least 2 elements.")
  r <- p[1]
  theta <- p[-1]
  x <- r * sin(theta[d-1])
  if (d == 2) {
    x <- c(r * cos(theta), x)
  } else {
    for (i in 2:(d-1)) {
      x[i] <- r * prod(cos(theta[(d-i+1):(d-1)])) * sin(theta[d-i])
    }
    x[d] <- r * prod(cos(theta))
    x <- x[d:1]
  }
  return(x)
}
#' @title Converting a quantile function to a density
#' @noRd
#' @param qf a numerical vector holding the values of a quantile function at a probability grid.
#' @param prob a numerical vector holding the probability grid at which the quantile function takes values. By default, \code{prob} is an equidistant sequence with the same length as \code{qf}.
#' @param breaks a numerical vector holding the breakpoints between histogram bins. By default, \code{breaks} is an equidistant sequence from the minimum to the maximum values of \code{qf}.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{userBwMu}{The bandwidth value for the smoothed mean function; positive numeric - default: determine automatically based on the data-driven bandwidth selector proposed by Sheather and Jones (1991)}
#' \item{nRegGrid}{The number of support points the KDE; numeric - default: 101.}
#' \item{delta}{The size of the bin to be used; numeric - default: determine automatically as "max(c(diff(range(y))/1000, min(diff(sort(unique(y))))))". It only works when the raw sample is available.}
#' \item{kernel}{smoothing kernel choice, "rect", "gauss", "epan", "gausvar", "quar" - default: "gauss"}
#' \item{infSupport}{logical if we expect the distribution to have infinite support or not; logical - default: TRUE}
#' \item{outputGrid}{User defined output grid for the support of the KDE, it overrides nRegGrid; numeric - default: NULL}
#' }
#'
#' @return A list containing the following fields:
#' \item{bw}{Variance for measure error.The bandwidth used by smoothing.}
#' \item{x}{A vector of length \emph{nGridReg} with the values of the KDE's support points.}
#' \item{y}{A vector of length \emph{nGridReg} with the values of the KDE at the support points.}

qf2pdf <- function(qf=NULL, prob=NULL, breaks=NULL, optns=list()){
  hist = qf2hist(qf=qf, prob=prob, breaks=breaks)
  return(CreateDensity(histogram = hist, optns=optns))
}
#'@title Global Fréchet regression of covariance matrices with Frobenius metric
#'@noRd
#'@description Global Fréchet regression of covariance matrices with Euclidean predictors and Frobenius metric.
#'@param x An n by p matrix of predictors.
#'@param y An n by l matrix, each row corresponds to an observation, l is the length of time points where the responses are observed.
#'@param M A q by q by n array (resp. list) where \code{M[,,i]} (resp. \code{M[[i]]}) contains the i-th covariance matrix of dimension q by q.
#'@param xout An m by p matrix of output predictor levels.
#' @param optns A list of options control parameters specified by \code{list(name=value)}. See `Details'.
#' @details Available control options are
#' \describe{
#' \item{corrOut}{Boolean indicating if output is shown as correlation or covariance matrix. Default is \code{FALSE} and corresponds to a covariance matrix.}
#' }
#' @return A list containing the following fields:
#' \item{xout}{An m by p matrix of output predictor levels.}
#' \item{Mout}{A list of estimated conditional covariance or correlation matrices at \code{xout}.}
#' \item{optns}{A list containing the \code{optns} parameters utilized.}
#' @examples
#' \donttest{
#'#Example raw vector y as input
#'n=200             # sample size
#'t=seq(0,1,length.out=100)       # length of data
#'x = matrix(runif(n),n)
#'theta1 = theta2 = array(0,n)
#'for(i in 1:n){
#'  theta1[i] = rnorm(1,x[i],x[i]^2)
#'  theta2[i] = rnorm(1,x[i]/2,(1-x[i])^2)
#'}
#'y = matrix(0,n,length(t))
#'phi1 = sqrt(3)*t
#'phi2 = sqrt(6/5)*(1-t/2)
#'y = theta1%*%t(phi1) + theta2 %*% t(phi2)
#'xout = matrix(c(0.25,0.5,0.75),3)
#'Cov_est=GFRCov(x=x,y=y,xout=xout,optns=list(corrOut=FALSE))
#'#Example array of covariance matrices M as input
#'n=10 #sample size
#'m=5 # dimension of covariance matrices
#'M <- array(0,c(m,m,n))
#'for (i in 1:n){
#'  y0=rnorm(m)
#'  aux<-diag(m)+y0%*%t(y0)
#'  M[,,i]<-aux
#'}
#'x=matrix(rnorm(n),n) #vector of predictor values
#'xout=matrix(rnorm(3,0,1.5),3) #output predictor levels
#'Cov_est=GFRCov(x=x,M=M,xout=xout,optns=list(corrOut=FALSE))
#'}
#' @references
#' \cite{Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean predictors. The Annals of Statistics, 47(2), 691--719.}
#' \cite{Petersen, A., Deoni, S. and Müller, H.-G. (2019). Fréchet estimation of time-varying covariance matrices from sparse data, with application to the regional co-evolution of myelination in the developing brain. The Annals of Applied Statistics, 13(1), 393--419.}
#' @importFrom Matrix nearPD forceSymmetric


GFRCov  = function(x, y=NULL,M=NULL,xout,optns = list()){
  if(is.null(optns$corrOut)){
    corrOut=FALSE
  } else{
    corrOut=optns$corrOut
  }
  if(!is.matrix(x)){
    stop('x must be a matrix')
  }
  if(!is.matrix(xout)){
    stop('xout must be a matrix')
  }
  if(ncol(x) != ncol(xout)){
    stop('x and xout must have the same number of columns')
  }
  invVa = solve(var(x))
  mx = apply(x,2,mean)
  if(!is.null(y)){
    if(!is.matrix(y)){
      stop('y must be a matrix')
    }
    if(nrow(x) != nrow(y)){
      stop('x and y must have the same number of rows')
    }
    n = nrow(y)
    cm = mean4GloCovReg(x,y,x)$mean_out
    M=array(0,c(dim(y)[2], dim(y)[2], dim(y)[1]))
    for(i in 1:n){
      M[,,i] = (y[i,] - cm[i,]) %*% t(y[i,] - cm[i,])
    }
  } else{
    if(!is.null(M)){
      if(is.list(M)){
        M=array(as.numeric(unlist(M)), dim=c(dim(M[[1]])[1],dim(M[[1]])[1],length(M)))
      } else{
        if(!is.array(M)){
          stop('M must be an array or a list')
        } else if (length(dim(M))!=3) {
          stop('M must be an array or a list')
        }
      }
      if(nrow(x)!=dim(M)[3]){
        stop("The number of rows of x must be the same as the number of covariance matrices in M")
      }
    } else{
      stop("y or M must be provided.")
    }
  }
  M_hat=array(0,c(dim(M)[1],dim(M)[1],nrow(xout)))
  n=dim(x)[1]
  for(j in 1:nrow(xout)){
    s = array(0,n)
    for(i in 1:n){
      s[i] = 1+(x[i,]-mx)%*%invVa%*%(xout[j,]-mx)
    }
    for(i in 1:n){
      M_hat[,,j]=M_hat[,,j]+s[i]*M[,,i]/n
    }
    M_hat[,,j]=as.matrix(Matrix::nearPD(M_hat[,,j],corr = FALSE)$mat)
    M_hat[,,j]=as.matrix(Matrix::forceSymmetric(M_hat[,,j]))
  }
  if(corrOut){ #Convert to correlation matrix if required
    for(j in 1:nrow(xout)){
      D=diag(1/sqrt(diag(M_hat[,,j])))
      M_hat[,,j]=D%*%M_hat[,,j]%*%D
      M_hat[,,j]=as.matrix(Matrix::forceSymmetric(M_hat[,,j]))
    }
  }
  Mout=list()
  for(j in 1:nrow(xout)){
    Mout=c(Mout,list(M_hat[,,j]))
  }
  optns$corrOut=corrOut
  return(list(xout=xout, Mout=Mout, optns=optns))
}
